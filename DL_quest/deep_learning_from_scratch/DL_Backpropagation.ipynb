{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CAVASOL/aiffel_quest/blob/main/DL_quest/DL_prep/DL_Backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "415d6d07",
      "metadata": {
        "id": "415d6d07"
      },
      "source": [
        "## Backward propagation of errors\n",
        "\n",
        "* 계산 결과와 정답의 오차를 구해 이 오차에 관여하는 값들의 가증치를 수정하여 오차가 작아지는 방향으로 일정 횟수를 반복해 수정하는 방법.\n",
        "* 수치 미분과 오차역전파법의 결과를 비교하면 오차역전파법의 구현에 오류가 없는지 확인할 수 있음.\n",
        "* 이번 장에서 계층 구현을 위해 명시하는 규칙 2가지:  \n",
        "    1) 모든 계층은 forward()와 backward() 메서드를 가진다.  \n",
        "    2) 모든 계층은 인스턴스 변수인 params(가중치와 편향)와 grads(기울기)를 가진다.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92749538",
      "metadata": {
        "id": "92749538"
      },
      "source": [
        "### 5-1. Computational Graph\n",
        "\n",
        "* 단순한 계산에 집중하여 문제를 단순화.\n",
        "* 중간 계산 결과를 모두 저장할 수 있음.\n",
        "* 역전파를 통해 미분을 효율적으로 계산."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b832d3ad",
      "metadata": {
        "id": "b832d3ad"
      },
      "source": [
        "### 5-2. Chain rule\n",
        "\n",
        "* 역전파(backpropagation) 알고리즘의 핵심.\n",
        "* 합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다.\n",
        "* 미분의 기본 원리 중 하나로, 함수가 다른 함수로 구성되어 있을 때 전체 함수의 도함수(미분)를 각 함수의 도함수의 곱으로 표현하는 규칙."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f19bfac",
      "metadata": {
        "id": "5f19bfac"
      },
      "source": [
        "### 5-3. Backpropagation\n",
        "\n",
        "* **기울기 계산**: 역전파는 손실 함수의 기울기(그레이디언트)를 효과적으로 계산하는 데 사용됩니다. 이는 모델의 가중치 및 편향을 업데이트하기 위해 필요한 정보로, 모델의 출력에서 입력 방향으로 오차를 전파하며 각 레이어에서 그레이디언트를 계산합니다.  \n",
        "\n",
        "\n",
        "* **연쇄 법칙 활용**: 역전파는 연쇄 법칙(Chain Rule)을 기반으로 하며, 그레이디언트를 각 레이어로 전파하는 과정을 효율적으로 수행합니다. 이를 통해 많은 수의 레이어로 이루어진 딥 신경망에서도 그레이디언트를 계산할 수 있습니다.  \n",
        "\n",
        "\n",
        "* **자동 미분**: 역전파는 자동 미분(automatic differentiation)의 핵심 요소입니다. 그레이디언트 계산을 자동으로 처리하므로 모델을 개발할 때 복잡한 미분식을 직접 유도하지 않아도 됩니다.  \n",
        "\n",
        "\n",
        "* **신경망 학습**: 역전파는 신경망 모델의 가중치 및 편향을 학습하기 위해 사용됩니다. 손실 함수의 기울기를 최소화하는 방향으로 가중치를 조정하면 모델이 데이터를 더 잘 반영하도록 학습됩니다.  \n",
        "\n",
        "\n",
        "* **비선형성 학습**: 역전파를 사용하면 다양한 활성화 함수(예: ReLU, 시그모이드, 소프트맥스)를 통해 비선형성을 학습할 수 있습니다. 이를 통해 신경망이 복잡한 함수 근사를 수행할 수 있습니다.  \n",
        "\n",
        "\n",
        "* **반복 학습**: 역전파는 반복적인 최적화 알고리즘을 기반으로 하며, 데이터를 여러 번 사용하여 모델을 학습합니다. 이를 통해 모델이 데이터에 적응하고 예측 성능을 향상시킬 수 있습니다.  \n",
        "\n",
        "\n",
        "* **병렬 및 분산 계산**: 역전파를 사용하면 그래프의 다양한 부분을 병렬로 계산하거나 분산 컴퓨팅을 활용하여 학습 속도를 향상시킬 수 있습니다.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0b6f0b6",
      "metadata": {
        "id": "a0b6f0b6"
      },
      "source": [
        "**Sum node**\n",
        "\n",
        "* 범용 덧셈 노드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c82f1672",
      "metadata": {
        "id": "c82f1672"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "D, N = 8, 7\n",
        "x = np.random.randn(N, D) # 입력\n",
        "y = np.sum(x, axis-0, keepdims=True) # 순전파\n",
        "\n",
        "dy = np.random.randn(1, D) # 무작위 기울기\n",
        "dx = np.repeat(dy, N, axis=0) # 역전파"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68517241",
      "metadata": {
        "id": "68517241"
      },
      "source": [
        "**MatMul node**\n",
        "\n",
        "* Matrix Multiply"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c44d47a",
      "metadata": {
        "id": "4c44d47a"
      },
      "outputs": [],
      "source": [
        "class MatMul:\n",
        "\n",
        "    def __init__(self, W):\n",
        "        self.params = [W]\n",
        "        self.grads = [np.zeros_like(W)]\n",
        "        self.x = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        W, = self.params\n",
        "        out = np.matmul(x, W)\n",
        "        self.x = x\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        W, = self.params\n",
        "        dx = np.matmul(dout, W.T)\n",
        "        dW = np.matmul(self.x.T, dout)\n",
        "        self.grads[0][...] = dW\n",
        "\n",
        "        return dx"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdb0d9ff",
      "metadata": {
        "id": "cdb0d9ff"
      },
      "source": [
        "### 5-4. MulLayer / AddLayer 계층 구현하기\n",
        "\n",
        "**MulLayer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c40df1e",
      "metadata": {
        "id": "1c40df1e"
      },
      "outputs": [],
      "source": [
        "class MulLayer:\n",
        "    def __init__(self):\n",
        "        self.x = None\n",
        "        self.y = None\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        out = x*y\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout*self.y\n",
        "        dy = dout*self.x\n",
        "\n",
        "        return dx, dy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "473b65b2",
      "metadata": {
        "id": "473b65b2",
        "outputId": "a94c8ad7-1cdf-499f-b0b6-0519f3df2e47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "220.00000000000003\n"
          ]
        }
      ],
      "source": [
        "apple = 100\n",
        "apple_num = 2\n",
        "tax = 1.1\n",
        "\n",
        "mul_apple_layer = MulLayer()\n",
        "mul_tax_layer = MulLayer()\n",
        "\n",
        "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
        "price = mul_tax_layer.forward(apple_price, tax)\n",
        "\n",
        "print(price)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1464b67",
      "metadata": {
        "id": "d1464b67"
      },
      "source": [
        "**AddLayer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c023d72d",
      "metadata": {
        "id": "c023d72d"
      },
      "outputs": [],
      "source": [
        "class AddLayer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        out = x + y\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout*1\n",
        "        dy = dout*1\n",
        "        return dx, dy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58f846c0",
      "metadata": {
        "id": "58f846c0",
        "outputId": "ce6720e5-7d28-4297-ab2f-ad9985b0cd6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "price: 715\n",
            "dApple: 2.2\n",
            "dApple_num: 110\n",
            "dOrange: 3.3000000000000003\n",
            "dOrange_num: 165\n",
            "dTax: 650\n"
          ]
        }
      ],
      "source": [
        "apple = 100\n",
        "apple_num = 2\n",
        "orange = 150\n",
        "orange_num = 3\n",
        "tax = 1.1\n",
        "\n",
        "# layer\n",
        "mul_apple_layer = MulLayer()\n",
        "mul_orange_layer = MulLayer()\n",
        "add_apple_orange_layer = AddLayer()\n",
        "mul_tax_layer = MulLayer()\n",
        "\n",
        "# forward\n",
        "apple_price = mul_apple_layer.forward(apple, apple_num)  # (1)\n",
        "orange_price = mul_orange_layer.forward(orange, orange_num)  # (2)\n",
        "all_price = add_apple_orange_layer.forward(apple_price, orange_price)  # (3)\n",
        "price = mul_tax_layer.forward(all_price, tax)  # (4)\n",
        "\n",
        "# backward\n",
        "dprice = 1\n",
        "dall_price, dtax = mul_tax_layer.backward(dprice)  # (4)\n",
        "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)  # (3)\n",
        "dorange, dorange_num = mul_orange_layer.backward(dorange_price)  # (2)\n",
        "dapple, dapple_num = mul_apple_layer.backward(dapple_price)  # (1)\n",
        "\n",
        "print(\"price:\", int(price))\n",
        "print(\"dApple:\", dapple)\n",
        "print(\"dApple_num:\", int(dapple_num))\n",
        "print(\"dOrange:\", dorange)\n",
        "print(\"dOrange_num:\", int(dorange_num))\n",
        "print(\"dTax:\", dtax)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a0317f8",
      "metadata": {
        "id": "8a0317f8"
      },
      "source": [
        "### 5-5. ReLU / Sigmoid 활성화 함수 계층 구현하기\n",
        "\n",
        "완전연결계층에 의한 변환은 선형 변환. 여기에 비선형 효과를 부여하는 것이 활성화 함수.  \n",
        "즉, 비선형 활성화 함수를 이용함으로써 신경망의 표현력을 높일 수 있음.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "047c9fd6",
      "metadata": {
        "id": "047c9fd6"
      },
      "source": [
        "**ReLU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28590edc",
      "metadata": {
        "id": "28590edc"
      },
      "outputs": [],
      "source": [
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "\n",
        "        return dx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af0ba95c",
      "metadata": {
        "id": "af0ba95c",
        "outputId": "c8c5ebdb-3aeb-4f8c-9d0b-7ba25a6a7812"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 1.  -0.5]\n",
            " [-2.   3. ]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c59c47c",
      "metadata": {
        "id": "4c59c47c",
        "outputId": "a472ff64-4c16-4446-d27e-ddee3bdf28fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[False  True]\n",
            " [ True False]]\n"
          ]
        }
      ],
      "source": [
        "mask = (x <= 0)\n",
        "print(mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddb43446",
      "metadata": {
        "id": "ddb43446"
      },
      "source": [
        "**Sigmoid**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "411b1d47",
      "metadata": {
        "id": "411b1d47"
      },
      "source": [
        "```\n",
        "class Sigmoid:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.params, self.grads = [], []\n",
        "        self.out = None\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = 1 / (1 + np.exp(-x))\n",
        "        self.out = out\n",
        "        \n",
        "        return out\n",
        "        \n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "        \n",
        "        return dx\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec2cb2fb",
      "metadata": {
        "id": "ec2cb2fb"
      },
      "outputs": [],
      "source": [
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = 1 / (1 + np.exp(-x))\n",
        "        self.out = out\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "\n",
        "        return dx"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd0cd668",
      "metadata": {
        "id": "dd0cd668"
      },
      "source": [
        "### 5-6. Affine / Softmax 계층 구현하기\n",
        "\n",
        "**Affine**\n",
        "\n",
        "```\n",
        "class Affine:\n",
        "    \n",
        "    def __init__(self, W, b):\n",
        "        self.params = [W, b]\n",
        "        self.grads - [np.zeros_like(W), np.zeros_like(b)]\n",
        "        self.x = None\n",
        "        \n",
        "    def forward(self, x):\n",
        "        W, b = self.params\n",
        "        out = np.matmul(x, W) + b\n",
        "        self.x = x\n",
        "        \n",
        "        return out\n",
        "        \n",
        "    def backward(self, dout):\n",
        "        W, b = self.params\n",
        "        dx = np.matmul(dout, W.T)\n",
        "        dW = np.matmul(self.x.T, dout)\n",
        "        db = np.sum(dout, axis=0)\n",
        "        \n",
        "        self.grads[0][...] = dW\n",
        "        self.grads[1][...] = db\n",
        "        \n",
        "        return dx\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95721ef6",
      "metadata": {
        "id": "95721ef6"
      },
      "source": [
        "* 완전연결계층에 의한 변환.\n",
        "* 입력 데이터에 가중치를 곱하고 편향을 더하는 선형 변환을 수행.\n",
        "* Affine 연산은 딥러닝 모델의 히든 레이어에서 사용되며, 비선형성을 도입하기 위해 활성화 함수(예: ReLU)와 함께 사용."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46bb9400",
      "metadata": {
        "id": "46bb9400"
      },
      "outputs": [],
      "source": [
        "X = np.random.rand(2)\n",
        "W = np.random.rand(2,3)\n",
        "B = np.random.rand(3)\n",
        "\n",
        "X.shape\n",
        "W.shape\n",
        "B.shape\n",
        "\n",
        "Y = np.dot(X, W) + B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1d6b91c",
      "metadata": {
        "id": "b1d6b91c",
        "outputId": "fb1a6cce-b3c8-4bbd-848a-d935586aca52"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0],\n",
              "       [10, 10, 10]])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_dot_W = np.array([[0, 0, 0], [10, 10, 10]])\n",
        "B = np.array([1, 2, 3])\n",
        "\n",
        "X_dot_W"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbf728bd",
      "metadata": {
        "id": "cbf728bd",
        "outputId": "6a6d0957-ffae-41f4-9d5c-317aabb7e057"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 1,  2,  3],\n",
              "       [11, 12, 13]])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_dot_W + B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5af2fe28",
      "metadata": {
        "id": "5af2fe28",
        "outputId": "0694f2c1-41f0-4485-83ec-783ba1242a4a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1, 2, 3],\n",
              "       [4, 5, 6]])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dY = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "dY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0404c9cf",
      "metadata": {
        "id": "0404c9cf",
        "outputId": "05bbb0c0-e0d1-4155-cb3e-4e47d14e233e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([5, 7, 9])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dB = np.sum(dY, axis=0)\n",
        "dB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "944aaf7d",
      "metadata": {
        "id": "944aaf7d"
      },
      "outputs": [],
      "source": [
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        self.x = None\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        out = np.dot(x, self.W) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "\n",
        "        return dx"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0171029",
      "metadata": {
        "id": "a0171029"
      },
      "source": [
        "**Softmax**\n",
        "\n",
        "* 입력 벡터를 받아 각 클래스에 대한 확률 분포로 변환.\n",
        "* 주로 분류 문제에서 출력 레이어에서 사용.\n",
        "* Softmax는 입력을 정규화하고, 각 클래스에 대한 예측 확률을 계산하여 이를 출력.\n",
        "* 이 확률 값은 0과 1 사이에 있고, 모든 클래스의 확률의 합은 1."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df418ac9",
      "metadata": {
        "id": "df418ac9"
      },
      "source": [
        "**Softmax with Loss**\n",
        "\n",
        "* 소프트맥스 함수와 교차 엔트로피 오차를 계산하는 계층\n",
        "* 이 두 계층을 통합하면 역전파 계산이 쉬워져"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b329e8fa",
      "metadata": {
        "id": "b329e8fa"
      },
      "outputs": [],
      "source": [
        "class SoftmasWithLoass:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.loss = None\n",
        "        self.y = None\n",
        "        self.t = None\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "        dx = (self.y - self.t) / batch_size\n",
        "\n",
        "        return dx"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1340b694",
      "metadata": {
        "id": "1340b694"
      },
      "source": [
        "### 5-7. Backpropagation 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f25d346",
      "metadata": {
        "id": "2f25d346"
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "sys.path.append(os.pardir)\n",
        "import numpy as np\n",
        "from common.layers import *\n",
        "from common.gradient import numerical_gradient\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "class TwoLayerNet:\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
        "        # 가중치 초기화\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "        self.params['b1'] = np.zeros(hidden_size)\n",
        "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "        self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
        "\n",
        "        self.lastLayer = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        return self.lastLayer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis=1)\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "\n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "        return accuracy\n",
        "\n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def numerical_gradient(self, x, t):\n",
        "        loss_W = lambda W: self.loss(x, t)\n",
        "\n",
        "        grads = {}\n",
        "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.lastLayer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b184289",
      "metadata": {
        "id": "1b184289",
        "outputId": "ec6b9e3c-125b-4e56-a7ae-edb5d8100883"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W1:1.8380962847574696e-10\n",
            "b1:1.0226736920347126e-09\n",
            "W2:6.817680850200458e-08\n",
            "b2:1.3701536088345234e-07\n"
          ]
        }
      ],
      "source": [
        "import sys, os\n",
        "sys.path.append(os.pardir)\n",
        "import numpy as np\n",
        "from dataset.mnist import load_mnist\n",
        "from two_layer_net import TwoLayerNet\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "x_batch = x_train[:3]\n",
        "t_batch = t_train[:3]\n",
        "\n",
        "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
        "grad_backprop = network.gradient(x_batch, t_batch)\n",
        "\n",
        "# 각 가중치의 절대 오차의 평균을 구한다.\n",
        "for key in grad_numerical.keys():\n",
        "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
        "    print(key + \":\" + str(diff))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9587e38b",
      "metadata": {
        "id": "9587e38b"
      },
      "source": [
        "**오차역전파법을 사용한 학습 구현하기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19532402",
      "metadata": {
        "id": "19532402",
        "outputId": "d83844d7-7794-4059-e4a0-d4f72acf186a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.09736666666666667 0.0982\n",
            "0.7969166666666667 0.8013\n",
            "0.8770333333333333 0.8797\n",
            "0.8973 0.9001\n",
            "0.9079666666666667 0.9078\n",
            "0.9138833333333334 0.9141\n",
            "0.91915 0.9194\n",
            "0.9235666666666666 0.9253\n",
            "0.9279 0.9283\n",
            "0.9314666666666667 0.9304\n",
            "0.93405 0.9329\n",
            "0.9364666666666667 0.9363\n",
            "0.93795 0.9373\n",
            "0.9411666666666667 0.9409\n",
            "0.9430333333333333 0.9423\n",
            "0.94595 0.9432\n",
            "0.9468666666666666 0.945\n"
          ]
        }
      ],
      "source": [
        "import sys, os\n",
        "sys.path.append(os.pardir)\n",
        "\n",
        "import numpy as np\n",
        "from dataset.mnist import load_mnist\n",
        "from two_layer_net import TwoLayerNet\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    # 기울기 계산\n",
        "    #grad = network.numerical_gradient(x_batch, t_batch) # 수치 미분 방식\n",
        "    grad = network.gradient(x_batch, t_batch) # 오차역전파법 방식(훨씬 빠르다)\n",
        "\n",
        "    # 갱신\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(train_acc, test_acc)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}