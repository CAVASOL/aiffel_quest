{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CAVASOL/aiffel_quest/blob/main/DL_quest/cv_with_dl/8_Object_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f560b7cf",
      "metadata": {
        "id": "f560b7cf"
      },
      "source": [
        "## Object Detection\n",
        "\n",
        "**학습 목표**\n",
        "\n",
        "* Object Detection과 Localization, Classification을 비교해 보며 개념을 정리해 봅시다.\n",
        "* Object detection 모델들을 Two-Stage Detector와 One-Stage Detector로 나눠서 이해하고 어떤 차이점이 있는지 생각해 봅시다.\n",
        "* R-CNN 모델의 전체적인 구조를 보며 학습 과정을 이해하고 각 단계별로 관련 개념과 학습과정을 자세히 알아봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3d44ac0",
      "metadata": {
        "id": "f3d44ac0"
      },
      "source": [
        "**Q. Computer Vision에는 다양한 Task들이 있습니다. 그 Task 안에서도 이미지 안에 하나의 객체가 있는 경우(Single Object)와 이미지 안에 여러 개의 객체가 있는 경우(Multiple Objects)로 크게 2가지로 분류할 수 있습니다. 이 2가지 경우에 해당하는 Task는 어떤 게 있었고 어떤 특징이 있는지 설명해 보세요.**\n",
        "```\n",
        "Single Object\n",
        "• Image Classification: 단 하나의 대상의 true label이 무엇인지 분류합니다.\n",
        "• Localization: 단 하나의 대상의 위치를 가장 적절하게 지정하는 bounding box(좌표)를 찾습니다.\n",
        "\n",
        "Multiple Objects\n",
        "• Object Detection: 여러 개의 bounding box를 찾아 여러 대상의 위치를 가장 적절하게 지정하는 동시에 각 bounding box 내의 대상을 판별합니다.\n",
        "• Segmentation: 픽셀 단위별로 detection을 수행합니다. (특정 픽셀이 속한 대상 판별) Segmentation 종류에는 Semantic Segmentation와 Instance Segmentation가 있습니다.\n",
        "```\n",
        "\n",
        "**Q. Localization (+ Classification)과 Object Detection의 공통점과 차이점에 대해서 설명해 보세요.**\n",
        "```\n",
        "Localization과 Detection 모두 대상의 위치를 bounding box로 지정하고, 해당 bounding box 내의 대상이 무엇인지를 판별합니다. 두 task의 차이점으로는 Localization과 비교하면, Object Detection은 하나의 이미지 내의 여러 대상의 위치를 찾고 분류해야 하기 때문에 Localization에 비해 난이도가 높다는 점이 있습니다.\n",
        "```\n",
        "\n",
        "**Q. AlexNet, VGG 등 일반적인 CNN 모델만 사용해서 Object Detection task를 해결할 수 있을까요?**\n",
        "```\n",
        "객체 감지 모델은 일반적인 CNN 모델보다 훨씬 더 복잡하며, 객체의 위치, 크기, 클래스 등을 예측하기 위한 다양한 구성 요소와 모듈을 사용해요 Object Detection 작업을 수행하기 위해서는 일반적인 CNN 모델을 특수한 객체 감지 모델로 확장하거나, 사전 훈련된 객체 감지 모델을 사용해야 합니다.\n",
        "```\n",
        "\n",
        "**Q. Object Detection task 방법에는 1-stage detector과 2-stage detector가 있습니다. 그 중에서 Two-Stage Detector의 특징과 장점에 대해서 설명해 보세요.**\n",
        "```\n",
        "Two-Stage Detector는 Region Proposal을 먼저 진행하면서 이미지 내에 대상이 있을 법한 영역인 RoI(Region of Interest)를 찾아냅니다. 그 찾아낸 RoI 안에 있는 이미지를 classification 합니다. Two-Stage Detector는 두 단계로 나누어져서 느리지만, 정확도가 비교적 높다는 장점이 존재합니다.\n",
        "```\n",
        "\n",
        "**Q. Object Detection 모델 중, Two-Stage Detector인 모델을 찾아보고 작성해 보세요.**\n",
        "```\n",
        "대표적으로 R-CNN 계열의 모델들이 있습니다. (R-CNN, Fast R-CNN 등) 그 외에도 SPPNet, Pyramid Networks 등이 있습니다.\n",
        "```\n",
        "\n",
        "**Q. 이번에는 One-Stage Detector의 특징과 장점에 대해서 설명해 보세요.**\n",
        "```\n",
        "One-Stage Detector는 Region Proposal과 Classification을 동시에 수행합니다. RoI를 찾아내는 대신, 이미지 전체를 대상으로 Classification을 수행합니다. YOLO v1의 경우에는 전체 이미지를 특정 크기의 grid로 분할한 후, cell의 중심에 object가 있다고 판단되는 특정 cell에 대하여 classification을 수행합니다.\n",
        "\n",
        "속도는 Two-Stage Detector보다 빠르지만 정확도는 상대적으로 떨어질 수 있습니다. (현재는 정확도가 많이 개선되었습니다.) 자율주행 자동차, 영상 등 real-time processing을 요구하는 태스크에는 One-Stage Detector가 자주 활용됩니다.\n",
        "```\n",
        "\n",
        "**Q. Object Detection 모델 중, One-Stage Detector인 모델을 찾아보고 작성해 보세요.**\n",
        "```\n",
        "You Only Look Once! (YOLOv1, YOLOv3 등) 그 외에도 SSD, Retina-Net 등이 있습니다.\n",
        "```\n",
        "\n",
        "**Q. R-CNN 모델 전체적인 구조를 보고 R-CNN 모델의 동작을 순서대로 설명해 보세요.**\n",
        "```\n",
        "• Region Proposal: 입력 이미지에 selective search 알고리즘 적용, 객체가 있을 만한 RoI(Region of Interest)의 후보 2천 개를 추출.\n",
        "• Resize: 추출된 RoI의 후보 2천 개의 크기를 227x227로 변형. (동일한 사이즈로 변형하기 때문에 이미지의 왜곡이 있을 수 있습니다.)\n",
        "• 이미 학습된 CNN 구조를 통해서 4,096차원의 특징 벡터를 추출.\n",
        "• 각각의 객체별로 학습된 SVM classifier를 이용해서, 추출된 특징 벡터를 분류.\n",
        "• Bounding box regression으로 적절한 객체의 경계(bounding box)를 설정.\n",
        "```\n",
        "\n",
        "**Q. 객체가 있을만한 후보를 찾는 Region Proposal과 관련된 여러가지 알고리즘이 있습니다. Selective Search 알고리즘 과정에 대해서 설명해 보세요.**\n",
        "```\n",
        "Selective Search는 색, 무늬 크기, 형태를 바탕으로 주변 픽셀 간의 유사도를 계산합니다. 계산한 유사도를 바탕으로 segmentation을 수행한 후, 작은 segment들을 묶어가며 최종 후보를 찾습니다.\n",
        "\n",
        "Selective Search의 초기 segmentation은 매우 세밀한 영역까지 segmentation 하는 over-segmentation을 하며 유사도가 비슷한 segment들을 반복적으로 묶어갑니다.\n",
        "```\n",
        "\n",
        "**Q. R-CNN 모델은 Region Proposal이 끝난 후 최종적으로 Classification을 진행합니다. 이 Classification의 진행과정에 대해서 설명해 보세요.**\n",
        "```\n",
        "R-CNN은 객체 후보 영역을 분류하기 위해 객체의 위치와 클래스에 대한 예측을 동시에 수행해요 이를 통해 객체 감지와 분류 작업을 수행합니다. 이러한 접근 방식은 객체 감지 작업에서 좋은 성능을 제공할 수 있고, Faster R-CNN과 같은 모델은 이러한 아키텍처를 효율적으로 개선하고 속도를 향상시킬 수 있습니다.\n",
        "\n",
        "RoI를 동일한 사이즈로 맞춘 후, Pre-trained된 Convolutional Neural Network 모델을 통해서 feature extraction(4,096차원)을 수행합니다. 앞의 Feature Extraction 결과 바탕으로 학습한 SVM을 이용해서 feature extraction 결과를 분류합니다.\n",
        "\n",
        "그 후, 2,000개의 proposed region 중에서 IoU 값을 이용해 \"non-maximum suppression\"을 적용해 적합하지 않은 것을 탈락시킵니다. 마지막으로 Bounding box의 위치를 맞추기 위해서 bounding box regression을 실행합니다.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd09c280",
      "metadata": {
        "id": "fd09c280"
      },
      "source": [
        "### Object Detection과 관련된 개념들\n",
        "\n",
        "**Sliding window**\n",
        "\n",
        "Object Detection은 이미지의 “어느 위치”에 Object가 있는지 알아보는 태스크입니다.\n",
        "\n",
        "Sliding window는 일정 크기의 window를 이미지 위에서 조금씩 옮겨가며 전수조사를 하는 것입니다. Window 사이즈를 바꿔 가면서 Object가 있는 위치를 찾고, 효율적으로 Object 위치를 찾기 위해서 stride를 변경할 수 있습니다. 그러나 계산 비용이 많이 들고 학습 속도가 느리다는 단점이 있습니다.\n",
        "\n",
        "**IoU (Intersection over Union)**\n",
        "\n",
        "IoU는 모델이 예측한 bounding box와 실제 정답인 ground truth box가 얼마나 겹치는 지를 측정하는 지표입니다. 만약 100%로 겹치게 되면 IoU 값은 1이 됩니다.\n",
        "\n",
        "* Area of Union: predicted bounding box와 ground-truth bounding box를 모두 포함하는 영역\n",
        "* Area of Overlap: predicted bounding box와 ground-truth bounding box가 겹치는 부분\n",
        "\n",
        "**NMS (Non Maximum/maximal Suppression)**\n",
        "\n",
        "NMS은 수많은 bounding box 중 가장 적합한 box를 선택하는 기법입니다.\n",
        "\n",
        "NMS의 과정\n",
        "\n",
        "* 모든 bounding box에 대하여 threshold 이하의 confidence score를 가지는 bounding box는 제거합니다.\n",
        "* 남은 bounding box들을 confidence score 기준으로 내림차순 정렬합니다.\n",
        "* 정렬 후 가장 confidence score가 높은 bounding box를 기준으로 다른 bounding box와 IoU를 구합니다.\n",
        "* IoU가 특정 기준 값보다 높으면, confidence score가 낮은 bounding box를 제거합니다.\n",
        "* 해당 과정을 순차적으로 반복합니다.\n",
        "\n",
        "**mAP (mean Average Precision)**\n",
        "\n",
        "* Precision-Recall Curve: confidence threshold의 변화에 따른 정밀도와 재현율의 변화 곡선입니다.\n",
        "* AP: Precision-Recall Curve의 아래 부분 면적을 의미합니다.\n",
        "* mAP: AP는 하나의 object에 대한 성능 수치이며, mAP는 여러 object들의 AP를 평균한 값을 의미합니다. 따라서 Object Detection 모델의 성능 평가에 사용합니다.\n",
        "\n",
        "**Bounding Box Regression**\n",
        "\n",
        "[참고 자료](https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}