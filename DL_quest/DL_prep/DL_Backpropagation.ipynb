{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CAVASOL/aiffel_quest/blob/main/DL_quest/DL_prep/DL_Backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "181aea52",
      "metadata": {
        "id": "181aea52"
      },
      "source": [
        "## Backward propagation of errors\n",
        "\n",
        "* 계산 결과와 정답의 오차를 구해 이 오차에 관여하는 값들의 가증치를 수정하여 오차가 작아지는 방향으로 일정 횟수를 반복해 수정하는 방법.\n",
        "* 수치 미분과 오차역전파법의 결과를 비교하면 오차역전파법의 구현에 오류가 없는지 확인할 수 있음."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "824a7a31",
      "metadata": {
        "id": "824a7a31"
      },
      "source": [
        "### 5-1. Computational Graph\n",
        "\n",
        "* 단순한 계산에 집중하여 문제를 단순화.\n",
        "* 중간 계산 결과를 모두 저장할 수 있음.\n",
        "* 역전파를 통해 미분을 효율적으로 계산."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96ad3e03",
      "metadata": {
        "id": "96ad3e03"
      },
      "source": [
        "### 5-2. Chain rule\n",
        "\n",
        "* 역전파(backpropagation) 알고리즘의 핵심.\n",
        "* 합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다.\n",
        "* 미분의 기본 원리 중 하나로, 함수가 다른 함수로 구성되어 있을 때 전체 함수의 도함수(미분)를 각 함수의 도함수의 곱으로 표현하는 규칙."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00a91e62",
      "metadata": {
        "id": "00a91e62"
      },
      "source": [
        "### 5-3. Backpropagation\n",
        "\n",
        "* **기울기 계산**: 역전파는 손실 함수의 기울기(그레이디언트)를 효과적으로 계산하는 데 사용됩니다. 이는 모델의 가중치 및 편향을 업데이트하기 위해 필요한 정보로, 모델의 출력에서 입력 방향으로 오차를 전파하며 각 레이어에서 그레이디언트를 계산합니다.  \n",
        "\n",
        "\n",
        "* **연쇄 법칙 활용**: 역전파는 연쇄 법칙(Chain Rule)을 기반으로 하며, 그레이디언트를 각 레이어로 전파하는 과정을 효율적으로 수행합니다. 이를 통해 많은 수의 레이어로 이루어진 딥 신경망에서도 그레이디언트를 계산할 수 있습니다.  \n",
        "\n",
        "\n",
        "* **자동 미분**: 역전파는 자동 미분(automatic differentiation)의 핵심 요소입니다. 그레이디언트 계산을 자동으로 처리하므로 모델을 개발할 때 복잡한 미분식을 직접 유도하지 않아도 됩니다.  \n",
        "\n",
        "\n",
        "* **신경망 학습**: 역전파는 신경망 모델의 가중치 및 편향을 학습하기 위해 사용됩니다. 손실 함수의 기울기를 최소화하는 방향으로 가중치를 조정하면 모델이 데이터를 더 잘 반영하도록 학습됩니다.  \n",
        "\n",
        "\n",
        "* **비선형성 학습**: 역전파를 사용하면 다양한 활성화 함수(예: ReLU, 시그모이드, 소프트맥스)를 통해 비선형성을 학습할 수 있습니다. 이를 통해 신경망이 복잡한 함수 근사를 수행할 수 있습니다.  \n",
        "\n",
        "\n",
        "* **반복 학습**: 역전파는 반복적인 최적화 알고리즘을 기반으로 하며, 데이터를 여러 번 사용하여 모델을 학습합니다. 이를 통해 모델이 데이터에 적응하고 예측 성능을 향상시킬 수 있습니다.  \n",
        "\n",
        "\n",
        "* **병렬 및 분산 계산**: 역전파를 사용하면 그래프의 다양한 부분을 병렬로 계산하거나 분산 컴퓨팅을 활용하여 학습 속도를 향상시킬 수 있습니다.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b416351",
      "metadata": {
        "id": "0b416351"
      },
      "source": [
        "### 5-4. 계층 구현하기\n",
        "\n",
        "**MulLayer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63578ad4",
      "metadata": {
        "id": "63578ad4"
      },
      "outputs": [],
      "source": [
        "class MulLayer:\n",
        "    def __init__(self):\n",
        "        self.x = None\n",
        "        self.y = None\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        out = x*y\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout*self.y\n",
        "        dy = dout*self.x\n",
        "\n",
        "        return dx, dy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1136b6de",
      "metadata": {
        "id": "1136b6de",
        "outputId": "dba35e13-fa8a-4908-e245-bd53cd10cd69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "220.00000000000003\n"
          ]
        }
      ],
      "source": [
        "apple = 100\n",
        "apple_num = 2\n",
        "tax = 1.1\n",
        "\n",
        "mul_apple_layer = MulLayer()\n",
        "mul_tax_layer = MulLayer()\n",
        "\n",
        "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
        "price = mul_tax_layer.forward(apple_price, tax)\n",
        "\n",
        "print(price)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55ea3787",
      "metadata": {
        "id": "55ea3787"
      },
      "source": [
        "**AddLayer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1efa03b",
      "metadata": {
        "id": "f1efa03b"
      },
      "outputs": [],
      "source": [
        "class AddLayer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        out = x + y\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout*1\n",
        "        dy = dout*1\n",
        "        return dx, dy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaa8c3e9",
      "metadata": {
        "id": "eaa8c3e9",
        "outputId": "d627c012-41df-47d6-f26a-33db712fb26c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "price: 715\n",
            "dApple: 2.2\n",
            "dApple_num: 110\n",
            "dOrange: 3.3000000000000003\n",
            "dOrange_num: 165\n",
            "dTax: 650\n"
          ]
        }
      ],
      "source": [
        "apple = 100\n",
        "apple_num = 2\n",
        "orange = 150\n",
        "orange_num = 3\n",
        "tax = 1.1\n",
        "\n",
        "# layer\n",
        "mul_apple_layer = MulLayer()\n",
        "mul_orange_layer = MulLayer()\n",
        "add_apple_orange_layer = AddLayer()\n",
        "mul_tax_layer = MulLayer()\n",
        "\n",
        "# forward\n",
        "apple_price = mul_apple_layer.forward(apple, apple_num)  # (1)\n",
        "orange_price = mul_orange_layer.forward(orange, orange_num)  # (2)\n",
        "all_price = add_apple_orange_layer.forward(apple_price, orange_price)  # (3)\n",
        "price = mul_tax_layer.forward(all_price, tax)  # (4)\n",
        "\n",
        "# backward\n",
        "dprice = 1\n",
        "dall_price, dtax = mul_tax_layer.backward(dprice)  # (4)\n",
        "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)  # (3)\n",
        "dorange, dorange_num = mul_orange_layer.backward(dorange_price)  # (2)\n",
        "dapple, dapple_num = mul_apple_layer.backward(dapple_price)  # (1)\n",
        "\n",
        "print(\"price:\", int(price))\n",
        "print(\"dApple:\", dapple)\n",
        "print(\"dApple_num:\", int(dapple_num))\n",
        "print(\"dOrange:\", dorange)\n",
        "print(\"dOrange_num:\", int(dorange_num))\n",
        "print(\"dTax:\", dtax)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34c1a740",
      "metadata": {
        "id": "34c1a740"
      },
      "source": [
        "### 5-5. ReLU / Sigmoid 활성화 함수 계층 구현하기\n",
        "\n",
        "**ReLU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c16511fa",
      "metadata": {
        "id": "c16511fa"
      },
      "outputs": [],
      "source": [
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "\n",
        "        return dx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a5261d7",
      "metadata": {
        "id": "6a5261d7",
        "outputId": "196be153-4c96-4d91-b98c-11a14430a205"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 1.  -0.5]\n",
            " [-2.   3. ]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5865573",
      "metadata": {
        "id": "c5865573",
        "outputId": "0e54f3c4-39cf-4e60-f71c-ed62a35377a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[False  True]\n",
            " [ True False]]\n"
          ]
        }
      ],
      "source": [
        "mask = (x <= 0)\n",
        "print(mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f2fa132",
      "metadata": {
        "id": "2f2fa132"
      },
      "source": [
        "**Sigmoid**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d9af4a7",
      "metadata": {
        "id": "0d9af4a7"
      },
      "outputs": [],
      "source": [
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = 1 / (1 + np.exp(-x))\n",
        "        self.out = out\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "\n",
        "        return dx"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83bb113d",
      "metadata": {
        "id": "83bb113d"
      },
      "source": [
        "### 5-6. Affine / Softmax 계층 구현하기\n",
        "\n",
        "**Affine**\n",
        "\n",
        "* 입력 데이터에 가중치를 곱하고 편향을 더하는 선형 변환을 수행.\n",
        "* Affine 연산은 딥러닝 모델의 히든 레이어에서 사용되며, 비선형성을 도입하기 위해 활성화 함수(예: ReLU)와 함께 사용."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ab9418f",
      "metadata": {
        "id": "3ab9418f"
      },
      "outputs": [],
      "source": [
        "X = np.random.rand(2)\n",
        "W = np.random.rand(2,3)\n",
        "B = np.random.rand(3)\n",
        "\n",
        "X.shape\n",
        "W.shape\n",
        "B.shape\n",
        "\n",
        "Y = np.dot(X, W) + B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed72918b",
      "metadata": {
        "id": "ed72918b",
        "outputId": "92ff2035-6c0b-457b-9578-4d0bba556748"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0],\n",
              "       [10, 10, 10]])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_dot_W = np.array([[0, 0, 0], [10, 10, 10]])\n",
        "B = np.array([1, 2, 3])\n",
        "\n",
        "X_dot_W"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de970c06",
      "metadata": {
        "id": "de970c06",
        "outputId": "c8e031e0-7021-4b1d-f74b-67b96b26358b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 1,  2,  3],\n",
              "       [11, 12, 13]])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_dot_W + B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1fb48ab",
      "metadata": {
        "id": "f1fb48ab",
        "outputId": "2edd7dda-07ad-406f-8e20-441e92d796bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1, 2, 3],\n",
              "       [4, 5, 6]])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dY = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "dY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed27de93",
      "metadata": {
        "id": "ed27de93",
        "outputId": "74b803ee-1401-4936-8ba7-d8b287aaf0b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([5, 7, 9])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dB = np.sum(dY, axis=0)\n",
        "dB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6da45fce",
      "metadata": {
        "id": "6da45fce"
      },
      "outputs": [],
      "source": [
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        self.x = None\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        out = np.dot(x, self.W) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "\n",
        "        return dx"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37fe7c38",
      "metadata": {
        "id": "37fe7c38"
      },
      "source": [
        "**Softmax**\n",
        "\n",
        "* 입력 벡터를 받아 각 클래스에 대한 확률 분포로 변환.\n",
        "* 주로 분류 문제에서 출력 레이어에서 사용.\n",
        "* Softmax는 입력을 정규화하고, 각 클래스에 대한 예측 확률을 계산하여 이를 출력.\n",
        "* 이 확률 값은 0과 1 사이에 있고, 모든 클래스의 확률의 합은 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8c0e5e3",
      "metadata": {
        "id": "d8c0e5e3"
      },
      "outputs": [],
      "source": [
        "class SoftmasWithLoass:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.loss = None\n",
        "        self.y = None\n",
        "        self.t = None\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "        dx = (self.y - self.t) / batch_size\n",
        "\n",
        "        return dx"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98ad5536",
      "metadata": {
        "id": "98ad5536"
      },
      "source": [
        "### 5-7. Backpropagation 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d51c210",
      "metadata": {
        "id": "7d51c210"
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "sys.path.append(os.pardir)\n",
        "import numpy as np\n",
        "from common.layers import *\n",
        "from common.gradient import numerical_gradient\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "class TwoLayerNet:\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
        "        # 가중치 초기화\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "        self.params['b1'] = np.zeros(hidden_size)\n",
        "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "        self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
        "\n",
        "        self.lastLayer = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        return self.lastLayer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis=1)\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "\n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "        return accuracy\n",
        "\n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def numerical_gradient(self, x, t):\n",
        "        loss_W = lambda W: self.loss(x, t)\n",
        "\n",
        "        grads = {}\n",
        "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.lastLayer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cbb42fe",
      "metadata": {
        "id": "7cbb42fe",
        "outputId": "ff907513-7954-43af-8ce4-09c80887bb66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W1:1.8380962847574696e-10\n",
            "b1:1.0226736920347126e-09\n",
            "W2:6.817680850200458e-08\n",
            "b2:1.3701536088345234e-07\n"
          ]
        }
      ],
      "source": [
        "import sys, os\n",
        "sys.path.append(os.pardir)\n",
        "import numpy as np\n",
        "from dataset.mnist import load_mnist\n",
        "from two_layer_net import TwoLayerNet\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "x_batch = x_train[:3]\n",
        "t_batch = t_train[:3]\n",
        "\n",
        "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
        "grad_backprop = network.gradient(x_batch, t_batch)\n",
        "\n",
        "# 각 가중치의 절대 오차의 평균을 구한다.\n",
        "for key in grad_numerical.keys():\n",
        "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
        "    print(key + \":\" + str(diff))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e5a9e92",
      "metadata": {
        "id": "0e5a9e92"
      },
      "source": [
        "**오차역전파법을 사용한 학습 구현하기**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18314884",
      "metadata": {
        "id": "18314884",
        "outputId": "8230fdf2-095c-4721-e36e-a886615dd25c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.09736666666666667 0.0982\n",
            "0.7969166666666667 0.8013\n",
            "0.8770333333333333 0.8797\n",
            "0.8973 0.9001\n",
            "0.9079666666666667 0.9078\n",
            "0.9138833333333334 0.9141\n",
            "0.91915 0.9194\n",
            "0.9235666666666666 0.9253\n",
            "0.9279 0.9283\n",
            "0.9314666666666667 0.9304\n",
            "0.93405 0.9329\n",
            "0.9364666666666667 0.9363\n",
            "0.93795 0.9373\n",
            "0.9411666666666667 0.9409\n",
            "0.9430333333333333 0.9423\n",
            "0.94595 0.9432\n",
            "0.9468666666666666 0.945\n"
          ]
        }
      ],
      "source": [
        "import sys, os\n",
        "sys.path.append(os.pardir)\n",
        "\n",
        "import numpy as np\n",
        "from dataset.mnist import load_mnist\n",
        "from two_layer_net import TwoLayerNet\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    # 기울기 계산\n",
        "    #grad = network.numerical_gradient(x_batch, t_batch) # 수치 미분 방식\n",
        "    grad = network.gradient(x_batch, t_batch) # 오차역전파법 방식(훨씬 빠르다)\n",
        "\n",
        "    # 갱신\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(train_acc, test_acc)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}