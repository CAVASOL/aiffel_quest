{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CAVASOL/aiffel_quest/blob/main/DL_quest/DL_prep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sacred-large",
      "metadata": {
        "id": "sacred-large"
      },
      "source": [
        "## 1. Can AI replace human?\n",
        "\n",
        "\n",
        "### 1-1. Contents Guide\n",
        "\n",
        "\n",
        "    1) Can AI replace human?\n",
        "    2) Paired Data\n",
        "    3) Unpaired Data\n",
        "    4) Reinforcement Learning\n",
        "    5) Generator and Discriminator\n",
        "    6) Convolution\n",
        "    7) Convolutional Neural Network\n",
        "    8) RNN\n",
        "    9) Understanding RNNs Mathematically  \n",
        "\n",
        "\n",
        "### 1-2. 인공지능의 개요와 현황\n",
        "\n",
        "**Q. 인공지능으로 새롭게 생겨날 직업은 무엇일까요?**\n",
        ">인공지능 데이터와 현실 데이터를 구분하는 직업, 데이터 편향 감별사, AI변리사, AI변호사, 메타버스 건물주 ...\n",
        "\n",
        "### 1-3. 인공지능이란 무엇인가\n",
        "\n",
        "**Q. 인공지능으로 만들 수 있는 것들은 무엇이 있알까요?**  \n",
        ">개별화 학습 도우미, 기후 모니터, 방위 및 안보 시스템 등\n",
        "\n",
        "### 1-4. 왜 인공지능은 무서워졌을까?\n",
        "\n",
        "**Q. 영상을 참고하여, 인공지능을 설명해보세요.**\n",
        ">인공지능은 수학으로조차 표현할 수 없었던 복잡한 인간의 두뇌를 데이터를 기반으로 흉내낼 수 있는 기술입니다. 하지만 인공지능은 함수에요. 어떤 변수를 넣느냐에 따라 그 성능과 가능성은 무한할테지만 결국 인간에 의해 디자인 되고, 기능이라고 부를만한 걸 갖게 되죠.\n",
        "### 1-5. 인공지능, 머신러닝은 다른가\n",
        "\n",
        "**Q. 영상을 참고하여, 그네를 더 잘 타기 위해 사용한 방법은 무엇인가?**\n",
        ">머신러닝, 유전자 알고리즘을 통해 세대를 거듭할 수록 그네를 잘 타게 학습이 되요\n",
        "\n",
        "### 1-6. 인공지능 어떻게 똑똑해지는가\n",
        "\n",
        "**Q. 인공지능이 더 똑똑해질 수 있는 방법은 무엇일까요?**\n",
        ">데이터가 늘어날수록 인공지능 알고리즘이 많이 학습합니다\n",
        "\n",
        "### 1-7. 머신러닝의 세 가지 유형\n",
        "\n",
        "**Q. 만들고 싶은 인공지능을 작성하고, 세 가지 분류 중 어디에 해당하는지 작성해보세요.**\n",
        ">컴퓨터비전으로 이미지를 활용하여 시장분석을 수행, 가령 소비자 행동이나 경쟁업체 분석, 제품 추적, 매출 예측 등. 컴퓨터 비전을 사용하여 시장 분석을 수행하는 경우, 주로 다양한 기계 학습 접근 방식을 조합해서 활용할 수 있습니다. 어떤 방식을 사용할 것인지는 분석 목표와 데이터 유형에 따라 다를 수 있습니다.\n",
        "\n",
        "지도 학습 (Supervised Learning): 지도 학습은 레이블이 있는 데이터를 사용하여 모델을 학습하는 방식입니다. 예를 들어, 소비자 행동 분석을 수행하려면 이전의 소비자 행동 데이터와 해당 데이터에 대한 레이블(예: 어떤 제품을 구매했는지)를 사용하여 모델을 학습할 수 있습니다. 경쟁업체 분석, 제품 추적, 매출 예측 등도 지도 학습으로 처리할 수 있습니다.\n",
        "\n",
        "비지도 학습 (Unsupervised Learning): 비지도 학습은 레이블이 없는 데이터에서 패턴을 발견하는 데 사용됩니다. 예를 들어, 이미지 데이터를 군집화(cluster)하여 비슷한 제품을 찾는 데 사용할 수 있으며, 이를 통해 경쟁업체나 제품 분석을 수행할 수 있습니다. 레이블이 없는 데이터에서 경향성을 발견하는 데 유용합니다.\n",
        "\n",
        "강화 학습 (Reinforcement Learning): 강화 학습은 시장 분석에서 상호 작용과 의사 결정을 필요로 하는 경우에 사용될 수 있습니다. 예를 들어, 가격 동적 조정에 대한 의사 결정을 모델화하거나, 소비자 행동에 따른 특정 조치를 최적화하는 데 활용할 수 있습니다. 강화 학습은 시장에서의 상호 작용을 모델링하고 최적화하는 데 도움이 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fifty-montana",
      "metadata": {
        "id": "fifty-montana"
      },
      "source": [
        "## 2. Paired Data\n",
        "\n",
        "### 2-1. Paired data를 알아보자\n",
        "\n",
        "**Q. Paired data란 무엇일까?**\n",
        ">입력과 출력/정답이 하나의 짝을 이룬 데이터\n",
        "\n",
        "### 2-2. Image Classifier\n",
        "\n",
        "**Q. 사람이 구분하기 어렵지만, 인공지능이 구분할 수 있는 image classifier 예시**\n",
        ">근접한 색상 분류, 텍스터, 동물의 종류, 문서의 글꼴, 식물이나 꽃의 종류 등\n",
        "\n",
        "\n",
        "### 2-3. Object Detection/Tracking\n",
        "\n",
        "**Q. Object detection을 활용한 서비스**\n",
        ">보행자 수 점검, 마스크 착용 여부 확인, 얼굴 표정 분류, 교통정보 모니터, 배송추적, 보안 시스템, 상품 인식, 기후 모니터 등\n",
        "\n",
        "### 2-4. Image Segmentation\n",
        "\n",
        "**Q. 용종검출에서 Object detection이 아닌, Image segmentation 기법이 활용되는 이유**\n",
        ">용종의 모양이 일정하지 않으니까! Image segmentation은 Object Detection의 바운딩 박스보다 더 정확하게 비정형의 도형으로 Object를 찾아서 학습. 따라서, 정확도가 중요한 헬스케어 분야에서 Image segmentation 기법이 활용. 쉽게 말하자면, Object Detection은 주로 물체 주위를 감싸는 직사각형 박스(바운딩 박스)를 사용하여 물체를 찾는 기술입니다. 이것은 대략적으로 어디에 물체가 있는지 알려줄 수 있지만, 정확한 물체의 모양과 경계를 제공하지는 않습니다. 반면에 Image segmentation은 물체의 경계를 픽셀 수준에서 정확하게 식별하고 분할합니다. 이것은 더 정확한 모양과 크기의 물체를 식별할 수 있도록 도와줍니다. 의료 분야에서는 정확한 이미지 분할이 매우 중요합니다. 예를 들어, 종양 또는 병변의 정확한 위치와 크기를 식별하려면 Image segmentation이 필요합니다. 이렇게 정확한 정보는 의료 전문가에게 중요한 진단 및 치료 결정을 내릴 때 도움이 됩니다. 요약하면, Image segmentation은 Object Detection보다 정교하고 정확하게 이미지에서 물체를 식별하는 기술이며, 이는 특히 의료 분야와 같이 정확한 결과가 필요한 분야에서 매우 유용하게 활용됩니다.\n",
        "\n",
        "\n",
        "### 2-5. Pose Estimation\n",
        "\n",
        "**Q. Pose Estimation 활용 방안**\n",
        ">Pose Estimation은 이미지나 비디오에서 사람 또는 물체의 자세를 파악하는 기술입니다. 자세란 물체 또는 사람의 신체 부위의 위치와 방향을 의미합니다. Pose Estimation은 주로 컴퓨터 비전 및 머신 러닝 기술을 사용하여 실시간으로 물체나 사람의 자세를 추정하고 이를 숫자로 나타내거나 시각적으로 보여줍니다.예를 들어, 사람의 Pose Estimation은 사람의 머리, 어깨, 팔, 다리 및 다른 신체 부위의 위치와 방향을 추정합니다. 이것은 컴퓨터 비전 시스템에서 인간의 동작 분석, 모션 캡처, 게임 개발, 로봇 제어, 보안 및 의료 분야에서 유용하게 활용됩니다. Pose Estimation은 실제 시나리오에서 사람의 동작을 추적하거나 로봇이 환경에서 물체를 조작할 때 도움이 됩니다. 간단히 말해, Pose Estimation은 이미지나 비디오에서 물체 또는 사람의 자세를 이해하고 추정하는 기술로, 다양한 응용 분야에서 활용됩니다. Pose Estimation이 활용 방안으로는 수어 번역, 재활치료, 보안, 운동 훈련(프로 스포츠 선수의 자세를 학습하여 자세 교정을 받을 수 있는 서비스: 골프, 주짓수..) 등이 있습니다.\n",
        "\n",
        "### 2-6. Paired Data의 활용\n",
        "\n",
        "**Q. 인공지능 서비스를 찾아서 어떠한 기술을 활용하였는지**\n",
        ">CS나 챗봇. 고객지원을 비롯한 다양한 응대, 문제 해결 도우미, 구글 번역기 같은 기계 번역, 공항에서 얼굴 인식, 음성 비서, 등 그리고 클로바노트,\n",
        "음성 기록을 텍스트로 바꾸어 주는 어플. 인식한 음성으로 화자로 나누는 '참석자 목소리 구분'과 음성을 텍스트로 변환하는 인공지능 기술을 활용\n",
        "\n",
        "### 2-7. 우리가 가진 Paired Data에는 무엇이 있을까\n",
        "\n",
        "**Q. 우리가 가진 paired data에는 무엇이 있을까요?**\n",
        ">립스틱의 불량 상품, 이력서 합격/불합격 여부, 언어 특히 다국어 학습, 감정 인식, 시간이나 일정 관리, 인간의 상호 작용 등\n",
        "\n",
        "### 2-8. Paired Data 만들기: [TeachableMachine](https://teachablemachine.withgoogle.com/train/image)\n",
        "\n",
        "**Q. TeachableMachine으로 만들 수 있는 분류기를 무엇이 있을까?**\n",
        ">A. 이미지, 음성인식, 이상치, 예측이나 감지 분류 등\n",
        "\n",
        "### Q. Image Classification / Object Detection / Image Segmentation / Pose Estimation\n",
        "\n",
        "1. 드론 이미지 처리: 드론촬영 이미지에서 특정지형이나 건물을 분리 - Image Segmentation\n",
        "2. 고기 이미지를 바탕으로 등급 판별하기: 특정 등급의 고기와 그렇지 않은 고기를 구분 - Image Classification\n",
        "3. 게임 인터페이스 : 사용자의 움직임을 감지하여 게임 컨트롤러로 사용 - Pose Estimation\n",
        "4. 차량 번호판 인식: 주차관리, 교통위반 감지 등을 위해 차량 번호판을 탐지 - Object Detection\n",
        "5. 보안감시 : CCTV 등에서 움직이는 사람이나 물체를 탐지하여 보안위반을 감지 - Object Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "assisted-region",
      "metadata": {
        "id": "assisted-region"
      },
      "source": [
        "## 3. Unpaired Data\n",
        "\n",
        ">Unpaired Data는 입력과 출력 간의 정확한 대응이 없는 데이터 세트입니다. 다시 말하면, 각 입력 데이터에 대응되는 출력 데이터가 없거나 매핑이 명확하지 않을 수 있습니다. Unpaired Data는 어떤 입력이 주어졌을 때 모델이 어떤 출력을 생성해야 하는지를 학습하는 데 도움이 필요한 경우에 사용됩니다. Unpaired Data는 주로 이미지 변환 및 스타일 변환 작업에서 활용됩니다. 예를 들어, 한 스타일의 이미지를 다른 스타일의 이미지로 변환하는 작업에서, 입력 이미지와 대응 출력 이미지의 쌍을 수집하는 것이 어려운 경우, Unpaired Data를 사용하여 모델을 학습할 수 있습니다. Unpaired Data는 생성적 적대 신경망(GAN)과 같은 모델에서 유용하게 활용됩니다. 요약하면, Unpaired Data는 입력과 출력 간의 정확한 대응이 없는 데이터로, 일부 작업에서 데이터 수집이 어려울 때 유용한 것으로, 머신 러닝 모델을 학습하고 생성하는 데 활용됩니다.\n",
        "\n",
        "### 3-1. Unpaired Data는 무엇인가\n",
        "\n",
        "**Q. Paired data와 Unpaired Data의 차이**\n",
        ">Paired data는 정답(label)이 있습니다. Unpaired data는 정답은 없지만 정답 그룹의 데이터가 있어요. 즉, Paired data는 종종 짝지어진 관측값 간의 차이에 중점을 두는 반면, Unpaired data는 두 그룹 간의 차이에 중점을 둡니다.\n",
        "\n",
        "### 3-2. Unpaired Data의 활용_영상과 음성\n",
        "\n",
        "**Q. Unpaired Data를 사용한 예**\n",
        ">일반 도로 환경에서 자율 주행 자동차 모델을 학습한 후, 다른 지역의 도로에서도 작동하도록 도메인 간의 차이를 보상,  원하는 목소리 또는 음악 스타일을 가진 Unpaired 데이터를 활용하여 음성이나 음악을 생성하거나 변조 등. Universal Music Translation, Deepfake 등. Deepfake 기술은 주로 딥 러닝과 생성적 적대 신경망(GAN)을 기반으로 작동하며, 학습 데이터에 Unpaired 데이터를 사용하여 얼굴 특징, 표현, 그림자 등을 모방. 이 기술은 예술, 엔터테인먼트, 광고, 특수 효과, 그래픽 디자인 및 다른 분야에서 활용. 그러나 또다른 중요한 측면은 Deepfake 기술의 윤리적 및 법적 고려사항.\n",
        "\n",
        "### 3-3. 우리가 가진 Unpaired Data는 무엇이 있을까\n",
        "\n",
        "**Q. 우리가 가진 Unpaired Data는 무엇이 있을까**\n",
        ">일상 대화, 사진, 동영상, 지리 정보 등\n",
        "\n",
        "### 3-4. U-GAT-IT을 이용한 애니메이션 프사 만들기\n",
        "\n",
        "**Prep env**\n",
        "```\n",
        "import matplotlib.image as img\n",
        "import matplotlib.pyplot as plt\n",
        "import os, random\n",
        "\n",
        "def show_n_images_from_dir(dir_path, n, shuffle=True):\n",
        "    file_list = os.listdir(dir_path)\n",
        "    if shuffle:\n",
        "        random.shuffle(file_list)\n",
        "        \n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    for idx, file in enumerate(file_list):\n",
        "        if idx >= n: break\n",
        "        img_path = os.path.join(dir_path, file)\n",
        "        plt.subplot(1,n,idx+1)\n",
        "        plt.title(file)\n",
        "        plt.imshow(img.imread(img_path))\n",
        "\n",
        "# 실제 인물 예제\n",
        "img_dir_A = os.path.join('dataset', 'selfie2anime', 'trainA')\n",
        "show_n_images_from_dir(img_dir_A, 5)\n",
        "\n",
        "# 애니메이션 캐릭터 예제\n",
        "img_dir_B = os.path.join('dataset', 'selfie2anime', 'trainB')\n",
        "show_n_images_from_dir(img_dir_B, 5)\n",
        "```\n",
        "\n",
        "**Fitting data after upload img**\n",
        "```\n",
        "import demo\n",
        "import matplotlib.pyplot as plt\n",
        "from utils import *\n",
        "gan = demo.gan\n",
        "\n",
        "plt.figure(figsize=(10, 20))\n",
        "\n",
        "def show_my_animated_images(img_count=5):\n",
        "    for n, (real_A, _) in enumerate(gan.test_mine_loader):\n",
        "        real_A = real_A.to(gan.device)\n",
        "        fake_A2B, _, _ = gan.genA2B(real_A)\n",
        "\n",
        "        if n >= img_count: break\n",
        "        plt.subplot(img_count,2,2*n+1)\n",
        "        plt.title('original')\n",
        "        plt.imshow(tensor2numpy(denorm(real_A[0])))\n",
        "\n",
        "        plt.subplot(img_count,2,2*n+2)\n",
        "        plt.title('generated')\n",
        "        plt.imshow(tensor2numpy(denorm(fake_A2B[0])))\n",
        "\n",
        "# 이미지 변환을 수행합니다!\n",
        "# 업로드한 이미지 갯수로 바꿔주세요.\n",
        "show_my_animated_images(2)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "popular-alexandria",
      "metadata": {
        "id": "popular-alexandria"
      },
      "source": [
        "## 4. 강화학습 Reinforcement Learning\n",
        "\n",
        ">강화학습(Reinforcement Learning)은 컴퓨터 프로그램이 환경과 상호작용하며 원하는 목표를 달성하기 위해 학습하는 머신 러닝의 한 분야입니다. 강화학습의 목표는 에이전트가 최적의 정책을 학습하여 보상을 최대화하는 행동을 선택할 수 있도록 하는 것입니다. 에이전트는 환경과 상호작용하며 시행착오를 통해 보상을 최대화하는 방법을 배우게 됩니다. 이러한 개념은 게임 플레이, 자율 주행 자동차, 로봇 제어, 금융 거래 등 다양한 응용 분야에서 사용됩니다.\n",
        "\n",
        "* 에이전트(Agent): 강화학습에서는 프로그램 또는 로봇을 \"에이전트\"라고 부릅니다. 이 에이전트는 환경에서 특정 작업을 수행하려고 노력합니다.\n",
        "* 환경(Environment): 에이전트가 작업을 수행하는 공간 또는 상황을 \"환경\"이라고 합니다. 환경은 에이전트가 상호작용하고 배우는 곳입니다.\n",
        "* 상태(State): 환경은 특정 시점에서 에이전트에게 어떤 정보를 제공합니다.  \n",
        "    이 정보를 \"상태\"라고 부릅니다. 상태는 에이전트가 어디에 있는지, 어떤 조건에 있는지 등을 설명합니다.\n",
        "* 행동(Action): 에이전트는 현재 상태에 따라 특정 행동을 선택합니다. 이 행동은 에이전트가 환경에 적용하는 작업을 나타냅니다.\n",
        "* 보상(Reward): 환경은 에이전트가 한 행동에 대한 보상을 제공합니다.  \n",
        "    이 보상은 행동이 얼마나 좋거나 나쁜지를 나타냅니다. 에이전트의 목표는 보상을 최대화하는 방법을 학습하는 것입니다.\n",
        "* 정책(Policy): 에이전트는 어떤 상태에서 어떤 행동을 선택할지 결정하기 위한 \"정책\"을 가집니다. 정책은 학습된 전략 또는 규칙을 의미합니다.\n",
        "\n",
        "\n",
        "### 4-1. 강화학습의 예\n",
        "\n",
        "**Q. 강화학습의 예**\n",
        ">특수교육에서 개별화 학습 지도, 환자 관리나 치료, 자동화 생산에 사용되는 로봇 제어, 자율주행 자동차 등\n",
        "\n",
        "### 4-2. 지도학습 vs 강화학습\n",
        "\n",
        "**Q. 강화학습의 장점**\n",
        ">자동화 된/실시간 의사 결정, 시행착오를 통한 학습, 유연성 등. 학습 프로세스의 불안정성과 샘플 효율성에 대한 도전과제도 있으며, 데이터, 모델 선택 및 하이퍼파라미터 튜닝과 관련된 작업이 포함.\n",
        "\n",
        "### 4-3. 강화학습 더 깊게 파악하기\n",
        "\n",
        "**Q. 강화학습을 할 때 중요한 3가지 요소**\n",
        ">입력: State(환경 정보), 출력: Action(행동), Reward(보상).\n",
        "\n",
        "**Q. 강화학습은 어떻게 이루어지나요?**\n",
        ">에이전트와 환경의 상호작용 > 정책/전략 > 보상 > 상태 확인 > 강화학습 알고리즘 > 학습 > 탐험이나 활용 > 학습 종료 > 평가. Agent가 Action을 했을 때 reward를 받습니다. Agent는 그 reward를 보고 다음 state에서 더 많은 reward를 받을 수 있는 Action을 하도록 학습합니다.\n",
        "\n",
        "### 4-4. 강화학습 요소(1) 슈퍼마리오로 파악해보자\n",
        "\n",
        "**Q. 강화학습에서 State, Action, Reward가 왜 중요한가요?**\n",
        ">State, Action, Reward를 어떻게 설계하느냐에 따라 agent의 action이 변하고, 그로 인해 강화학습의 결과도 달라지기 때문입니다. 더 좋은 설계를 할수록 강화학습의 결과가 더 좋아집니다. 즉, 에이전트는 상태 정보를 기반으로 행동을 선택하고, 이로부터 받은 보상을 통해 행동을 개선하며, 시간이 지남에 따라 최상의 정책을 학습합니다. 이러한 프로세스를 통해 에이전트는 주어진 환경에서 목표를 달성하기 위한 최적의 전략을 찾습니다.\n",
        "\n",
        "### 4-5. 강화학습 요소(2) 자율주행자동차로 파악해보자\n",
        "\n",
        "**Q. 원하는 목적지로 갈 수 있도록 안전하게 주행하는 자율주행자동차를 만들기 위해 강화학습을 하려고 합니다. 이 강화학습의 State, Action, Reward를 찾아 보세요.**\n",
        ">차량의 색상, 위치, 속도는 State. 주행 속도의 가속이나 감속 또는 정지 그리고 핸들의 동작 등은 Action, 차량의 색상에 따른 목적지로 이동 여부 그리고 교통법규를 준수했는지 여부 등에 따라 Reward\n",
        "\n",
        "### 4-6. 강화학습 요소(3) 양궁로봇으로 파악해보자\n",
        "\n",
        "**Q. 양궁로봇의 만들기 위한 강화학습의 State, Action, Reward는 어떻게 설정하면 좋을까요?**\n",
        ">목표 (표적)의 위치, 활을 잡는 손의 위치 및 자세, 바람의 세기 및 방향, 이전 화살의 목표와의 거리 등이 State, 활을 높이거나 낮추는 동작, 활의 높이 각도를 조절하는 동작, 활을 놓치는 방향으로 이동하거나 회전하는 동작, 활을 당기거나 놓는 동작 등이 Action, 목표 지점에 활이 맞았거나 특정 점수에 도달했을 때 Reward\n",
        "\n",
        "### 4-7. 강화학습, 어디에 활용할까\n",
        "\n",
        "**Q. 강화학습이 활용되면 좋지 않은 경우에는 어떤 것이 있을까요?**\n",
        ">수술이나 자율주행 자동차와 같이 자칫 사람이 위험한 상황에 빠지게 될 수 있는 경우라면 좋지 않습니다. 무수히 많은 state를 학습시키지 않고는 사람의 생명이 위태로울 수 있습니다. 잘못된 보상 함수는 원치않는 행동을 유도하거나 원하는 목표와는 다른 방향으로 학습을 이끌 수 있어요. 현실 환경을 정확하게 모델링하는 것은 어렵고 복잡합니다. 윤리적 문제. 강화학습을 사용하기 전에 문제의 복잡성, 데이터 요구, 보상 설계, 안정성 등을 신중하게 고려해야 합니다.\n",
        "\n",
        "**Q. 강화학습을 적용할 수 있는 응용분야에는 무엇이 있을까요?**\n",
        ">광고 레이아웃 결정, 동영상 Thumbnail 고르기, 자원관리나 환경 보전, 환자의 진단이나 치료 계획, 로보틱스 등\n",
        "\n",
        "### Q. 체스 게임을 위한 강화학습의 State, Action, Reward는 어떻게 설정하면 좋을까요?\n",
        "\n",
        "* State(상태): 체스 게임에서 상태는 현재 체스 보드의 배치와 모든 말의 위치를 나타냅니다.  \n",
        "    이것은 에이전트가 현재 게임 상황을 이해하고 그 다음 행동을 선택하는 데 사용되는 정보입니다.  \n",
        "* Action(행동): 행동은 에이전트가 게임에서 취할 수 있는 다양한 움직임 또는 수를 나타냅니다.  \n",
        "    예를 들어, 폰을 전진시키거나 룩을 이동시키는 등의 체스 게임 내의 허용된 움직임이 행동에 해당합니다.  \n",
        "* Reward(보상): 보상은 에이전트의 각 행동에 대한 피드백으로, 게임의 결과와 에이전트의 목표에 따라 다를 수 있습니다.  \n",
        "    보상은 에이전트의 행동이 얼마나 좋은지 또는 나쁜지를 나타내며, 게임에서 이긴 경우 양의 보상을 받고 패배한 경우 음의 보상을 받을 수 있습니다.  \n",
        "    중요한 목표는 무한한 게임 시나리오에서 최종 보상을 최대화하는 것입니다.  \n",
        "\n",
        ">에이전트는 현재 게임 상태를 관찰하고 가능한 다양한 움직임(행동) 중 하나를 선택합니다. 그런 다음 선택한 행동에 따른 보상을 받고 게임이 진행됩니다. 강화 학습 에이전트의 목표는 게임을 플레이하면서 최대 보상을 얻을 수 있는 정책(policy)을 학습하는 것입니다. 정책은 상태에 따라 어떤 행동을 선택할지를 결정하는 전략입니다. 이를 통해 에이전트는 게임을 플레이하며 더 나은 움직임을 찾고 최종 보상을 최대화하기 위해 학습합니다. 강화 학습은 게임 분야에서 많이 사용되며, 체스와 같은 전략 보드 게임에서 게임 플레이를 향상시키는 데 활용될 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "protected-alpha",
      "metadata": {
        "id": "protected-alpha"
      },
      "source": [
        "## 5. 의미없는 데이터를 의미있게 만들기\n",
        "\n",
        "### 5-1. 내 안에 존재하는 Paired Data\n",
        "\n",
        "**Q. 데이터가 Paired data도, Unpaired Data도 아닌 경우, 데이터에서 Paired data를 추출할 수 있습니다. 영상 속 예시로 제시된 사진 속에 숨겨져 있는 label은 무엇일까요?**\n",
        ">진짜 사진\n",
        "\n",
        "### 5-2. 인공지능으로 얼굴 만들기 Generator와 Discriminator의 학습\n",
        "\n",
        "**Q. White noise는 무엇인가요?**\n",
        ">머신러닝에서 noise는 데이터에서의 무작위 또는 시스템적인 불확실성을 나타내는 용어로서 모델 학습 및 예측 프로세스에서 데이터의 정확성과 신뢰성을 감소시키는 요소로 작용합니다. 즉, 모든 주파수의 성분을 가지고 있어요. White noise는 무분별, 무작위의 예측하기 어려운 일종의 신호나 패턴으로서 시간의 흐름에 따라 아무런 패턴이 남아있지 않은 경우나 의미 없는 값들의 연속 또는 무작위한 진동을 나타냅니다.\n",
        "\n",
        "\n",
        "**noise 유형**\n",
        "\n",
        "* 데이터 노이즈: 데이터 수집 과정에서 발생하는 무작위 오류 또는 불확실성. 센서 장비의 정확성 문제, 환경 변인, 측정 오차, 휴먼 에러 등.\n",
        "\n",
        "* 모델 노이즈: 모델은 실제 데이터와 일치시키려고 시도하지만 모든 관계를 완벽하게 잡아내지 못할 수 있으므로(모델 자체의 한계) 모델 예측에는 오차가 포함.\n",
        "\n",
        "* 측정 노이즈: 측정 노이즈는 모델의 입력 데이터에 대한 노이즈, 입력 데이터 자체에 오류가 있을 수 있음(입력 데이터의 정확성 제한).\n",
        "\n",
        "\n",
        "**Q. 영상에서 설명하고 있는 알고리즘은 무엇인가요?**\n",
        ">GAN은 \"Generative Adversarial Network\"의 약자로, 생성적 적대 신경망이라고 불립니다. 이는 머신 러닝과 딥 러닝의 하위 분야로, 물체나 이미지 등을 생성하거나 변형하는데 사용되는 모델입니다. GAN은 Generator와 Discriminator 두 개의 신경망이 서로 경쟁하며 학습하는 구조를 가지고 있습니다. GAN의 핵심 아이디어는 생성자와 판별자가 서로를 무시하면서 경쟁하며 성장하고, 결과적으로 생성자는 더 실제와 비슷한 이미지를 만들어내게 되고, 판별자는 생성자가 만든 이미지와 실제 이미지를 더 잘 구별하게 됩니다. 이 경쟁과 상호 작용을 통해 GAN은 점차적으로 높은 품질의 이미지 생성을 달성하거나 데이터 변형을 수행할 수 있게 됩니다. 간단히 말하면, GAN은 생성자와 판별자가 서로 경쟁하여 더 나은 결과물을 만들게 되는 모델로, 주로 이미지 생성, 스타일 변환, 이미지 개선 등의 작업에 활용됩니다.\n",
        "\n",
        "```\n",
        "Generator (생성기):  \n",
        "Generator는 무작위 노이즈 또는 입력 데이터를 사용하여 가짜 데이터를 생성하는 역할을 합니다. 이것은 원본 데이터와 유사한 분포를 가지는 데이터를 생성하려는 것이 목표입니다. Generator는 처음에는 랜덤한 노이즈에서 시작하며, 학습 과정을 통해 더 실제와 유사한 데이터를 생성하는 능력을 향상시킵니다.\n",
        "\n",
        "Discriminator (판별기):  \n",
        "Discriminator는 주어진 데이터가 실제 데이터인지 (원본 데이터) 또는 가짜 데이터인지를 구분하는 역할을 합니다. 이것은 이진 분류기 역할을 하며, 가짜 데이터를 생성한 Generator로부터 실제 데이터와 구별하기 위해 학습됩니다. Discriminator는 가짜 데이터와 실제 데이터를 받고 두 가지 클래스로 분류합니다. (예: \"가짜\" 및 \"진짜\").\n",
        "```\n",
        "\n",
        "**Q. Generator와 Discriminator의 역할은 무엇인가요?**\n",
        ">GAN(Generative adversarial network)의 학습 프로세스는 Generator와 Discriminator 간의 경쟁으로 진행됩니다. Generator는 더 실제와 유사한 데이터를 생성하려고 노력하며, Discriminator는 그 데이터를 판별하려고 노력합니다. 이러한 경쟁은 두 네트워크가 서로를 개선하고 결국 Generator가 높은 품질의 데이터를 생성하도록 하게 됩니다.\n",
        "\n",
        "### 5-3. 텍스트 분석을 위한 데이터 변환\n",
        "\n",
        "**Q. ‘웜테일은 가늘게 ( ) 목소리로 재빨리 말했다’와 같이 맞출 단어에 마스크를 씌우고, 마스크의 위치를 변경하면서 학습시키는 모델은 무엇인가요?**\n",
        ">언어 모델(language model)은 자연어 처리 분야에서 사용되는 중요한 컴퓨터 프로그램 또는 인공 신경망 아키텍처 입니다. 언어에 대한 이해와 생성을 컴퓨터가 수행할 수 있게 하는 머신 러닝 모델(통계적 모델)입니다. 간단하게 설명하면, 언어 모델은 언어의 패턴과 규칙을 학습하여 문장을 이해하고 새로운 문장을 생성할 수 있도록 도와줍니다.\n",
        "\n",
        "### 5-4. 언어모델 활용해 보기: [Hugging Face](https://huggingface.co/tasks/text-generation)를 이용한 GPT 모델 실습\n",
        "\n",
        "**CASE 1. GPT : 다음 문장에 이어질 단어는 무엇일까요?**\n",
        ">사람처럼 유창한 글을 쓰는 것으로 널리 알려진 GPT 모델은 텍스트 생성 모델 중 가장 대표적인 것입니다. 이 모델의 학습 데이터는 주어진 문장 다음에 오기에 가장 적당한 단어를 맞추는 방식으로 구성. ex) 웜테일은 가늘게 떨리는 목소리로 재빨리 __.\n",
        "\n",
        "\n",
        "**CASE 2. BERT : 다음 빈칸에 알맞은 단어는 무엇일까요?**\n",
        ">BERT는 텍스트의 의미를 정확하게 분석하는 작업에 널리 활용되는 가장 대표적인 언어 모델입니다. 이 모델의 학습 데이터는 텍스트에서 중간 위치를 빈칸으로 가려놓고 그게 무엇인지 맞추는 방식으로 구성. ex) 웜테일은 가늘게 __ 목소리로 재빨리 말했다.\n",
        "\n",
        "### 5-5. Paired data를 만들어보자 Everybody Dance Now로 도출하는 데이터\n",
        "\n",
        "**Q. Everybody Dance now로 paired data를 도출하는 방법을 설명해 보세요.**\n",
        ">비디오 데이터 수집 > 제너레이터로 영상 생성 > 제너레이터 학습 > 원본 데이터와 테스트 데이터를 합성, 일반인의 동작과 댄서의 동작 페어링\n",
        "\n",
        "### 5-6. Final\n",
        "\n",
        ">인공지능은 Paired Data, Unpaired Data, 강화학습을 종합적으로 사용하여, 정답 데이터나 정답 그룹이 부족한 상황에서도 다양한 작업을 수행하고 의미 있는 결과를 얻을 수 있습니다. 그러나 인간을 완전히 대체하는 데에는 여전히 한계가 존재하며, 윤리, 창의성, 판단력 등 인간의 고유한 능력을 모방하기는 어려울 것입니다.\n",
        "\n",
        "### Q. 언어모델의 의미는 무엇이며, GPT 학습 방식을 지도학습 / 비지도 학습 / 강화 학습과 관련하여 설명하세요.\n",
        "\n",
        ">언어 모델(Language Model)은 자연어 처리(Natural Language Processing, NLP) 분야에서 텍스트의 확률 분포를 모델링하는 인공지능 모델입니다. 이 모델은 주어진 문맥에서 다음 단어나 문장의 확률을 예측하는 데 사용됩니다. 언어 모델은 이전 텍스트 데이터를 기반으로 문법, 의미, 맥락 등을 이해하고 다음 단어를 생성하거나 텍스트를 분석하는 작업을 수행합니다. GPT (Generative Pre-trained Transformer)는 언어 모델의 한 종류로, 특히 비지도 학습(Unsupervised Learning) 방식으로 학습된 모델입니다. 요약하면, GPT는 비지도 학습을 통해 언어 이해 능력을 학습하고, Fine-Tuning을 통해 지도 학습 방식을 사용하여 특정 NLP 작업에 적용됩니다. 강화 학습은 GPT와 관련된 작업에서 선택적으로 사용될 수 있습니다. 다음은 GPT의 학습 방식과 관련된 설명입니다:\n",
        "\n",
        "* 비지도 학습 (Unsupervised Learning): GPT 모델은 대규모 텍스트 데이터를 사용하여 사전 훈련(pre-training)됩니다. 이때, 모델은 레이블이 없는 텍스트 데이터를 입력으로 받아 문법, 의미, 문맥, 문장 구조, 단어 간의 관계 등을 학습합니다. 즉, 모델은 단어와 문장의 패턴을 파악하고 언어 이해 능력을 향상시킵니다. 이 단계에서 GPT는 다양한 언어 처리 작업에 대해 범용적으로 사용될 수 있는 언어 표현을 습득합니다.\n",
        "\n",
        "* Fine-Tuning (미세 조정): GPT는 비지도 학습을 통해 학습된 후, 특정 NLP 작업을 위해 Fine-Tuning 단계를 거칩니다. 이때, 지도 학습(Supervised Learning) 방식을 사용할 수 있습니다. 예를 들어, GPT 모델을 특정 언어 이해 작업 (예: 질문 응답, 기계 번역, 텍스트 분류)에 맞게 조정하여 해당 작업을 수행하도록 합니다. 레이블이 있는 훈련 데이터를 사용하여 모델을 특정 작업에 맞게 개조하고 최적화합니다.\n",
        "\n",
        "* 강화 학습 (Reinforcement Learning): GPT 모델은 일반적으로 강화 학습 방식을 직접적으로 사용하지는 않습니다. 그러나 강화 학습과 관련된 작업에서 GPT를 활용할 수 있습니다. 예를 들어, GPT를 텍스트 기반의 강화 학습 환경에서 에이전트로 활용하여 특정 작업을 수행하도록 조작할 수 있습니다. 이 경우, GPT는 문맥을 이해하고 보상을 최대화하기 위한 텍스트 기반의 의사 결정을 내릴 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "proved-ferry",
      "metadata": {
        "id": "proved-ferry"
      },
      "source": [
        "## 6. Convolution\n",
        "\n",
        "### 6-1. 합성 Convolution\n",
        "\n",
        "**Q. CNN은 Neural Network를 어디에 특화시킨 것인가요?**\n",
        "> 이미지, 영상 처리\n",
        "\n",
        "**Q. Convolution은 +, x, shift의 조합으로 된 복잡한 연산입니다. [1 2 3] * [1 1 1]의 값은 무엇인가요?**\n",
        "> [1 3 6 5 3]\n",
        "\n",
        "```\n",
        "      1 2 3 >>> 0\n",
        "1 1 1\n",
        "```\n",
        "```\n",
        "    1 2 3   >>> 1*1=1\n",
        "1 1 1\n",
        "```\n",
        "```\n",
        "  1 2 3     >>> 1*1+2*1=3\n",
        "1 1 1\n",
        "```\n",
        "```\n",
        "1 2 3       >>> 1*1+2*1+3*1=6  \n",
        "1 1 1\n",
        "```\n",
        "```\n",
        "1 2 3       >>> 2*1+3*1=5\n",
        "  1 1 1\n",
        "```\n",
        "```\n",
        "1 2 3       >>> 3*1=3\n",
        "    1 1 1\n",
        "```\n",
        "```\n",
        "1 2 3       >>> 0\n",
        "      1 1 1\n",
        "```\n",
        "\n",
        "### 6-2. convolution_mask\n",
        "\n",
        "**Q. A, B, C, D, E 영역의 Convolution 값은 무엇인가요?**\n",
        "> A, B, C, D, E 순으로 0, 3. 3, 0, 0\n",
        "\n",
        "**Q. 전체 이미지의 Convolution 값은 무엇인가요?**\n",
        ">\n",
        "\n",
        "### 6-3. 영상에서의 convolution\n",
        "\n",
        "**Q. 영상에서 mask(혹은 filter)를 Convolution하는 것의 의미는 무엇인가요?**\n",
        ">영상의 특징(세로선, 가로선, 대각선 등)을 찾을 수 있습니다.\n",
        "\n",
        "**Q. Convolution 연산자의 특징은 무엇인가요?**\n",
        ">Convolution 연산자는 결합법칙이 성립합니다. Convolution 연산자(Convolution Operator)는 신경망과 신호 처리에서 중요한 개념으로, 주로 이미지 처리와 시계열 데이터 분석에서 사용됩니다. Convolution 연산은 합성곱 신경망(Convolutional Neural Network, CNN)의 핵심 요소로 사용되며, 이미지 분류, 객체 감지, 얼굴 인식, 자율 주행 자동차 및 음성 처리와 같은 다양한 응용 분야에서 활발하게 활용됩니다. Convolution 연산자의 주요 특징은 다음과 같습니다.\n",
        "\n",
        "* 패턴 탐지 및 특징 추출: Convolution은 주로 패턴을 탐지하고 데이터로부터 중요한 특징을 추출하는 데 사용됩니다. 이미지 처리에서는 Convolution 연산을 사용하여 에지(edge), 질감(texture), 객체 등의 시각적 패턴을 감지합니다.\n",
        "\n",
        "* 가중치 공유: Convolution 연산에서는 가중치(weight)를 여러 위치에 공유합니다. 이것은 학습 파라미터의 수를 크게 줄이고, 모델을 공간적 불변성을 갖도록 만들어줍니다. 예를 들어, 이미지에서 어떤 특징이 각 위치에 독립적으로 나타나는 것이 아니라 다양한 위치에서 유사한 패턴을 공유한다면, Convolution 연산을 사용하여 이를 효과적으로 모델링할 수 있습니다.\n",
        "\n",
        "* 차원 축소: Convolution 연산은 입력 데이터의 공간 차원을 축소합니다. 이는 모델의 계산 복잡성을 줄이고, 계층을 쌓는 데 유용합니다. 예를 들어, 2D 이미지의 경우, 입력 이미지의 공간 차원을 점점 축소하면서 중요한 정보를 추출할 수 있습니다.\n",
        "\n",
        "* 학습 가능한 필터: Convolution 연산의 필터(또는 커널)는 학습 가능한 가중치로 초기화되며, 역전파(backpropagation)를 통해 모델의 학습 과정 중에 조정됩니다. 이로써 모델은 주어진 작업에 가장 적합한 특징을 학습하게 됩니다.\n",
        "\n",
        "* 다양한 차원 및 채널에 적용 가능: Convolution 연산은 다양한 데이터 형식에 적용할 수 있습니다. 1D Convolution은 시계열 데이터에, 2D Convolution은 이미지에, 3D Convolution은 볼륨 데이터나 동영상에 사용됩니다. 또한 Convolution 연산은 다중 채널(예: RGB 이미지)에도 적용할 수 있으며, 필터의 수를 조절하여 다양한 특징을 추출할 수 있습니다.\n",
        "\n",
        "### 6-4. 영상 분석의 기초\n",
        "\n",
        "**Q. 서로 다른 크기의 영상 feature를 분석하려면 서로 다른 크기의 마스크가 필요합니다. 크기가 큰 꼭지점을 찾으려면 어떤 크기의 마스크가 필요할까요?**\n",
        ">크기가 큰 꼭지점을 찾으려면 일반적으로 큰 마스크(필터)를 사용해야 합니다. 이러한 마스크는 큰 영역을 탐지하고 꼭지점과 같은 주변 환경에서 특징을 추출할 수 있어야 합니다. 크기가 큰 꼭지점을 찾는 데 사용되는 일반적인 마스크 중 하나는 \"Laplacian of Gaussian (LoG)\" 필터입니다. LoG 필터는 라플라시안(Laplacian) 필터와 가우시안(Gaussian) 필터를 결합한 것으로, 큰 특징을 감지하고 노이즈를 감소시키는 데 사용됩니다. 이 필터는 주변 환경에서 밝기의 변화를 확인하고 이러한 밝기 변화가 꼭지점과 관련이 있는지 판단할 수 있습니다. 다른 방법으로, 크기가 큰 꼭지점을 찾는 데 스케일 스페이스(scale-space) 방법을 사용할 수 있습니다. 스케일 스페이스는 이미지에서 다양한 크기의 특징을 동시에 감지하는 기술로, 이미지를 서로 다른 크기로 반복적으로 스무딩(smoothing)하여 특징을 찾습니다. 이러한 스케일 스페이스 분석을 통해 큰 꼭지점을 탐지할 수 있습니다. 요약하면, 크기가 큰 꼭지점을 찾으려면 큰 마스크 또는 스케일 스페이스 기술을 사용하여 이미지의 다양한 크기에서 특징을 추출하고 분석해야 합니다.\n",
        "\n",
        "### 6-5. 서로 다른 크기의 feature 만들기\n",
        "\n",
        "**Q. 서로 다른 크기의 영상 feature를 만드는 두 가지 방법은 무엇인가요?**\n",
        "* 첫 번째 방법 - 필터 사이즈를 점점 크게 만드는 방법: 이 방법은 이미지에 크기가 다른 필터를 적용하여 서로 다른 크기의 특징을 생성하는 것입니다. 예를 들어, 작은 필터(3x3)로 이미지를 스캔하여 작은 세부 특징(작은 엣지)를 찾고, 그 다음 큰 필터(5x5)로 이미지를 스캔하여 더 큰 특징(대표적인 모양)를 찾습니다. 이런 식으로 필터 크기를 증가시켜 다양한 크기의 특징을 얻을 수 있습니다.\n",
        "\n",
        "* 두 번째 방법 - 영상 사이즈를 점점 작게 만드는 방법: 이 방법은 이미지의 크기를 축소하면서 서로 다른 스케일에서 특징을 추출하는 것입니다. 예를 들어, 처음에는 원본 이미지를 사용하여 특징을 찾고, 그 다음 이미지를 절반 크기로 축소한 후에도 동일한 특징을 찾습니다. 그리고 한 번 더 크기를 축소하여 또 다른 스케일에서 특징을 추출합니다. 이렇게 여러 스케일에서 특징을 추출하면 다양한 크기의 특징을 얻을 수 있습니다.\n",
        "\n",
        "**서로 다른 크기의 영상 특징(feature)를 만드는 두 가지 주요 방법은 다음과 같습니다.**\n",
        "* Pyramid(피라미드) 기법: Pyramid 기법은 이미지를 다양한 해상도 레벨로 변환하는 방법입니다. 이를 통해 서로 다른 크기의 이미지나 특징을 생성할 수 있습니다. 주로 두 가지 유형의 피라미드가 사용됩니다:\n",
        "    1) 가우시안 피라미드(Gaussian Pyramid): 이미지를 스무딩하여 저해상도 이미지로 만들고, 이어서 이미지를 다시 다운샘플링하여 여러 해상도의 이미지를 생성합니다.\n",
        "    2) 라플라시안 피라미드(Laplacian Pyramid): 가우시안 피라미드에서 원래 이미지를 업샘플링하여 라플라시안 이미지를 생성합니다. 이 라플라시안 이미지는 이미지의 고주파 부분을 나타냅니다.\n",
        "    \n",
        "* 스케일 스페이스(Scale Space) 분석: 스케일 스페이스 분석은 이미지의 스케일(크기)을 연속적으로 변화시키면서 특징을 추출하는 방법입니다. 이는 스케일마다 이미지를 스무딩하고 필터링하여 다양한 크기의 특징을 얻을 수 있습니다. 스케일 스페이스 분석은 LoG(Laplacian of Gaussian) 필터 또는 Difference of Gaussians (DoG) 필터를 사용하여 이미지의 다양한 스케일에서 특징을 검출합니다. 이 방법은 이미지 피라미드를 만들지 않고도 다양한 크기의 특징을 찾을 수 있어 효율적입니다.\n",
        "\n",
        "이러한 두 가지 방법은 객체 검출, 이미지 피라미드, 스케일 불변 특징 추출 등 다양한 컴퓨터 비전 및 이미지 처리 작업에서 활용됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "refined-indie",
      "metadata": {
        "id": "refined-indie"
      },
      "source": [
        "## 7. Convolutional Neural Network\n",
        "\n",
        "### 7-1. CNN의 작동 원리를 이해하자 (1) 필터 사이즈\n",
        "\n",
        "**Q. 3x3의 filter를 3번 통과하면 어떤 크기의 filter가 되나요?**\n",
        ">7*7\n",
        "\n",
        "**Q. 서로 다른 크기의 feature를 추출하려면 어떻게 하면 되나요?**\n",
        ">Convolution layer를 여러 개 쌓으면 됩니다.\n",
        "\n",
        "### 7-2. CNN의 작동 원리를 이해하자 (2) 영상 사이즈\n",
        "\n",
        "**Q. 영상 사이즈를 점점 작게 만드는 방법에는 무엇이 있나요?**\n",
        ">Pooling layer를 CNN의 중간 중간에 넣어줍니다.\n",
        "\n",
        "**Q. Max pooling을 설명하고, Max pooling을 주로 사용하는 이유를 적어보세요.**\n",
        ">Max pooling은 4개의 픽셀 중 가장 큰 값만 남기고 버리는 방법입니다. Max pooling을 많이 사용하는 이유는 Vanishing Gradient 문제 때문입니다. weight가 학습될 때, ‘학습되는 양 = 미분값 x 출력값’인데, 학습되는 양이 작으면 학습이 잘 안 되므로 출력값을 최대로 만들기 위해서 가장 큰 값만 사용합니다. Max pooling은 컨볼루션 신경망에서 사용되는 풀링 연산 중 하나로, 주로 이미지 처리와 컴퓨터 비전 작업에 사용됩니다. Max pooling은 특정 영역(보통 2x2 또는 3x3)에서 가장 큰 값(Max)을 선택하여 입력 데이터를 다운샘플링(축소)하는 연산입니다. Max pooling은 CNN 아키텍처에서 일반적으로 컨볼루션 레이어 뒤에 배치되며, 입력 데이터의 공간 차원을 축소하고 중요한 특징을 강조하여 이미지 처리 작업에서 중요한 역할을 합니다. Max pooling은 다음과 같은 특징을 가집니다:\n",
        "\n",
        "* 동작 방식: Max pooling은 입력 데이터를 격자 형태의 영역으로 분할하고, 각 영역에서 가장 큰 값을 선택하여 새로운 값을 생성합니다. 보통는 2x2 또는 3x3 크기의 영역을 사용하며, 이 크기의 영역을 이동하면서 특징 맵(feature map)을 생성합니다.\n",
        "\n",
        "* 주요 사용 이유:\n",
        "    1) 공간 차원 축소: 입력 데이터의 공간 차원을 축소함으로써 계산량을 줄이고 모델을 간단하게 만듭니다. 이는 과적합을 방지하고 모델의 효율성을 높이는 데 도움이 됩니다.  \n",
        "    2) 위치 불변성: Max pooling은 작은 이동에 대해 불변성을 제공합니다. 즉, 객체가 이미지 내에서 이동해도 Max pooling은 가장 큰 특징값을 유지하여 객체의 위치가 변해도 특징을 감지할 수 있습니다.  \n",
        "    3) 강한 특징 추출: Max pooling은 입력 영역에서 가장 중요한 정보(가장 큰 값)만을 선택하므로 중요한 특징을 강조하고 불필요한 정보를 제거하여 특징 추출에 도움을 줍니다.  \n",
        "    4) 노이즈 감소: 작은 노이즈나 변동성을 제거하여 입력 데이터를 정리하고 간결하게 만듭니다.  \n",
        "\n",
        "\n",
        "### 7-3. 영상 특징을 파악하기 위한 CNN의 구조\n",
        "\n",
        "**Q. CNN은 어떤 구조로 되어 있나요?**\n",
        ">CNN은 feature extraction과 Neural Network로 이루어집니다. feature를 추출하기 위해 Convolution layer와 pooling layer를 쌓았고, 영상에서 추출된 feature를 한 줄로 세워 Neural Network로 분류합니다. 다시 말해, 합성곱 신경망은 다양한 계층(layer)과 구조로 구성됩니다. CNN의 구조는 이러한 레이어의 연속적인 스택으로 이루어져 있으며, 학습을 통해 필터, 가중치, 및 모델의 파라미터를 조정하여 입력 데이터로부터 유용한 정보를 추출하고 작업을 수행합니다. 이러한 구조는 이미지 처리, 객체 감지, 이미지 분류, 얼굴 인식, 자율 주행 자동차, 음성 처리 및 자연어 처리와 같은 다양한 응용 분야에서 사용됩니다.\n",
        "\n",
        "* 입력 레이어(Input Layer): 이미지나 다차원 데이터를 받아들이는 역할. 이미지 데이터의 경우, 가로, 세로, 채널(예: RGB)의 차원으로 구성.\n",
        "\n",
        "* 합성곱 레이어(Convolutional Layers): 입력 데이터에서 특징을 추출하는 역할. 합성곱 연산을 사용하여 다양한 특징 맵(feature map)을 생성.  \n",
        "    각 합성곱 레이어는 여러 개의 필터(커널)를 가지며, 필터는 입력 데이터에 스캔하여 특징을 검출합니다.  \n",
        "\n",
        "* 활성화 함수 레이어 (Activation Function Layers): 합성곱 레이어 이후에 활성화 함수(예: ReLU)가 적용.  \n",
        "    이 함수는 특징 맵의 비선형성을 도입하여 네트워크가 복잡한 패턴을 학습하도록 조력.\n",
        "\n",
        "* 풀링 레이어 (Pooling Layers): 공간 차원을 축소하고 계산량을 줄이는 역할.  \n",
        "    주로 Max 풀링이 사용되며, 특정 영역에서 가장 큰 값을 선택하여 중요한 정보를 유지하고 작은 특징을 간결하게 만듦.\n",
        "\n",
        "* 완전 연결 레이어 (Fully Connected Layers): 모델의 출력을 생성하기 위해 사용.  \n",
        "    모든 특징을 고려하여 최종 예측을 수행하며 신경망의 마지막 부분에 위치.\n",
        "\n",
        "* 출력 레이어 (Output Layer): 모델의 최종 출력을 생성하는 데 사용.  \n",
        "    출력 레이어의 구조는 작업에 따라 다르며, 분류 작업의 경우는 클래스 확률을 나타내는 노드 수가 필요.  \n",
        "    회귀 작업의 경우는 연속적인 값을 예측할 수 있도록 구성.\n",
        "\n",
        "### 7-4. 지금까지의 내용으로 딥러닝 연구가 가능한가요?\n",
        "\n",
        "**Q. Artistic Style 논문에서 언급된 외곽선 정보와 스타일 정보는 각각 어디에서 가져올 수 있나요?**\n",
        ">외곽선 정보는 사진 영상에서, 스타일 정보는 아트/스타일 영상에서 가져올 수 있어요.\n",
        "\n",
        "**Q. 영상 스타일 정보는 함수 h를 통해 얻을 수 있다고 하였습니다. 이 함수 h를 정의해 보세요.**\n",
        ">함수 h는 Gram matrix라고 부르며, feature간 곱하기를 의미. 이 Gram matrix를 사용해 영상의 스타일 정보를 추출할 수 있습니다.\n",
        "\n",
        "### 7-5. 지금까지의 내용으로 알고리즘 개발이 가능한가요?\n",
        "\n",
        "**Q. 지금까지 배운 내용으로 새로운 알고리즘을 개발할 수 있을까요?**\n",
        ">시작은 할 수 있습니다. 예를 들면, 새로운 함수를 만들어 내면 가능합니다.\n",
        "\n",
        "### 7-6. Convolution 레이어 구조 살펴보기 (1)\n",
        "\n",
        "**Q. 1920 x 1080 크기의 3채널 이미지를 나타내기 위해서는 얼마나 많은 값이 필요할까요?**\n",
        ">1920*1080*3=6,220,800. 이미지 정보는 1920 x 1080 x 3 크기의 텐서로 표현됩니다.\n",
        "\n",
        "**이미지에 대한 배경지식과 Convolution 레이어를 가지고 코드로 구현해 봅시다.**\n",
        "\n",
        "```\n",
        "import tensorflow as tf\n",
        "\n",
        "# 1장 x 가로 1920 픽셀 x 세로 1080 픽셀 x 3채널(빨, 초, 파)\n",
        "pic = tf.zeros((1, 1920, 1080, 3))\n",
        "print(\"입력 이미지 데이터:\", pic.shape)\n",
        "pic_flatten_out = tf.keras.layers.Flatten()(pic)\n",
        "print(\"이미지 데이터 값 개수: \", pic_flatten_out.shape)\n",
        "print(\"\\n\")\n",
        "\n",
        "single_conv_layer = tf.keras.layers.Conv2D(filters=1, # 1개 필터\n",
        "                                    kernel_size=(5, 5),    # 5 x 5 크기\n",
        "                                    use_bias=False)    # bias에 대해서는 여기서는 설명하지 않습니다.\n",
        "single_conv_out = single_conv_layer(pic)\n",
        "print(\"단일 필터 Convolution 레이어:\", single_conv_layer.weights[0].shape)\n",
        "print(\"단일 필터 Convolution 레이어의 파라미터 수:\", single_conv_layer.count_params())\n",
        "print(\"단일 필터 Convolution 결과 이미지:\", single_conv_out.shape)\n",
        "single_flatten_out = tf.keras.layers.Flatten()(single_conv_out)\n",
        "print(\"단일 필터 Convolution 결과 이미지 데이터 수: \", single_flatten_out.shape)\n",
        "print(\"\\n\")\n",
        "\n",
        "multiple_conv_layer = tf.keras.layers.Conv2D(filters=16, # 16개 필터\n",
        "                                    kernel_size=(5, 5),    # 5 x 5 크기\n",
        "                                    use_bias=False)    # bias에 대해서는 여기서는 설명하지 않습니다.\n",
        "multiple_conv_out = multiple_conv_layer(pic)\n",
        "print(\"16개 필터 Convolution 레이어:\", multiple_conv_layer.weights[0].shape)\n",
        "print(\"16개 필터 Convolution 레이어의 파라미터 수:\", multiple_conv_layer.count_params())\n",
        "print(\"16개 필터 Convolution 결과 이미지:\", multiple_conv_out.shape)\n",
        "multiple_flatten_out = tf.keras.layers.Flatten()(multiple_conv_out)\n",
        "print(\"16개 필터 Convolution 결과 이미지 데이터 수:\", multiple_flatten_out.shape)\n",
        "```\n",
        "\n",
        "**위 코드의 결과는 아래과 같습니다.**\n",
        "\n",
        "```\n",
        "입력 이미지 데이터: (1, 1920, 1080, 3)\n",
        "이미지 데이터 값 개수:  (1, 6220800)\n",
        "\n",
        "단일 필터 Convolution 레이어: (5, 5, 3, 1)\n",
        "단일 필터 Convolution 레이어의 파라미터 수: 75\n",
        "단일 필터 Convolution 결과 이미지: (1, 1916, 1076, 1)\n",
        "단일 필터 Convolution 결과 이미지 데이터 수:  (1, 2061616)\n",
        "\n",
        "16개 필터 Convolution 레이어: (5, 5, 3, 16)\n",
        "16개 필터 Convolution 레이어의 파라미터 수: 1200\n",
        "16개 필터 Convolution 결과 이미지: (1, 1916, 1076, 16)\n",
        "16개 필터 Convolution 결과 이미지 데이터 수: (1, 32985856)\n",
        "```\n",
        "\n",
        ">위 결과에서 주목해야 할 것은 Convolution layer의 weight가 필터의 개수에 따라서도 달라지고, 입력 이미지의 채널에 따라서도 달라진다는 점입니다.  \n",
        "특히 입력 이미지의 채널에 따라 달라지며, 여러 채널이 있다면 모든 채널의 합성곱 결과를 내놓아야 합니다.  \n",
        "\n",
        "### 7-7. Convolution 레이어 구조 살펴보기 (2)\n",
        "\n",
        ">Max pooling에 일반적으로 적용되는 stride라는 것이 있습니다. Max pooling을 적용하는 간격이라고 보면 됩니다. Stride는 일반적인 개념이라서 convolution에도 적용할 수 있어요. 다만 max pooling에서 훨씬 자주 사용됩니다. convolution을 설명할 때 필터가 한 칸씩 이동하며 연산을 수행했는데요. 이 경우는 stride가 1인 경우입니다. 그런데 이 필터를 두 칸씩 이동하며 연산을 할 수도 있겠죠? 이러면 stride가 2인 것입니다. 일반적으로 max pooling은 그 크기와 stride가 같도록 설정합니다. 2 x 2 max pooling은 stride를 2로 하고, 3 x 3 max pooling은 stride를 3으로 하는 식이에요. 겹쳐서 pooling하지 않도록 하는 것이죠. pooling 크기와 stride에 따라서 결과의 크기가 달라집니다. 보통 줄어들게 되는데요. 2 x 2 max pooling 을 2 stride로 적용하면 가로와 세로가 각각 절반이 됩니다. 면적으로는 1/4가 되죠.\n",
        "\n",
        "**Convolution과 max pooling을 함께 적용하면 아래처럼 코드를 작성할 수 있습니다.**\n",
        "\n",
        "```\n",
        "# 1장 x 가로 1920 픽셀 x 세로 1080 픽셀 x 3채널(빨, 초, 파)\n",
        "pic = tf.zeros((1, 1920, 1080, 3))\n",
        "print(\"입력 이미지 데이터:\", pic.shape)\n",
        "print('\\n')\n",
        "\n",
        "conv_layer = tf.keras.layers.Conv2D(filters=16, # 16개 필터\n",
        "                                    kernel_size=(5, 5),    # 5 x 5 크기\n",
        "                                    use_bias=False)    # bias에 대해서는 여기서는 설명하지 않습니다.\n",
        "conv_out = conv_layer(pic)\n",
        "print(\"16개 필터 Convolution 결과 데이터:\", conv_out.shape)\n",
        "print('\\n')\n",
        "\n",
        "pool_layer = tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2))\n",
        "pool_out = pool_layer(conv_out)\n",
        "print(\"Pooling 결과 데이터:\", pool_out.shape)\n",
        "```\n",
        "\n",
        "**위 코드의 결과는 아래와 같습니다.**\n",
        "\n",
        "```\n",
        "입력 이미지 데이터: (1, 1920, 1080, 3)\n",
        "16개 필터 Convolution 결과 데이터: (1, 1916, 1076, 16)\n",
        "Pooling 결과 데이터: (1, 958, 538, 16)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "genetic-bachelor",
      "metadata": {
        "id": "genetic-bachelor"
      },
      "source": [
        "## 8. RNN, Recurrent Neural Network\n",
        "\n",
        "### 8-1. 입력이 한 개가 아니라면? RNN\n",
        "\n",
        "**Q. RNN은 주로 어떤 특성이 있는 문제에 사용되나요?**\n",
        ">RNN은 입력 또는 출력에 시간 순서가 있거나, 어떤 문제를 풀기 위하여 입력이나 출력이 여러 개 일 때 주로 사용합니다. 즉, 시간적인 의존성이나 순서가 중요한 작업에 효과적이며, 이러한 문제를 처리하는 데 적합한 모델 구조를 제공합니다. 그러나 RNN은 장기 의존성을 캡처하기 어려운 \"기울어진 기울기(vanishing gradient)\" 문제가 있어, 이를 극복하기 위해 LSTM(Long Short-Term Memory) 및 GRU(Gated Recurrent Unit)와 같은 변종이 개발되었습니다.\n",
        "\n",
        "* 순차 데이터(Sequential Data): RNN은 순차 데이터를 다루는 데 특히 효과적. 텍스트 데이터(시퀀스 문장), 시계열 데이터(주식 가격, 날씨 데이터), 음성 데이터, 동영상 프레임 등과 같이 데이터의 순서가 중요한 경우에 사용.  \n",
        "\n",
        "* 시계열 예측: 시계열 데이터를 활용한 예측 작업에 RNN이 적합. 주식 가격 예측, 날씨 예측, 패턴 인식 및 시계열 데이터의 트렌드 분석 등.  \n",
        "\n",
        "* 자연어 처리(Natural Language Processing, NLP): RNN은 텍스트 처리 및 자연어 이해 작업에 사용됩니다. 번역, 텍스트 생성, 감정 분석, 문서 분류, 대화형 시스템, 음성 인식 및 기계 번역과 같은 NLP 작업에 활용됩니다.  \n",
        "\n",
        "* 음성 처리: 음성 데이터의 인식, 음성 합성, 음성 명령 처리 및 화자 인식과 같은 음성 처리 작업에 RNN이 사용.  \n",
        "\n",
        "* 이미지 캡션 생성: RNN은 이미지와 텍스트를 연결하여 이미지 캡션 생성 작업에 사용. 이미지를 입력으로 받아 그에 대한 설명(캡션)을 생성.  \n",
        "\n",
        "* 문서 요약: RNN은 문서를 요약하거나 중요한 정보를 추출하는 문서 요약 작업에도 활용.  \n",
        "\n",
        "* 감정 분석: 텍스트 또는 음성 데이터로부터 감정을 분석, 긍정적인 또는 부정적인 감정을 탐지하는 감정 분석 작업에 사용.  \n",
        "\n",
        "**Q. One to Many / Many to One / Many to Many 의 예시를 각각 적어보세요.**\n",
        "* One to Many: 이미지 캡션 생성, 이 경우, 하나의 입력(예: 이미지)에서 여러 개의 출력(예: 캡션 문장)을 생성합니다. 하나의 이미지를 입력으로 받아 RNN 모델은 이미지 내용을 이해하고 그에 맞는 설명(캡션)을 생성합니다. 또한 하나의 음표를 입력하면 음악을 만들어 내는 음악생성 모델, 분자구조 이미지를 입력하면 smiles 분자식을 내뱉는 모델 등이 해당할 수 있습니다.  \n",
        "\n",
        "* Many to One: 감정 분석, 이 경우, 여러 입력(예: 텍스트 리뷰 문장)에서 하나의 출력(예: 감정 분석 결과)을 생성합니다. 다수의 텍스트 문장을 입력으로 받아 RNN 모델은 문장 내의 감정(긍정 또는 부정)을 분석하고 전체 리뷰의 감정을 평가합니다. 또한 트위터나, 구매평 등을 입력하여 긍정, 부정을 분류하는 감성분석, 이전 주가 및 외부정보를 활용한 특정 시점의 주식 가격 예측 등이 해당할 수 있습니다.  \n",
        "\n",
        "* Many to Many: 기계 번역, 이 경우, 다수의 입력(예: 원본 언어의 문장)에서 다수의 출력(예: 번역된 언어의 문장)을 생성합니다. 여러 언어의 문장을 입력으로 받아 RNN 모델은 원본 문장을 번역하여 대응되는 번역 문장을 생성합니다. 한국어 문장을 영어 문장으로 번역하는 기계번역 모델, 어떤 단어를 보고 그 단어가 어떤 유형인지를 인식하는 개체명 인식 모델 등이 해당할 수 있습니다.\n",
        "\n",
        "### 8-2. 순서를 기억하는 인공지능 만들기\n",
        "\n",
        "**Q. 노래 한곡과 같은 긴 Sequence를 모두 neural network에 입력으로 넣어주게 되면 네트워크 사이즈가 너무 커지게 됩니다. RNN은 어떤 방법을 이용하여 입력사이즈를 일정하게 유지하고 과거의 입력값들을 반영할 수 있게 하나요?**\n",
        ">과거의 입력 값들을 잘 버무려 저장한 Neural Memory h를 만듭니다.\n",
        "\n",
        "* 시퀀스 패딩(Padding): 시퀀스의 길이를 일정하게 맞추기 위해 패딩을 사용합니다. 짧은 시퀀스는 0 또는 다른 특정 값으로 패딩하여 길이를 맞춥니다. 이렇게 하면 모든 시퀀스가 동일한 길이를 가지게 되며, 네트워크 입력 사이즈를 일정하게 유지할 수 있습니다.  \n",
        "\n",
        "* 슬라이딩 윈도우(Sliding Window): 긴 시퀀스를 작은 윈도우 또는 덩어리로 나누어 처리합니다. 이 윈도우는 시퀀스를 일정 길이로 분할하여 처리하고, 윈도우 간에 정보를 공유하여 과거 입력을 반영합니다.  \n",
        "\n",
        "* 순환 레이어 쌓기(Stacking Recurrent Layers): 여러 개의 순환 레이어를 쌓아 사용하면, 각 레이어는 입력을 처리한 후 다음 레이어로 전달됩니다. 이를 통해 네트워크는 다양한 시간 스텝에서 입력을 처리할 수 있습니다.  \n",
        "\n",
        "* 장기 의존성을 관리하기 위한 모델 (Long Short-Term Memory, LSTM, 또는 Gated Recurrent Unit, GRU): LSTM 또는 GRU와 같은 장기 의존성을 관리하기 위한 순환 레이어 변종을 사용하면 더 긴 시퀀스를 효과적으로 처리할 수 있습니다.\n",
        "\n",
        "* Attention 메커니즘(Attention Mechanism): Attention 메커니즘을 사용하면 모델은 특정 시간 스텝에서 입력의 특정 부분에 집중할 수 있습니다. 이를 통해 더 긴 시퀀스의 정보를 반영할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "assumed-window",
      "metadata": {
        "id": "assumed-window"
      },
      "source": [
        "## 9. Understanding RNNs Mathematically\n",
        "\n",
        "### 9-1. RNN 수학적 이해\n",
        "\n",
        "**Q. RNN 구현 시 이전 메모리 출력 값 ht-1을 (A),(B),(C)중 어디에 더했을 때에 가장 적절할지 생각해 봅시다. 각각의 경우에 어떻게 ht가 계산되는지 수식으로 표현 해보고, 어떤 차이점이 있는지 그리고 B에 더하는 이유는 무엇일지 적어봅시다.**\n",
        ">(A) ht = f(Wxh * Xt) + Whh * ht-1  \n",
        ">(B) ht = f(Wxh * Xt + Whh * ht-1)  \n",
        ">(C) ht = f(Wx *(Xt + Whh * ht-1)) = f(Wxh * Xt + Wxh * Whh(ht-1))  \n",
        "\n",
        "Neral network를 강력하게 하는 가장 큰 요인은 non-linear activation function이다. 이게 있기 때문에 layer를 통과할 때마다 점점 더 복잡한 데이터를 학습할 수 있게 하며 인간의 두뇌를 모방할 수 있게 됩니다. A의 경우에는 B와 비교했을때 뒷부분이 non-linear activation function을 포함하지 않아서 이러한 장점을 버리는 결과를 갖습니다. B는 RNN에서 채택한 방법입니다. C를 B와 비교하면 h_t-1에 Wxh가 하나 더 곱해져 있어서 뉴럴 네트워크를 2층 통과시키는 효과가 나타납니다. 따라서 학습시키는데 비용이 많이 들게 되지요.\n",
        "\n",
        "\n",
        "### 9-2. LSTM\n",
        "\n",
        "* 게이트 메커니즘(Gate Mechanism): LSTM은 게이트를 사용하여 정보를 선택적으로 흘려보내거나 차단합니다. 주요 게이트로는 Forget Gate, Input Gate, Output Gate 등이 있습니다. 각 게이트는 시그모이드 활성화 함수를 사용하여 0과 1 사이의 값을 출력하며, 이 게이트들이 정보의 흐름을 조절합니다.\n",
        "\n",
        "* Forget Gate: Forget Gate는 이전 메모리 상태 중에서 어떤 정보를 잊을 것인지 결정합니다. 이 게이트의 출력이 0에 가까울수록 해당 정보는 거의 완전히 잊혀집니다. 따라서, 필요 없는 정보가 오랫동안 유지되는 것을 방지하고 그라디언트 소멸 문제를 완화합니다.\n",
        "\n",
        "* Input Gate: Input Gate는 현재 입력에서 어떤 정보를 메모리에 저장할 것인지를 결정합니다. 이 게이트의 출력이 0에 가까울수록 현재 입력의 정보는 거의 저장되지 않습니다. 따라서, 중요한 정보만이 메모리에 저장되고, 필요 없는 정보는 버려지므로 장기 의존성을 관리할 수 있습니다.\n",
        "\n",
        "* LSTM 셀 구조: LSTM 셀은 이전 메모리 상태와 현재 입력을 결합하는 방식으로 작동하며, 이 결합은 덧셈 연산을 사용합니다. 이로써 정보를 그대로 전달하거나 차단하는 게이트 메커니즘과 결합된 것으로, 그라디언트가 비교적 잘 전파됩니다.\n",
        "\n",
        "* 게이트 간 연산의 영향: 게이트의 출력값은 시그모이드 활성화 함수를 사용하므로 그라디언트를 안정적으로 관리합니다. 이러한 안정성은 역전파(backpropagation)를 통해 오랫동안 거슬러 올라가는 장기 의존성을 다루는 데 도움이 됩니다.\n",
        "\n",
        "**Q. LSTM의 Input Gate와 Forget Gate에서는 왜 relu를 쓰지 않고 sigmoid activation function을 사용할까요?**\n",
        ">1-a값은 0보다 크고 1보다 작아야 한다는 사실을 만족시키기 위해서에요. sigmoid함수의 출력은 항상 0과 1 사이의 값을 가지므로 이러한 조건을 만족시킬 수 있습니다. LSTM (Long Short-Term Memory)의 Input Gate와 Forget Gate에서 sigmoid 활성화 함수를 사용하는 이유는 LSTM의 주요 목표 중 하나인 장기 의존성(Long-Term Dependencies)을 관리하기 위함입니다. 반면에, ReLU 활성화 함수는 출력 범위가 [0, ∞]로 양수 범위에 있어서 게이트의 역할을 적절히 표현하기 어려우며, 그라디언트 관리에 어려움을 겪을 수 있습니다. 따라서 LSTM에서는 Sigmoid 함수가 게이트 제어에 적합하다고 판단되어 사용됩니다.\n",
        "\n",
        "**Q. LSTM이 vanishing gradient problem에 강한 이유는 무엇인가요?**\n",
        ">LSTM은 게이트 메커니즘을 통해 정보의 흐름을 세밀하게 조절하고, 시그모이드 활성화 함수를 사용하여 그라디언트를 안정적으로 유지하므로, 장기 의존성을 관리하는 데 강하고, vanishing gradient problem을 해결하는 데 효과적입니다."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}