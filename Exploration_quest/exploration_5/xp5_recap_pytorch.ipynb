{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CAVASOL/aiffel_quest/blob/main/Exploration_quest/exploration_5/xp5_recap_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a86d54e",
      "metadata": {
        "id": "3a86d54e"
      },
      "source": [
        "## A Chatbot with Korean\n",
        "\n",
        "**Creator: Yeon Kim**\n",
        "\n",
        "**Goal**\n",
        "\n",
        "* Recap `xp5_project.ipynb` with `PyTorch`\n",
        "* Transformer chatbot model implementation\n",
        "\n",
        "**Index**\n",
        "\n",
        "    Set up\n",
        "    Transformer Model\n",
        "    Import Dataset\n",
        "    Data field settings\n",
        "    Train Transformer model\n",
        "    Run Transformer chatbot for real sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e266a21e",
      "metadata": {
        "id": "e266a21e"
      },
      "source": [
        "### Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d682d72",
      "metadata": {
        "id": "0d682d72"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import torch\n",
        "from torch.jit import script, trace\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import codecs\n",
        "from io import open\n",
        "import itertools\n",
        "import math\n",
        "import json\n",
        "\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63be6d82",
      "metadata": {
        "id": "63be6d82"
      },
      "source": [
        "### Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f0dd8e0",
      "metadata": {
        "id": "5f0dd8e0"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, position, d_model):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "\n",
        "        pe = torch.zeros(position, d_model)\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        pe = pe.to(device)\n",
        "\n",
        "        for pos in range(position):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
        "                pe[pos, i + 1] = math.cos(pos /\n",
        "                                          (10000 ** ((2 * i)/d_model)))\n",
        "\n",
        "        self.pe = pe.unsqueeze(0)\n",
        "\n",
        "        self.pe.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        ret = math.sqrt(self.d_model)*x + self.pe[:, :x.size(1)]\n",
        "        return ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45bb5cc9",
      "metadata": {
        "id": "45bb5cc9"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask):\n",
        "\n",
        "  matmul_qk = torch.matmul(query, torch.transpose(key,2,3))\n",
        "\n",
        "  depth = key.shape[-1]\n",
        "  logits = matmul_qk / math.sqrt(depth)\n",
        "\n",
        "  if mask is not None:\n",
        "    logits += (mask * -1e9)\n",
        "\n",
        "  attention_weights = F.softmax(logits, dim=-1)\n",
        "\n",
        "  output = torch.matmul(attention_weights, value)\n",
        "\n",
        "  return output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f299098",
      "metadata": {
        "id": "9f299098"
      },
      "outputs": [],
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiheadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = int(d_model/self.num_heads)\n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def split_heads(self, inputs, batch_size):\n",
        "      inputs = torch.reshape(\n",
        "          inputs, (batch_size, -1, self.num_heads, self.depth))\n",
        "      return torch.transpose(inputs, 1,2)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        query, key, value, mask = inputs['query'], inputs['key'], inputs['value'], inputs['mask']\n",
        "        batch_size = query.shape[0]\n",
        "        query = self.q_linear(query)\n",
        "        key = self.k_linear(key)\n",
        "        value = self.v_linear(value)\n",
        "\n",
        "        query = self.split_heads(query, batch_size)\n",
        "        key = self.split_heads(key, batch_size)\n",
        "        value = self.split_heads(value, batch_size)\n",
        "\n",
        "        scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)\n",
        "        scaled_attention = torch.transpose(scaled_attention, 1,2)\n",
        "\n",
        "        concat_attention = torch.reshape(scaled_attention,\n",
        "                                      (batch_size, -1, self.d_model))\n",
        "\n",
        "        outputs = self.out(concat_attention)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddcacb65",
      "metadata": {
        "id": "ddcacb65"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, attention):\n",
        "        outputs = self.linear_1(attention)\n",
        "        outputs = F.relu(outputs)\n",
        "        outputs = self.linear_2(outputs)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1664d972",
      "metadata": {
        "id": "1664d972"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "      def __init__(self, d_ff, d_model, num_heads, dropout):\n",
        "\n",
        "        super(EncoderBlock, self).__init__()\n",
        "\n",
        "        self.attn = MultiheadAttention(d_model, num_heads)\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.norm_1 = nn.LayerNorm(d_model)\n",
        "        self.ff = FeedForward(d_model, d_ff)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        self.norm_2 = nn.LayerNorm(d_model)\n",
        "\n",
        "      def forward(self, inputs, padding_mask):\n",
        "\n",
        "        attention = self.attn({'query': inputs, 'key': inputs, 'value': inputs, 'mask': padding_mask})\n",
        "        attention = self.dropout_1(attention)\n",
        "        attention = self.norm_1(inputs + attention)\n",
        "        outputs = self.ff(attention)\n",
        "        outputs = self.dropout_2(outputs)\n",
        "        outputs = self.norm_2(attention + outputs)\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fd82e9f",
      "metadata": {
        "id": "3fd82e9f"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "      def __init__(self,text_embedding_vectors, vocab_size, num_layers, d_ff, d_model, num_heads, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.embb = nn.Embedding(text_embedding_vectors, d_model)\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.PE = PositionalEncoder(vocab_size, d_model)\n",
        "        self.encoder_block = EncoderBlock(d_ff, d_model, num_heads, dropout)\n",
        "\n",
        "      def forward(self, x, padding_mask):\n",
        "        emb = self.embb(x)\n",
        "        emb *= math.sqrt(self.d_model)\n",
        "        emb = self.PE(emb)\n",
        "        output = self.dropout_1(emb)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "          output = self.encoder_block(output, padding_mask)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bfd6ec3",
      "metadata": {
        "id": "9bfd6ec3"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "      def __init__(self, d_ff, d_model, num_heads, dropout):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "\n",
        "        self.attn = MultiheadAttention(d_model, num_heads)\n",
        "        self.attn_2 = MultiheadAttention(d_model, num_heads)\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.norm_1 = nn.LayerNorm(d_model)\n",
        "        self.ff = FeedForward(d_model, d_ff)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        self.dropout_3 = nn.Dropout(dropout)\n",
        "        self.norm_2 = nn.LayerNorm(d_model)\n",
        "        self.norm_3 = nn.LayerNorm(d_model)\n",
        "\n",
        "      def forward(self, inputs, enc_outputs, padding_mask, look_ahead_mask):\n",
        "        attention1 = self.attn({'query': inputs, 'key': inputs, 'value': inputs, 'mask': look_ahead_mask})\n",
        "        attention1 = self.norm_1(inputs + attention1)\n",
        "        attention2 = self.attn_2({'query': attention1, 'key': enc_outputs, 'value': enc_outputs, 'mask': padding_mask})\n",
        "        attention2 = self.dropout_1(attention2)\n",
        "        attention2 = self.norm_2(attention1 + attention2)\n",
        "\n",
        "        outputs = self.ff(attention2)\n",
        "        outputs = self.dropout_3(outputs)\n",
        "        outputs = self.norm_3(attention2 + outputs)\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b12cbf8",
      "metadata": {
        "id": "7b12cbf8"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "      def __init__(self,text_embedding_vectors,  vocab_size, num_layers, d_ff, d_model, num_heads, dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.embb = nn.Embedding(text_embedding_vectors, d_model)\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.PE = PositionalEncoder(vocab_size, d_model)\n",
        "        self.decoder_block = DecoderBlock(d_ff, d_model, num_heads, dropout)\n",
        "\n",
        "      def forward(self, enc_output, dec_input, padding_mask, look_ahead_mask):\n",
        "        emb = self.embb(dec_input)\n",
        "        emb *= math.sqrt(self.d_model)\n",
        "        emb = self.PE(emb)\n",
        "        output = self.dropout_1(emb)\n",
        "        for i in range(self.num_layers):\n",
        "          output = self.decoder_block(output, enc_output, padding_mask, look_ahead_mask)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c85ac5a6",
      "metadata": {
        "id": "c85ac5a6"
      },
      "outputs": [],
      "source": [
        "class transformer(nn.Module):\n",
        "\n",
        "    def __init__(self, text_embedding_vectors, vocab_size, num_layers, d_ff, d_model, num_heads, dropout):\n",
        "        self.vocab_size = vocab_size\n",
        "        super(transformer, self).__init__()\n",
        "        self.enc_outputs = Encoder(text_embedding_vectors, vocab_size, num_layers, d_ff, d_model, num_heads, dropout)\n",
        "        self.dec_outputs = Decoder(text_embedding_vectors, vocab_size, num_layers, d_ff, d_model, num_heads, dropout)\n",
        "        self.output = nn.Linear(d_model, text_embedding_vectors)\n",
        "        self.softmax = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    def forward(self, input, dec_input):\n",
        "        enc_input = input\n",
        "        dec_input = dec_input\n",
        "        enc_padding_mask = create_padding_mask(enc_input)\n",
        "        dec_padding_mask = create_padding_mask(enc_input)\n",
        "        look_ahead_mask = create_look_ahead_mask(dec_input)\n",
        "\n",
        "        enc_output = self.enc_outputs(enc_input, enc_padding_mask)\n",
        "        dec_output = self.dec_outputs(enc_output, dec_input, dec_padding_mask, look_ahead_mask)\n",
        "        output = self.output(dec_output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b4f932e",
      "metadata": {
        "id": "7b4f932e"
      },
      "source": [
        "### Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8f2e0c8",
      "metadata": {
        "id": "a8f2e0c8",
        "outputId": "b11f441a-8a60-4274-e7b3-09e5930c5d85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: soynlp in /opt/conda/lib/python3.9/site-packages (0.0.493)\n",
            "Requirement already satisfied: psutil>=5.0.1 in /opt/conda/lib/python3.9/site-packages (from soynlp) (5.8.0)\n",
            "Requirement already satisfied: numpy>=1.12.1 in /opt/conda/lib/python3.9/site-packages (from soynlp) (1.21.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.9/site-packages (from soynlp) (1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.9/site-packages (from soynlp) (1.7.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.20.0->soynlp) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.20.0->soynlp) (1.1.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install soynlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "371ab5b9",
      "metadata": {
        "id": "371ab5b9"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "from torchtext import data, datasets\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "827cdbf1",
      "metadata": {
        "id": "827cdbf1",
        "outputId": "428cc2a0-6d1c-4eec-9f7a-dcf577638a48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data shape: (11823, 3)\n"
          ]
        }
      ],
      "source": [
        "train_data = pd.read_csv('/aiffel/data/ChatbotData .csv')\n",
        "\n",
        "print(\"Data shape:\", train_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5d5427c",
      "metadata": {
        "id": "b5d5427c",
        "outputId": "ee722b91-fa68-4633-abac-ac7c543cfa01"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12시 땡!</td>\n",
              "      <td>하루가 또 가네요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1지망 학교 떨어졌어</td>\n",
              "      <td>위로해 드립니다.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3박4일 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3박4일 정도 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PPL 심하네</td>\n",
              "      <td>눈살이 찌푸려지죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Q            A  label\n",
              "0           12시 땡!   하루가 또 가네요.      0\n",
              "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
              "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
              "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
              "4          PPL 심하네   눈살이 찌푸려지죠.      0"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(train_data.head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6a9486a",
      "metadata": {
        "id": "c6a9486a"
      },
      "outputs": [],
      "source": [
        "from soynlp.tokenizer import LTokenizer\n",
        "\n",
        "tokenizer = LTokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c164a992",
      "metadata": {
        "id": "c164a992",
        "outputId": "45e7553c-31e6-447b-efdf-b1e20621b07f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['내일', '역', '앞의', '식당에서', '밥', '먹으러', '나갈래', '?']"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer(\"내일 역 앞의 식당에서 밥 먹으러 나갈래 ?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a61451e3",
      "metadata": {
        "id": "a61451e3"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 40"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e380dbcd",
      "metadata": {
        "id": "e380dbcd"
      },
      "source": [
        "### Data field settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58946041",
      "metadata": {
        "id": "58946041"
      },
      "outputs": [],
      "source": [
        "pip install torchtext==0.8.1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a22b709",
      "metadata": {
        "id": "3a22b709"
      },
      "source": [
        "[attributeerror: module torchtext.data has no attribute field](https://itsourcecode.com/attributeerror/attributeerror-module-torchtext-data-has-no-attribute-field-solved/)\n",
        "\n",
        "torchtext.data.Pipeline -> torchtext.legacy.data.Pipeline  \n",
        "torchtext.data.Batch -> torchtext.legacy.data.Batch  \n",
        "torchtext.data.Example -> torchtext.legacy.data.Example  \n",
        "torchtext.data.Field -> torchtext.legacy.data.Field  \n",
        "torchtext.data.Iterator -> torchtext.legacy.data.Iterator  \n",
        "torchtext.data.Dataset -> torchtext.legacy.data.Dataset  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb0c97af",
      "metadata": {
        "id": "bb0c97af"
      },
      "outputs": [],
      "source": [
        "from torchtext.data import Field\n",
        "\n",
        "Q = data.Field(\n",
        "    sequential=True,\n",
        "    use_vocab=True,\n",
        "    lower=True,\n",
        "    tokenize=tokenizer,\n",
        "    batch_first=True,\n",
        "    init_token=\"<SOS>\",\n",
        "    eos_token=\"<EOS>\",\n",
        "    fix_length=VOCAB_SIZE\n",
        ")\n",
        "\n",
        "A = data.Field(\n",
        "    sequential=True,\n",
        "    use_vocab=True,\n",
        "    lower=True,\n",
        "    tokenize=tokenizer,\n",
        "    batch_first=True,\n",
        "    init_token=\"<SOS>\",\n",
        "    eos_token=\"<EOS>\",\n",
        "    fix_length=VOCAB_SIZE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59a875e0",
      "metadata": {
        "id": "59a875e0"
      },
      "outputs": [],
      "source": [
        "trainset = data.TabularDataset(\n",
        "        train_data, format='csv', skip_header=False,\n",
        "        fields=[('Q', Q),('A', A)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36a14146",
      "metadata": {
        "id": "36a14146"
      },
      "outputs": [],
      "source": [
        "print(vars(train_data[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "481ed881",
      "metadata": {
        "id": "481ed881"
      },
      "outputs": [],
      "source": [
        "print('Number of sample for train_data : {}'.format(len(train_data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d914f5f",
      "metadata": {
        "id": "2d914f5f"
      },
      "outputs": [],
      "source": [
        "Q.build_vocab(trainset.Q, trainset.A, min_freq = 2)\n",
        "A.vocab = Q.vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71556633",
      "metadata": {
        "id": "71556633"
      },
      "outputs": [],
      "source": [
        "PAD_TOKEN, START_TOKEN, END_TOKEN, UNK_TOKEN = Q.vocab.stoi['<pad>'], Q.vocab.stoi['<SOS>'], Q.vocab.stoi['<EOS>'], Q.vocab.stoi['<unk>']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61e1c1f2",
      "metadata": {
        "id": "61e1c1f2"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = VOCAB_SIZE\n",
        "text_embedding_vectors = len(Q.vocab)\n",
        "NUM_LAYERS = 4\n",
        "D_FF = 512\n",
        "D_MODEL = 128\n",
        "NUM_HEADS = 4\n",
        "DROPOUT = 0.3\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# train_iter = data.BucketIterator(\n",
        "#         trainset, batch_size=BATCH_SIZE,\n",
        "#         shuffle=True, repeat=False, sort=False, device = device)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "276e46ce",
      "metadata": {
        "id": "276e46ce"
      },
      "outputs": [],
      "source": [
        "print(text_embedding_vectors)\n",
        "net = transformer(text_embedding_vectors = text_embedding_vectors,\n",
        "                  vocab_size=VOCAB_SIZE, num_layers=NUM_LAYERS, d_ff=D_FF, d_model=D_MODEL,\n",
        "                  num_heads=NUM_HEADS, dropout=DROPOUT)\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Linear') != -1:\n",
        "\n",
        "        nn.init.kaiming_normal_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0.0)\n",
        "\n",
        "net.train()\n",
        "\n",
        "net.apply(weights_init)\n",
        "\n",
        "print(\"Completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7914ff55",
      "metadata": {
        "id": "7914ff55"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "learning_rate = 2e-4\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbd0fd8d",
      "metadata": {
        "id": "dbd0fd8d"
      },
      "outputs": [],
      "source": [
        "def create_padding_mask(x):\n",
        "      input_pad = 0\n",
        "      mask = (x == input_pad).float()\n",
        "      mask = mask.unsqueeze(1).unsqueeze(1)\n",
        "\n",
        "      return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba258041",
      "metadata": {
        "id": "ba258041"
      },
      "outputs": [],
      "source": [
        "def create_look_ahead_mask(x):\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  seq_len = x.shape[1]\n",
        "  look_ahead_mask = torch.ones(seq_len, seq_len)\n",
        "  look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1).to(device)\n",
        "\n",
        "  padding_mask = create_padding_mask(x).to(device)\n",
        "  return torch.maximum(look_ahead_mask, padding_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36f924b2",
      "metadata": {
        "id": "36f924b2"
      },
      "source": [
        "### Train Transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c7224aa",
      "metadata": {
        "id": "7c7224aa"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import datetime\n",
        "\n",
        "def train_model(net, train_iter, criterion, optimizer, num_epochs):\n",
        "    start_time = time.time()\n",
        "\n",
        "    ntokens = len(Q.vocab.stoi)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Device:\", device)\n",
        "    print('-----start-------')\n",
        "    net.to(device)\n",
        "    epoch_ = []\n",
        "    epoch_train_loss = []\n",
        "\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    net.train()\n",
        "\n",
        "    best_epoch_loss = float(\"inf\")\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0.0\n",
        "        cnt = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            questions = batch.question\n",
        "            answers = batch.answer\n",
        "\n",
        "            questions = questions.to(device)\n",
        "            answers = answers.to(device)\n",
        "\n",
        "            with torch.set_grad_enabled(True):\n",
        "                preds = net(questions, answers)\n",
        "                pad = torch.LongTensor(answers.size(0), 1).fill_(PAD_TOKEN).to(device)\n",
        "                preds_id = torch.transpose(preds, 1, 2)\n",
        "                outputs = torch.cat((answers[:, 1:], pad), -1)\n",
        "                optimizer.zero_grad()\n",
        "                loss = criterion(preds_id, outputs)\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(net.parameters(), 0.5)\n",
        "                optimizer.step()\n",
        "                epoch_loss += loss.item()\n",
        "                cnt += 1\n",
        "\n",
        "        epoch_loss = epoch_loss / cnt\n",
        "        if not best_epoch_loss or epoch_loss < best_epoch_loss:\n",
        "            if not os.path.isdir(\"snapshot\"):\n",
        "                os.makedirs(\"snapshot\")\n",
        "            torch.save(net.state_dict(), './snapshot/transformermodel.pt')\n",
        "            best_epoch_loss = epoch_loss\n",
        "\n",
        "        epoch_.append(epoch)\n",
        "        epoch_train_loss.append(epoch_loss)\n",
        "        print('Epoch {0}/{1} Average Loss: {2}'.format(epoch + 1, num_epochs, epoch_loss))\n",
        "        clear_output(wait=True)\n",
        "\n",
        "    fig = plt.figure(figsize=(8, 8))\n",
        "    fig.set_facecolor('white')\n",
        "    ax = fig.add_subplot()\n",
        "\n",
        "    ax.plot(epoch_, epoch_train_loss, label='Average loss')\n",
        "    ax.legend()\n",
        "    ax.set_xlabel('epoch')\n",
        "    ax.set_ylabel('loss')\n",
        "\n",
        "    plt.show()\n",
        "    end_time = time.time() - start_time\n",
        "    times = str(datetime.timedelta(seconds=end_time)).split(\".\")\n",
        "    print('Finished in {0}'.format(times[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4afcb54a",
      "metadata": {
        "id": "4afcb54a"
      },
      "outputs": [],
      "source": [
        "num_epochs = 100\n",
        "train_model(net, train_iter, criterion, optimizer, num_epochs=num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e41eb7b9",
      "metadata": {
        "id": "e41eb7b9"
      },
      "outputs": [],
      "source": [
        "net_trained = transformer(text_embedding_vectors = text_embedding_vectors, vocab_size=VOCAB_SIZE, num_layers=NUM_LAYERS, d_ff=D_FF, d_model=D_MODEL, num_heads=NUM_HEADS, dropout=DROPOUT).to(device)\n",
        "net_trained.load_state_dict(torch.load('./snapshot/transformermodel.pt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00936076",
      "metadata": {
        "id": "00936076"
      },
      "source": [
        "### Run Transformer chatbot for real sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fba4277",
      "metadata": {
        "id": "6fba4277"
      },
      "outputs": [],
      "source": [
        "def stoi(vocab, token, max_len):\n",
        "  #\n",
        "  indices=[]\n",
        "  token.extend(['<pad>'] * (max_len - len(token)))\n",
        "  for string in token:\n",
        "    if string in vocab:\n",
        "      i = vocab.index(string)\n",
        "    else:\n",
        "      i = 0\n",
        "    indices.append(i)\n",
        "  return torch.LongTensor(indices).unsqueeze(0)\n",
        "\n",
        "def itos(vocab, indices):\n",
        "  text = []\n",
        "  for i in indices.cpu()[0]:\n",
        "    if i==1:\n",
        "      break\n",
        "    else:\n",
        "      if i not in [PAD_TOKEN, START_TOKEN, END_TOKEN]:\n",
        "          if i != UNK_TOKEN:\n",
        "              text.append(vocab[i])\n",
        "          else:\n",
        "              text.append('??')\n",
        "  return \" \".join(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4e07761",
      "metadata": {
        "id": "f4e07761"
      },
      "outputs": [],
      "source": [
        "def evaluate(input_sentence):\n",
        "    VOCAB_SIZE = 40\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    tokenizer = LTokenizer()\n",
        "    token = tokenizer(input_sentence)\n",
        "    input = stoi(Q.vocab.itos, token, VOCAB_SIZE).to(device)\n",
        "    output = torch.LongTensor(1, 1).fill_(START_TOKEN).to(device)\n",
        "    for i in range(VOCAB_SIZE):\n",
        "        predictions = net_trained(input, output)\n",
        "        predictions = predictions[:, -1:, :]\n",
        "        predicted_id = torch.argmax(predictions[:,:,3:], axis=-1) + 3\n",
        "        if predicted_id == END_TOKEN:\n",
        "            predicted_id = predicted_id\n",
        "            break\n",
        "        output = torch.cat((output, predicted_id),-1)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a56a68c8",
      "metadata": {
        "id": "a56a68c8"
      },
      "outputs": [],
      "source": [
        "def predict(sentence):\n",
        "  out = evaluate(sentence)\n",
        "  out_text = itos(Q.vocab.itos, out)\n",
        "  print('input = [{0}]'.format(sentence))\n",
        "  print('output = [{0}]'.format(out_text))\n",
        "  return out_text\n",
        "\n",
        "out = predict('우리 내일 같이 영화 볼래?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b188bcfd",
      "metadata": {
        "id": "b188bcfd"
      },
      "outputs": [],
      "source": [
        "out = predict('그 영화 너무 별로더라')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}