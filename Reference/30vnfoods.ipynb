{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CAVASOL/aiffel_quest/blob/main/Reference/30vnfoods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45e82586",
      "metadata": {
        "id": "45e82586"
      },
      "source": [
        "## 3-1. 프로젝트: 베트남 음식 분류하기\n",
        "\n",
        "**Index**\n",
        "\n",
        "Set up  \n",
        "Remove broken files  \n",
        "Check data for training  \n",
        "Dataloader  \n",
        "Create a model  \n",
        "Custom Trainer  \n",
        "Training  \n",
        "Test model  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5a1b70e",
      "metadata": {
        "id": "d5a1b70e"
      },
      "source": [
        "### Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43d092a3",
      "metadata": {
        "id": "43d092a3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a004bfc4",
      "metadata": {
        "id": "a004bfc4"
      },
      "source": [
        "### Remove broken files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0925f735",
      "metadata": {
        "id": "0925f735"
      },
      "outputs": [],
      "source": [
        "# training set과 test set의 모든 이미지 파일에 대해서,\n",
        "# jpg image header가 포함되지 않은 (jpg의 파일 구조에 어긋나는) 파일들을 삭제해줍니다.\n",
        "\n",
        "data_path = '/aiffel/aiffel/model-fit/data/30vnfoods/'\n",
        "train_path = data_path + 'Train/'\n",
        "test_path = data_path + 'Test/'\n",
        "\n",
        "for path in [train_path, test_path]:\n",
        "    classes = os.listdir(path)\n",
        "\n",
        "    for food in classes:\n",
        "        food_path = os.path.join(path, food)\n",
        "        images = os.listdir(food_path)\n",
        "\n",
        "        for image in images:\n",
        "            with open(os.path.join(food_path, image), 'rb') as f:\n",
        "                bytes = f.read()\n",
        "            if bytes[:3] != b'\\xff\\xd8\\xff':\n",
        "                print(os.path.join(food_path, image))\n",
        "                os.remove(os.path.join(food_path, image))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82285b06",
      "metadata": {
        "id": "82285b06"
      },
      "source": [
        "### Check data for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffe56d0a",
      "metadata": {
        "id": "ffe56d0a",
        "outputId": "4af0e2e0-4bd0-4f36-bff4-6cb8c2a94627"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training data의 개수: 9775\n"
          ]
        }
      ],
      "source": [
        "classes = os.listdir(train_path)\n",
        "train_length = 0\n",
        "\n",
        "for food in classes:\n",
        "    food_path = os.path.join(train_path, food)\n",
        "    images = os.listdir(food_path)\n",
        "\n",
        "    train_length += len(images)\n",
        "\n",
        "print('training data의 개수: '+str(train_length))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8182c7d1",
      "metadata": {
        "id": "8182c7d1"
      },
      "source": [
        "### Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "782e5ad8",
      "metadata": {
        "id": "782e5ad8"
      },
      "outputs": [],
      "source": [
        "# 문제 1: dataloader 구현하기\n",
        "\n",
        "def process_path(file_path, class_names, img_shape=(224, 224)):\n",
        "    '''\n",
        "    file_path로 부터 class label을 만들고, 이미지를 읽는 함수\n",
        "    이미지 크기를 (224, 224)로 맞춰주세요.\n",
        "    '''\n",
        "    label = tf.strings.split(file_path, os.path.sep)\n",
        "    label = label[-2] == class_names\n",
        "\n",
        "    img = tf.io.read_file(file_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    img = tf.image.resize(img, img_shape)\n",
        "\n",
        "    return img, label\n",
        "\n",
        "def prepare_for_training(ds, batch_size=32, cache=True, shuffle_buffer_size=1000):\n",
        "    '''\n",
        "    TensorFlow Data API를 이용해 data batch를 만드는 함수\n",
        "    '''\n",
        "    if cache:\n",
        "        if isinstance(cache, str):\n",
        "            ds = ds.cache(cache)\n",
        "        else:\n",
        "            ds = ds.cache()\n",
        "\n",
        "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
        "    ds = ds.repeat()\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    return ds\n",
        "\n",
        "def load_data(data_path, batch_size=32):\n",
        "    '''\n",
        "    데이터를 만들기 위해 필요한 함수들을 호출하고 데이터를 리턴해주는 함수\n",
        "    TensorFlow Dataset 객체를 생성하고 process_path 함수로 이미지와 라벨을 묶은 다음,\n",
        "    prepare_for_training 함수로 batch가 적용된 Dataset 객체를 만들어주세요.\n",
        "    '''\n",
        "    class_names = [cls for cls in os.listdir(data_path) if cls != '.DS_Store']\n",
        "    print(class_names)\n",
        "    data_path = pathlib.Path(data_path)\n",
        "\n",
        "    list_ds = tf.data.Dataset.list_files(str(data_path/'*/*'))\n",
        "    labeled_ds = list_ds.map(lambda x: process_path(x, class_names, img_shape=(224, 224)))\n",
        "    ds = prepare_for_training(labeled_ds, batch_size=batch_size)\n",
        "\n",
        "    return ds"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d50f5ca9",
      "metadata": {
        "id": "d50f5ca9"
      },
      "source": [
        "### Create a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f045c9e2",
      "metadata": {
        "id": "f045c9e2",
        "outputId": "1ef7f545-6ea2-42ce-e069-fb7e0a611a8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "efficientnetb0 (Functional)  (None, None, None, 1280)  4049571   \n",
            "_________________________________________________________________\n",
            "sequential (Sequential)      (None, 1280)              5120      \n",
            "_________________________________________________________________\n",
            "pred (Dense)                 multiple                  6405      \n",
            "=================================================================\n",
            "Total params: 4,061,096\n",
            "Trainable params: 8,965\n",
            "Non-trainable params: 4,052,131\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# 문제 2: 모델 구현하기\n",
        "\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "\n",
        "class Model(tf.keras.Model):\n",
        "    '''\n",
        "    EfficientNetB0을 백본으로 사용하는 모델을 구성합니다.\n",
        "    Classification 문제로 접근할 것이기 때문에 맨 마지막 Dense 레이어에\n",
        "    우리가 원하는 클래스 개수만큼을 지정해주어야 합니다.\n",
        "    '''\n",
        "    def __init__(self, num_classes=10, freeze=False):\n",
        "        super(Model, self).__init__()\n",
        "        self.base_model = EfficientNetB0(include_top=False, weights='imagenet')\n",
        "        if freeze:\n",
        "            self.base_model.trainable = False\n",
        "        self.top = tf.keras.Sequential([tf.keras.layers.GlobalAveragePooling2D(name=\"avg_pool\"),\n",
        "                                       tf.keras.layers.BatchNormalization(),\n",
        "                                       tf.keras.layers.Dropout(0.5, name=\"top_dropout\")])\n",
        "        self.classifier = tf.keras.layers.Dense(num_classes, activation=\"softmax\", name=\"pred\")\n",
        "    def call(self, inputs, training=True):\n",
        "        x = self.base_model(inputs)\n",
        "        x = self.top(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = Model(num_classes=5, freeze=True)\n",
        "    model.build(input_shape=(None, 224, 224, 3))\n",
        "    print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f95b9935",
      "metadata": {
        "id": "f95b9935"
      },
      "source": [
        "### Custom Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19f935e1",
      "metadata": {
        "id": "19f935e1"
      },
      "outputs": [],
      "source": [
        "# 문제 3: custom trainer 구현하기\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, epochs, batch, ds_length, loss_fn, optimizer):\n",
        "        self.model = model\n",
        "        self.epochs = epochs\n",
        "        self.batch = batch\n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "    def train(self, train_dataset, train_metric):\n",
        "        for epoch in range(self.epochs):\n",
        "            print(\"\\nStart of epoch %d\" % (epoch+1,))\n",
        "            for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "                with tf.GradientTape() as tape:\n",
        "                    logits = model(x_batch_train, training=True)\n",
        "                    loss_value = self.loss_fn(y_batch_train, logits)\n",
        "                grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "                self.optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "                # train metric 업데이트\n",
        "                train_metric.update_state(y_batch_train, logits)\n",
        "                # 5 배치마다 로깅\n",
        "                if step % 5 == 0:\n",
        "                    print(\n",
        "                        \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                        % (step, float(loss_value))\n",
        "                    )\n",
        "                    print(\"Seen so far: %d samples\" % ((step + 1) * self.batch))\n",
        "                    print(train_metric.result().numpy())\n",
        "            # 마지막 epoch 학습이 끝나면 train 결과를 보여줌\n",
        "            train_acc = train_acc_metric.result()\n",
        "            print(\"Training acc over epoch: %.4f\" % (float(train_acc),))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "432c95be",
      "metadata": {
        "id": "432c95be"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ae8ce3d",
      "metadata": {
        "id": "1ae8ce3d",
        "outputId": "0f75bca3-c450-4e2e-b796-950cf3b32e56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Bun rieu', 'Banh mi', 'Banh xeo', 'Chao long', 'Pho', 'Banh khot', 'Bun bo Hue', 'Banh cuon', 'Com tam', 'Bun dau mam tom']\n",
            "\n",
            "Start of epoch 1\n",
            "Training loss (for one batch) at step 0: 3.8306\n",
            "Seen so far: 32 samples\n",
            "0.0625\n",
            "Training loss (for one batch) at step 5: 2.6737\n",
            "Seen so far: 192 samples\n",
            "0.17708333\n",
            "Training loss (for one batch) at step 10: 2.4080\n",
            "Seen so far: 352 samples\n",
            "0.23579545\n",
            "Training loss (for one batch) at step 15: 1.7167\n",
            "Seen so far: 512 samples\n",
            "0.30859375\n",
            "Training loss (for one batch) at step 20: 1.3033\n",
            "Seen so far: 672 samples\n",
            "0.3705357\n",
            "Training loss (for one batch) at step 25: 1.0595\n",
            "Seen so far: 832 samples\n",
            "0.390625\n",
            "Training loss (for one batch) at step 30: 1.6153\n",
            "Seen so far: 992 samples\n",
            "0.41330644\n",
            "Training loss (for one batch) at step 35: 1.3126\n",
            "Seen so far: 1152 samples\n",
            "0.4513889\n",
            "Training loss (for one batch) at step 40: 1.2370\n",
            "Seen so far: 1312 samples\n",
            "0.47408536\n",
            "Training loss (for one batch) at step 45: 1.4075\n",
            "Seen so far: 1472 samples\n",
            "0.49388587\n",
            "Training loss (for one batch) at step 50: 1.3065\n",
            "Seen so far: 1632 samples\n",
            "0.50919116\n",
            "Training loss (for one batch) at step 55: 0.8819\n",
            "Seen so far: 1792 samples\n",
            "0.5245536\n",
            "Training loss (for one batch) at step 60: 0.9567\n",
            "Seen so far: 1952 samples\n",
            "0.53790987\n",
            "Training loss (for one batch) at step 65: 1.1199\n",
            "Seen so far: 2112 samples\n",
            "0.5535038\n",
            "Training loss (for one batch) at step 70: 0.8969\n",
            "Seen so far: 2272 samples\n",
            "0.5638204\n",
            "Training loss (for one batch) at step 75: 0.6216\n",
            "Seen so far: 2432 samples\n",
            "0.5777138\n",
            "Training loss (for one batch) at step 80: 2.1359\n",
            "Seen so far: 2592 samples\n",
            "0.5856481\n",
            "Training loss (for one batch) at step 85: 1.3386\n",
            "Seen so far: 2752 samples\n",
            "0.58902615\n",
            "Training loss (for one batch) at step 90: 0.7103\n",
            "Seen so far: 2912 samples\n",
            "0.59752744\n",
            "Training loss (for one batch) at step 95: 0.3903\n",
            "Seen so far: 3072 samples\n",
            "0.6061198\n",
            "Training loss (for one batch) at step 100: 0.7734\n",
            "Seen so far: 3232 samples\n",
            "0.6144802\n",
            "Training loss (for one batch) at step 105: 0.7709\n",
            "Seen so far: 3392 samples\n",
            "0.6196934\n",
            "Training loss (for one batch) at step 110: 1.3221\n",
            "Seen so far: 3552 samples\n",
            "0.6238739\n",
            "Training loss (for one batch) at step 115: 0.9299\n",
            "Seen so far: 3712 samples\n",
            "0.63119614\n",
            "Training loss (for one batch) at step 120: 0.6887\n",
            "Seen so far: 3872 samples\n",
            "0.63610536\n",
            "Training loss (for one batch) at step 125: 0.8055\n",
            "Seen so far: 4032 samples\n",
            "0.64161706\n",
            "Training loss (for one batch) at step 130: 1.3243\n",
            "Seen so far: 4192 samples\n",
            "0.6452767\n",
            "Training loss (for one batch) at step 135: 0.5182\n",
            "Seen so far: 4352 samples\n",
            "0.6493566\n",
            "Training loss (for one batch) at step 140: 0.6455\n",
            "Seen so far: 4512 samples\n",
            "0.65381205\n",
            "Training loss (for one batch) at step 145: 0.9931\n",
            "Seen so far: 4672 samples\n",
            "0.66074485\n",
            "Training loss (for one batch) at step 150: 0.7219\n",
            "Seen so far: 4832 samples\n",
            "0.66411424\n",
            "Training loss (for one batch) at step 155: 0.4802\n",
            "Seen so far: 4992 samples\n",
            "0.6666667\n",
            "Training loss (for one batch) at step 160: 0.3905\n",
            "Seen so far: 5152 samples\n",
            "0.67197204\n",
            "Training loss (for one batch) at step 165: 0.2241\n",
            "Seen so far: 5312 samples\n",
            "0.6763931\n",
            "Training loss (for one batch) at step 170: 0.6011\n",
            "Seen so far: 5472 samples\n",
            "0.6789108\n",
            "Training loss (for one batch) at step 175: 0.6007\n",
            "Seen so far: 5632 samples\n",
            "0.6805753\n",
            "Training loss (for one batch) at step 180: 0.4419\n",
            "Seen so far: 5792 samples\n",
            "0.6838743\n",
            "Training loss (for one batch) at step 185: 0.3537\n",
            "Seen so far: 5952 samples\n",
            "0.68665993\n",
            "Training loss (for one batch) at step 190: 0.5778\n",
            "Seen so far: 6112 samples\n",
            "0.6886453\n",
            "Training loss (for one batch) at step 195: 0.6222\n",
            "Seen so far: 6272 samples\n",
            "0.69260204\n",
            "Training loss (for one batch) at step 200: 1.0933\n",
            "Seen so far: 6432 samples\n",
            "0.6944963\n",
            "Training loss (for one batch) at step 205: 0.9546\n",
            "Seen so far: 6592 samples\n",
            "0.6969053\n",
            "Training loss (for one batch) at step 210: 0.5548\n",
            "Seen so far: 6752 samples\n",
            "0.70008886\n",
            "Training loss (for one batch) at step 215: 0.3432\n",
            "Seen so far: 6912 samples\n",
            "0.7034144\n",
            "Training loss (for one batch) at step 220: 0.6657\n",
            "Seen so far: 7072 samples\n",
            "0.70475113\n",
            "Training loss (for one batch) at step 225: 0.3426\n",
            "Seen so far: 7232 samples\n",
            "0.70616704\n",
            "Training loss (for one batch) at step 230: 0.3159\n",
            "Seen so far: 7392 samples\n",
            "0.709145\n",
            "Training loss (for one batch) at step 235: 0.5639\n",
            "Seen so far: 7552 samples\n",
            "0.7106727\n",
            "Training loss (for one batch) at step 240: 0.6705\n",
            "Seen so far: 7712 samples\n",
            "0.7130446\n",
            "Training loss (for one batch) at step 245: 0.7172\n",
            "Seen so far: 7872 samples\n",
            "0.71430385\n",
            "Training loss (for one batch) at step 250: 1.2089\n",
            "Seen so far: 8032 samples\n",
            "0.71526396\n",
            "Training loss (for one batch) at step 255: 0.7105\n",
            "Seen so far: 8192 samples\n",
            "0.71765137\n",
            "Training loss (for one batch) at step 260: 0.5326\n",
            "Seen so far: 8352 samples\n",
            "0.71934867\n",
            "Training loss (for one batch) at step 265: 1.0235\n",
            "Seen so far: 8512 samples\n",
            "0.72027725\n",
            "Training loss (for one batch) at step 270: 0.6331\n",
            "Seen so far: 8672 samples\n",
            "0.72094095\n",
            "Training loss (for one batch) at step 275: 0.8547\n",
            "Seen so far: 8832 samples\n",
            "0.72225994\n",
            "Training loss (for one batch) at step 280: 0.6884\n",
            "Seen so far: 8992 samples\n",
            "0.7241993\n",
            "Training loss (for one batch) at step 285: 0.4187\n",
            "Seen so far: 9152 samples\n",
            "0.72563374\n",
            "Training loss (for one batch) at step 290: 0.8589\n",
            "Seen so far: 9312 samples\n",
            "0.7276632\n",
            "Training loss (for one batch) at step 295: 0.4161\n",
            "Seen so far: 9472 samples\n",
            "0.7288851\n",
            "Training loss (for one batch) at step 300: 0.5196\n",
            "Seen so far: 9632 samples\n",
            "0.73100084\n",
            "Training loss (for one batch) at step 305: 0.6336\n",
            "Seen so far: 9792 samples\n",
            "0.7326389\n",
            "Training loss (for one batch) at step 310: 0.6427\n",
            "Seen so far: 9952 samples\n",
            "0.73432475\n",
            "Training loss (for one batch) at step 315: 0.8074\n",
            "Seen so far: 10112 samples\n",
            "0.73605615\n",
            "Training loss (for one batch) at step 320: 0.8636\n",
            "Seen so far: 10272 samples\n",
            "0.7369548\n",
            "Training loss (for one batch) at step 325: 0.5244\n",
            "Seen so far: 10432 samples\n",
            "0.7383052\n",
            "Training loss (for one batch) at step 330: 0.6438\n",
            "Seen so far: 10592 samples\n",
            "0.7392372\n",
            "Training loss (for one batch) at step 335: 0.6670\n",
            "Seen so far: 10752 samples\n",
            "0.74032736\n",
            "Training loss (for one batch) at step 340: 0.4991\n",
            "Seen so far: 10912 samples\n",
            "0.74166054\n",
            "Training loss (for one batch) at step 345: 0.6476\n",
            "Seen so far: 11072 samples\n",
            "0.74277455\n",
            "Training loss (for one batch) at step 350: 0.3142\n",
            "Seen so far: 11232 samples\n",
            "0.74358976\n",
            "Training loss (for one batch) at step 355: 0.7612\n",
            "Seen so far: 11392 samples\n",
            "0.7442942\n",
            "Training loss (for one batch) at step 360: 0.4800\n",
            "Seen so far: 11552 samples\n",
            "0.74567175\n",
            "Training loss (for one batch) at step 365: 0.3663\n",
            "Seen so far: 11712 samples\n",
            "0.7476947\n",
            "Training loss (for one batch) at step 370: 0.4839\n",
            "Seen so far: 11872 samples\n",
            "0.7497473\n",
            "Training loss (for one batch) at step 375: 0.4561\n",
            "Seen so far: 12032 samples\n",
            "0.75116354\n",
            "Training loss (for one batch) at step 380: 0.4322\n",
            "Seen so far: 12192 samples\n",
            "0.7523786\n",
            "Training loss (for one batch) at step 385: 0.6626\n",
            "Seen so far: 12352 samples\n",
            "0.75340027\n",
            "Training loss (for one batch) at step 390: 0.4895\n",
            "Seen so far: 12512 samples\n",
            "0.75503516\n",
            "Training loss (for one batch) at step 395: 0.5209\n",
            "Seen so far: 12672 samples\n",
            "0.7566288\n",
            "Training loss (for one batch) at step 400: 0.1484\n",
            "Seen so far: 12832 samples\n",
            "0.7583385\n",
            "Training loss (for one batch) at step 405: 0.3899\n",
            "Seen so far: 12992 samples\n",
            "0.75946736\n",
            "Training loss (for one batch) at step 410: 0.2711\n",
            "Seen so far: 13152 samples\n",
            "0.7608729\n",
            "Training loss (for one batch) at step 415: 0.2826\n",
            "Seen so far: 13312 samples\n",
            "0.7619441\n",
            "Training loss (for one batch) at step 420: 0.6044\n",
            "Seen so far: 13472 samples\n",
            "0.76284146\n",
            "Training loss (for one batch) at step 425: 0.5892\n",
            "Seen so far: 13632 samples\n",
            "0.7641579\n",
            "Training loss (for one batch) at step 430: 0.4330\n",
            "Seen so far: 13792 samples\n",
            "0.7652987\n",
            "Training loss (for one batch) at step 435: 0.4323\n",
            "Seen so far: 13952 samples\n",
            "0.7667718\n",
            "Training loss (for one batch) at step 440: 0.1938\n",
            "Seen so far: 14112 samples\n",
            "0.7682823\n",
            "Training loss (for one batch) at step 445: 0.3230\n",
            "Seen so far: 14272 samples\n",
            "0.76940864\n",
            "Training loss (for one batch) at step 450: 0.2937\n",
            "Seen so far: 14432 samples\n",
            "0.7709257\n",
            "Training loss (for one batch) at step 455: 0.3408\n",
            "Seen so far: 14592 samples\n",
            "0.7720669\n",
            "Training loss (for one batch) at step 460: 0.2759\n",
            "Seen so far: 14752 samples\n",
            "0.77304775\n",
            "Training loss (for one batch) at step 465: 0.7100\n",
            "Seen so far: 14912 samples\n",
            "0.7736722\n",
            "Training loss (for one batch) at step 470: 0.5302\n",
            "Seen so far: 15072 samples\n",
            "0.77501327\n",
            "Training loss (for one batch) at step 475: 0.6001\n",
            "Seen so far: 15232 samples\n",
            "0.77606356\n",
            "Training loss (for one batch) at step 480: 0.2001\n",
            "Seen so far: 15392 samples\n",
            "0.7772219\n",
            "Training loss (for one batch) at step 485: 0.4899\n",
            "Seen so far: 15552 samples\n",
            "0.778678\n",
            "Training loss (for one batch) at step 490: 0.1587\n",
            "Seen so far: 15712 samples\n",
            "0.77978617\n",
            "Training loss (for one batch) at step 495: 0.1410\n",
            "Seen so far: 15872 samples\n",
            "0.780872\n",
            "Training loss (for one batch) at step 500: 0.5285\n",
            "Seen so far: 16032 samples\n",
            "0.781749\n",
            "Training loss (for one batch) at step 505: 0.4256\n",
            "Seen so far: 16192 samples\n",
            "0.78267044\n",
            "Training loss (for one batch) at step 510: 0.2380\n",
            "Seen so far: 16352 samples\n",
            "0.78357387\n",
            "Training loss (for one batch) at step 515: 0.2717\n",
            "Seen so far: 16512 samples\n",
            "0.78470206\n",
            "Training loss (for one batch) at step 520: 0.7674\n",
            "Seen so far: 16672 samples\n",
            "0.7850288\n",
            "Training loss (for one batch) at step 525: 0.3118\n",
            "Seen so far: 16832 samples\n",
            "0.7862405\n",
            "Training loss (for one batch) at step 530: 0.2769\n",
            "Seen so far: 16992 samples\n",
            "0.7870174\n",
            "Training loss (for one batch) at step 535: 0.3220\n",
            "Seen so far: 17152 samples\n",
            "0.78801304\n",
            "Training loss (for one batch) at step 540: 0.2534\n",
            "Seen so far: 17312 samples\n",
            "0.7892791\n",
            "Training loss (for one batch) at step 545: 0.2882\n",
            "Seen so far: 17472 samples\n",
            "0.7902358\n",
            "Training loss (for one batch) at step 550: 0.1265\n",
            "Seen so far: 17632 samples\n",
            "0.79128855\n",
            "Training loss (for one batch) at step 555: 0.4999\n",
            "Seen so far: 17792 samples\n",
            "0.7922662\n",
            "Training loss (for one batch) at step 560: 0.3917\n",
            "Seen so far: 17952 samples\n",
            "0.792725\n",
            "Training loss (for one batch) at step 565: 0.2805\n",
            "Seen so far: 18112 samples\n",
            "0.7936175\n",
            "Training loss (for one batch) at step 570: 0.3242\n",
            "Seen so far: 18272 samples\n",
            "0.7942207\n",
            "Training loss (for one batch) at step 575: 0.3504\n",
            "Seen so far: 18432 samples\n",
            "0.79519314\n",
            "Training loss (for one batch) at step 580: 0.0860\n",
            "Seen so far: 18592 samples\n",
            "0.7960413\n",
            "Training loss (for one batch) at step 585: 0.0956\n",
            "Seen so far: 18752 samples\n",
            "0.79682165\n",
            "Training loss (for one batch) at step 590: 0.2429\n",
            "Seen so far: 18912 samples\n",
            "0.7975888\n",
            "Training loss (for one batch) at step 595: 0.4352\n",
            "Seen so far: 19072 samples\n",
            "0.79844797\n",
            "Training loss (for one batch) at step 600: 0.4323\n",
            "Seen so far: 19232 samples\n",
            "0.7989809\n",
            "Training loss (for one batch) at step 605: 0.2974\n",
            "Seen so far: 19392 samples\n",
            "0.7997112\n",
            "Training loss (for one batch) at step 610: 0.5997\n",
            "Seen so far: 19552 samples\n",
            "0.80027616\n",
            "Training loss (for one batch) at step 615: 0.2563\n",
            "Seen so far: 19712 samples\n",
            "0.8008827\n",
            "Training loss (for one batch) at step 620: 0.3183\n",
            "Seen so far: 19872 samples\n",
            "0.80173105\n",
            "Training loss (for one batch) at step 625: 0.1883\n",
            "Seen so far: 20032 samples\n",
            "0.802516\n",
            "Training loss (for one batch) at step 630: 0.4720\n",
            "Seen so far: 20192 samples\n",
            "0.8031894\n",
            "Training loss (for one batch) at step 635: 0.4752\n",
            "Seen so far: 20352 samples\n",
            "0.8036065\n",
            "Training loss (for one batch) at step 640: 0.1507\n",
            "Seen so far: 20512 samples\n",
            "0.8044072\n",
            "Training loss (for one batch) at step 645: 0.2864\n",
            "Seen so far: 20672 samples\n",
            "0.80514705\n",
            "Training loss (for one batch) at step 650: 0.2400\n",
            "Seen so far: 20832 samples\n",
            "0.8058756\n",
            "Training loss (for one batch) at step 655: 0.6198\n",
            "Seen so far: 20992 samples\n",
            "0.8066406\n",
            "Training loss (for one batch) at step 660: 0.2440\n",
            "Seen so far: 21152 samples\n",
            "0.80753595\n",
            "Training loss (for one batch) at step 665: 0.1275\n",
            "Seen so far: 21312 samples\n",
            "0.8081832\n",
            "Training loss (for one batch) at step 670: 0.1585\n",
            "Seen so far: 21472 samples\n",
            "0.80886734\n",
            "Training loss (for one batch) at step 675: 0.2014\n",
            "Seen so far: 21632 samples\n",
            "0.8096801\n",
            "Training loss (for one batch) at step 680: 0.1683\n",
            "Seen so far: 21792 samples\n",
            "0.810435\n",
            "Training loss (for one batch) at step 685: 0.2754\n",
            "Seen so far: 21952 samples\n",
            "0.81140673\n",
            "Training loss (for one batch) at step 690: 0.4975\n",
            "Seen so far: 22112 samples\n",
            "0.8117764\n",
            "Training loss (for one batch) at step 695: 0.2446\n",
            "Seen so far: 22272 samples\n",
            "0.8126347\n",
            "Training loss (for one batch) at step 700: 0.2889\n",
            "Seen so far: 22432 samples\n",
            "0.8133024\n",
            "Training loss (for one batch) at step 705: 0.6221\n",
            "Seen so far: 22592 samples\n",
            "0.81400496\n",
            "Training loss (for one batch) at step 710: 0.1337\n",
            "Seen so far: 22752 samples\n",
            "0.8145658\n",
            "Training loss (for one batch) at step 715: 0.3326\n",
            "Seen so far: 22912 samples\n",
            "0.8154679\n",
            "Training loss (for one batch) at step 720: 0.2667\n",
            "Seen so far: 23072 samples\n",
            "0.8161841\n",
            "Training loss (for one batch) at step 725: 0.1552\n",
            "Seen so far: 23232 samples\n",
            "0.8168044\n",
            "Training loss (for one batch) at step 730: 0.4299\n",
            "Seen so far: 23392 samples\n",
            "0.81754446\n",
            "Training loss (for one batch) at step 735: 0.3413\n",
            "Seen so far: 23552 samples\n",
            "0.8180622\n",
            "Training loss (for one batch) at step 740: 0.0657\n",
            "Seen so far: 23712 samples\n",
            "0.8189946\n",
            "Training loss (for one batch) at step 745: 0.1214\n",
            "Seen so far: 23872 samples\n",
            "0.8196632\n",
            "Training loss (for one batch) at step 750: 0.4196\n",
            "Seen so far: 24032 samples\n",
            "0.81999004\n",
            "Training loss (for one batch) at step 755: 0.1920\n",
            "Seen so far: 24192 samples\n",
            "0.82080853\n",
            "Training loss (for one batch) at step 760: 0.2908\n",
            "Seen so far: 24352 samples\n",
            "0.8213699\n",
            "Training loss (for one batch) at step 765: 0.3646\n",
            "Seen so far: 24512 samples\n",
            "0.82196474\n",
            "Training loss (for one batch) at step 770: 0.1297\n",
            "Seen so far: 24672 samples\n",
            "0.82259244\n",
            "Training loss (for one batch) at step 775: 0.0403\n",
            "Seen so far: 24832 samples\n",
            "0.82329255\n",
            "Training loss (for one batch) at step 780: 0.1868\n",
            "Seen so far: 24992 samples\n",
            "0.82402366\n",
            "Training loss (for one batch) at step 785: 0.0490\n",
            "Seen so far: 25152 samples\n",
            "0.82482505\n",
            "Training loss (for one batch) at step 790: 0.1186\n",
            "Seen so far: 25312 samples\n",
            "0.82526076\n",
            "Training loss (for one batch) at step 795: 0.6123\n",
            "Seen so far: 25472 samples\n",
            "0.8257302\n",
            "Training loss (for one batch) at step 800: 0.1794\n",
            "Seen so far: 25632 samples\n",
            "0.8264669\n",
            "Training loss (for one batch) at step 805: 0.1634\n",
            "Seen so far: 25792 samples\n",
            "0.8271169\n",
            "Training loss (for one batch) at step 810: 0.1236\n",
            "Seen so far: 25952 samples\n",
            "0.8276819\n",
            "Training loss (for one batch) at step 815: 0.1368\n",
            "Seen so far: 26112 samples\n",
            "0.82843137\n",
            "Training loss (for one batch) at step 820: 0.1570\n",
            "Seen so far: 26272 samples\n",
            "0.8290576\n",
            "Training loss (for one batch) at step 825: 0.1951\n",
            "Seen so far: 26432 samples\n",
            "0.8295248\n",
            "Training loss (for one batch) at step 830: 0.5774\n",
            "Seen so far: 26592 samples\n",
            "0.83002406\n",
            "Training loss (for one batch) at step 835: 0.2061\n",
            "Seen so far: 26752 samples\n",
            "0.8304426\n",
            "Training loss (for one batch) at step 840: 0.3577\n",
            "Seen so far: 26912 samples\n",
            "0.83085614\n",
            "Training loss (for one batch) at step 845: 0.2616\n",
            "Seen so far: 27072 samples\n",
            "0.83133864\n",
            "Training loss (for one batch) at step 850: 0.1654\n",
            "Seen so far: 27232 samples\n",
            "0.8319624\n",
            "Training loss (for one batch) at step 855: 0.1741\n",
            "Seen so far: 27392 samples\n",
            "0.8322868\n",
            "Training loss (for one batch) at step 860: 0.1464\n",
            "Seen so far: 27552 samples\n",
            "0.8327526\n",
            "Training loss (for one batch) at step 865: 0.2202\n",
            "Seen so far: 27712 samples\n",
            "0.8333574\n",
            "Training loss (for one batch) at step 870: 0.4102\n",
            "Seen so far: 27872 samples\n",
            "0.8336323\n",
            "Training loss (for one batch) at step 875: 0.4618\n",
            "Seen so far: 28032 samples\n",
            "0.8340468\n",
            "Training loss (for one batch) at step 880: 0.3157\n",
            "Seen so far: 28192 samples\n",
            "0.8342438\n",
            "Training loss (for one batch) at step 885: 0.2306\n",
            "Seen so far: 28352 samples\n",
            "0.8347559\n",
            "Training loss (for one batch) at step 890: 0.0885\n",
            "Seen so far: 28512 samples\n",
            "0.8353325\n",
            "Training loss (for one batch) at step 895: 0.0999\n",
            "Seen so far: 28672 samples\n",
            "0.83590263\n",
            "Training loss (for one batch) at step 900: 0.5301\n",
            "Seen so far: 28832 samples\n",
            "0.8361543\n",
            "Training loss (for one batch) at step 905: 0.1766\n",
            "Seen so far: 28992 samples\n",
            "0.8361962\n",
            "Training loss (for one batch) at step 910: 0.3524\n",
            "Seen so far: 29152 samples\n",
            "0.83654636\n",
            "Training loss (for one batch) at step 915: 0.5006\n",
            "Seen so far: 29312 samples\n",
            "0.83685863\n",
            "Training loss (for one batch) at step 920: 0.3830\n",
            "Seen so far: 29472 samples\n",
            "0.83737105\n",
            "Training loss (for one batch) at step 925: 0.4384\n",
            "Seen so far: 29632 samples\n",
            "0.8376417\n",
            "Training loss (for one batch) at step 930: 0.3142\n",
            "Seen so far: 29792 samples\n",
            "0.8382116\n",
            "Training loss (for one batch) at step 935: 0.0807\n",
            "Seen so far: 29952 samples\n",
            "0.8384415\n",
            "Training loss (for one batch) at step 940: 0.1820\n",
            "Seen so far: 30112 samples\n",
            "0.8387354\n",
            "Training loss (for one batch) at step 945: 0.3090\n",
            "Seen so far: 30272 samples\n",
            "0.8390592\n",
            "Training loss (for one batch) at step 950: 0.0880\n",
            "Seen so far: 30432 samples\n",
            "0.83964247\n",
            "Training loss (for one batch) at step 955: 0.3622\n",
            "Seen so far: 30592 samples\n",
            "0.8401216\n",
            "Training loss (for one batch) at step 960: 0.3754\n",
            "Seen so far: 30752 samples\n",
            "0.84049815\n",
            "Training loss (for one batch) at step 965: 0.3432\n",
            "Seen so far: 30912 samples\n",
            "0.8408385\n",
            "Training loss (for one batch) at step 970: 0.0219\n",
            "Seen so far: 31072 samples\n",
            "0.84114313\n",
            "Training loss (for one batch) at step 975: 0.3971\n",
            "Seen so far: 31232 samples\n",
            "0.8414767\n",
            "Training loss (for one batch) at step 980: 0.2573\n",
            "Seen so far: 31392 samples\n",
            "0.8419024\n",
            "Training loss (for one batch) at step 985: 0.1592\n",
            "Seen so far: 31552 samples\n",
            "0.84245056\n",
            "Training loss (for one batch) at step 990: 0.0290\n",
            "Seen so far: 31712 samples\n",
            "0.84296167\n",
            "Training loss (for one batch) at step 995: 0.1051\n",
            "Seen so far: 31872 samples\n",
            "0.84356177\n",
            "Training loss (for one batch) at step 1000: 0.2321\n",
            "Seen so far: 32032 samples\n",
            "0.8441246\n",
            "Training loss (for one batch) at step 1005: 0.0958\n",
            "Seen so far: 32192 samples\n",
            "0.84465086\n",
            "Training loss (for one batch) at step 1010: 0.1696\n",
            "Seen so far: 32352 samples\n",
            "0.84504825\n",
            "Training loss (for one batch) at step 1015: 0.0246\n",
            "Seen so far: 32512 samples\n",
            "0.8455032\n",
            "Training loss (for one batch) at step 1020: 0.1531\n",
            "Seen so far: 32672 samples\n",
            "0.84577006\n",
            "Training loss (for one batch) at step 1025: 0.1846\n",
            "Seen so far: 32832 samples\n",
            "0.84609526\n",
            "Training loss (for one batch) at step 1030: 0.1226\n",
            "Seen so far: 32992 samples\n",
            "0.84650826\n",
            "Training loss (for one batch) at step 1035: 0.2811\n",
            "Seen so far: 33152 samples\n",
            "0.84697753\n",
            "Training loss (for one batch) at step 1040: 0.1601\n",
            "Seen so far: 33312 samples\n",
            "0.8473523\n",
            "Training loss (for one batch) at step 1045: 0.1123\n",
            "Seen so far: 33472 samples\n",
            "0.847843\n",
            "Training loss (for one batch) at step 1050: 0.0915\n",
            "Seen so far: 33632 samples\n",
            "0.8481208\n",
            "Training loss (for one batch) at step 1055: 0.2642\n",
            "Seen so far: 33792 samples\n",
            "0.8483665\n",
            "Training loss (for one batch) at step 1060: 0.1755\n",
            "Seen so far: 33952 samples\n",
            "0.84878653\n",
            "Training loss (for one batch) at step 1065: 0.1096\n",
            "Seen so far: 34112 samples\n",
            "0.84929055\n",
            "Training loss (for one batch) at step 1070: 0.3377\n",
            "Seen so far: 34272 samples\n",
            "0.849644\n",
            "Training loss (for one batch) at step 1075: 0.1604\n",
            "Seen so far: 34432 samples\n",
            "0.85011035\n",
            "Training loss (for one batch) at step 1080: 0.3232\n",
            "Seen so far: 34592 samples\n",
            "0.8505146\n",
            "Training loss (for one batch) at step 1085: 0.0636\n",
            "Seen so far: 34752 samples\n",
            "0.8509438\n",
            "Training loss (for one batch) at step 1090: 0.1468\n",
            "Seen so far: 34912 samples\n",
            "0.85128325\n",
            "Training loss (for one batch) at step 1095: 0.1083\n",
            "Seen so far: 35072 samples\n",
            "0.8517906\n",
            "Training loss (for one batch) at step 1100: 0.1852\n",
            "Seen so far: 35232 samples\n",
            "0.8520947\n",
            "Training loss (for one batch) at step 1105: 0.1228\n",
            "Seen so far: 35392 samples\n",
            "0.852509\n",
            "Training loss (for one batch) at step 1110: 0.2186\n",
            "Seen so far: 35552 samples\n",
            "0.85289156\n",
            "Training loss (for one batch) at step 1115: 0.0859\n",
            "Seen so far: 35712 samples\n",
            "0.85335463\n",
            "Training loss (for one batch) at step 1120: 0.3385\n",
            "Seen so far: 35872 samples\n",
            "0.8538136\n",
            "Training loss (for one batch) at step 1125: 0.2477\n",
            "Seen so far: 36032 samples\n",
            "0.8541019\n",
            "Training loss (for one batch) at step 1130: 0.1870\n",
            "Seen so far: 36192 samples\n",
            "0.85441536\n",
            "Training loss (for one batch) at step 1135: 0.1425\n",
            "Seen so far: 36352 samples\n",
            "0.854726\n",
            "Training loss (for one batch) at step 1140: 0.1166\n",
            "Seen so far: 36512 samples\n",
            "0.85497916\n",
            "Training loss (for one batch) at step 1145: 0.1037\n",
            "Seen so far: 36672 samples\n",
            "0.85531193\n",
            "Training loss (for one batch) at step 1150: 0.2960\n",
            "Seen so far: 36832 samples\n",
            "0.85561466\n",
            "Training loss (for one batch) at step 1155: 0.0902\n",
            "Seen so far: 36992 samples\n",
            "0.8559148\n",
            "Training loss (for one batch) at step 1160: 0.3425\n",
            "Seen so far: 37152 samples\n",
            "0.8560239\n",
            "Training loss (for one batch) at step 1165: 0.1352\n",
            "Seen so far: 37312 samples\n",
            "0.8563733\n",
            "Training loss (for one batch) at step 1170: 0.1097\n",
            "Seen so far: 37472 samples\n",
            "0.8566129\n",
            "Training loss (for one batch) at step 1175: 0.0458\n",
            "Seen so far: 37632 samples\n",
            "0.85701\n",
            "Training loss (for one batch) at step 1180: 0.2689\n",
            "Seen so far: 37792 samples\n",
            "0.8572449\n",
            "Training loss (for one batch) at step 1185: 0.5198\n",
            "Seen so far: 37952 samples\n",
            "0.8576096\n",
            "Training loss (for one batch) at step 1190: 0.2130\n",
            "Seen so far: 38112 samples\n",
            "0.85784006\n",
            "Training loss (for one batch) at step 1195: 0.1940\n",
            "Seen so far: 38272 samples\n",
            "0.8580947\n",
            "Training loss (for one batch) at step 1200: 0.9278\n",
            "Seen so far: 38432 samples\n",
            "0.8582431\n",
            "Training loss (for one batch) at step 1205: 0.2067\n",
            "Seen so far: 38592 samples\n",
            "0.8583644\n",
            "Training loss (for one batch) at step 1210: 0.2487\n",
            "Seen so far: 38752 samples\n",
            "0.8585621\n",
            "Training loss (for one batch) at step 1215: 0.1364\n",
            "Seen so far: 38912 samples\n",
            "0.85883534\n",
            "Training loss (for one batch) at step 1220: 0.1799\n",
            "Seen so far: 39072 samples\n",
            "0.8590295\n",
            "Training loss (for one batch) at step 1225: 0.3208\n",
            "Seen so far: 39232 samples\n",
            "0.859273\n",
            "Training loss (for one batch) at step 1230: 0.2802\n",
            "Seen so far: 39392 samples\n",
            "0.85948926\n",
            "Training loss (for one batch) at step 1235: 0.4931\n",
            "Seen so far: 39552 samples\n",
            "0.85977954\n",
            "Training loss (for one batch) at step 1240: 0.1487\n",
            "Seen so far: 39712 samples\n",
            "0.86011785\n",
            "Training loss (for one batch) at step 1245: 0.3838\n",
            "Seen so far: 39872 samples\n",
            "0.8604284\n",
            "Training loss (for one batch) at step 1250: 0.2257\n",
            "Seen so far: 40032 samples\n",
            "0.8605366\n",
            "Training loss (for one batch) at step 1255: 0.1949\n",
            "Seen so far: 40192 samples\n",
            "0.86056924\n",
            "Training loss (for one batch) at step 1260: 1.1445\n",
            "Seen so far: 40352 samples\n",
            "0.8606017\n",
            "Training loss (for one batch) at step 1265: 0.2914\n",
            "Seen so far: 40512 samples\n",
            "0.8606339\n",
            "Training loss (for one batch) at step 1270: 0.5867\n",
            "Seen so far: 40672 samples\n",
            "0.8608133\n",
            "Training loss (for one batch) at step 1275: 0.4242\n",
            "Seen so far: 40832 samples\n",
            "0.86096686\n",
            "Training loss (for one batch) at step 1280: 0.1459\n",
            "Seen so far: 40992 samples\n",
            "0.8611924\n",
            "Training loss (for one batch) at step 1285: 0.2503\n",
            "Seen so far: 41152 samples\n",
            "0.8613919\n",
            "Training loss (for one batch) at step 1290: 0.3613\n",
            "Seen so far: 41312 samples\n",
            "0.86156565\n",
            "Training loss (for one batch) at step 1295: 0.1099\n",
            "Seen so far: 41472 samples\n",
            "0.86171395\n",
            "Training loss (for one batch) at step 1300: 0.1237\n",
            "Seen so far: 41632 samples\n",
            "0.86195713\n",
            "Training loss (for one batch) at step 1305: 0.2997\n",
            "Seen so far: 41792 samples\n",
            "0.8621267\n",
            "Training loss (for one batch) at step 1310: 0.0953\n",
            "Seen so far: 41952 samples\n",
            "0.8623665\n",
            "Training loss (for one batch) at step 1315: 0.1105\n",
            "Seen so far: 42112 samples\n",
            "0.86274695\n",
            "Training loss (for one batch) at step 1320: 0.4244\n",
            "Seen so far: 42272 samples\n",
            "0.8628643\n",
            "Training loss (for one batch) at step 1325: 0.4036\n",
            "Seen so far: 42432 samples\n",
            "0.86302793\n",
            "Training loss (for one batch) at step 1330: 0.1793\n",
            "Seen so far: 42592 samples\n",
            "0.8633546\n",
            "Training loss (for one batch) at step 1335: 0.2733\n",
            "Seen so far: 42752 samples\n",
            "0.86372566\n",
            "Training loss (for one batch) at step 1340: 0.0841\n",
            "Seen so far: 42912 samples\n",
            "0.8638143\n",
            "Training loss (for one batch) at step 1345: 0.1833\n",
            "Seen so far: 43072 samples\n",
            "0.8640416\n",
            "Training loss (for one batch) at step 1350: 0.1206\n",
            "Seen so far: 43232 samples\n",
            "0.8643366\n",
            "Training loss (for one batch) at step 1355: 0.1753\n",
            "Seen so far: 43392 samples\n",
            "0.8645833\n",
            "Training loss (for one batch) at step 1360: 0.0657\n",
            "Seen so far: 43552 samples\n",
            "0.86478233\n",
            "Training loss (for one batch) at step 1365: 0.3340\n",
            "Seen so far: 43712 samples\n",
            "0.86497986\n",
            "Training loss (for one batch) at step 1370: 0.1462\n",
            "Seen so far: 43872 samples\n",
            "0.8652443\n",
            "Training loss (for one batch) at step 1375: 0.4725\n",
            "Seen so far: 44032 samples\n",
            "0.86541605\n",
            "Training loss (for one batch) at step 1380: 0.0543\n",
            "Seen so far: 44192 samples\n",
            "0.8657223\n",
            "Training loss (for one batch) at step 1385: 0.1179\n",
            "Seen so far: 44352 samples\n",
            "0.8660714\n",
            "Training loss (for one batch) at step 1390: 0.1197\n",
            "Seen so far: 44512 samples\n",
            "0.8663731\n",
            "Training loss (for one batch) at step 1395: 0.4219\n",
            "Seen so far: 44672 samples\n",
            "0.8665831\n",
            "Training loss (for one batch) at step 1400: 0.0742\n",
            "Seen so far: 44832 samples\n",
            "0.86676925\n",
            "Training loss (for one batch) at step 1405: 0.2979\n",
            "Seen so far: 44992 samples\n",
            "0.86699855\n",
            "Training loss (for one batch) at step 1410: 0.1201\n",
            "Seen so far: 45152 samples\n",
            "0.86715984\n",
            "Training loss (for one batch) at step 1415: 0.2371\n",
            "Seen so far: 45312 samples\n",
            "0.867342\n",
            "Training loss (for one batch) at step 1420: 0.0294\n",
            "Seen so far: 45472 samples\n",
            "0.8675669\n",
            "Training loss (for one batch) at step 1425: 0.2759\n",
            "Seen so far: 45632 samples\n",
            "0.8678778\n",
            "Training loss (for one batch) at step 1430: 0.1061\n",
            "Seen so far: 45792 samples\n",
            "0.8680556\n",
            "Training loss (for one batch) at step 1435: 0.4330\n",
            "Seen so far: 45952 samples\n",
            "0.86829734\n",
            "Training loss (for one batch) at step 1440: 0.2051\n",
            "Seen so far: 46112 samples\n",
            "0.8684507\n",
            "Training loss (for one batch) at step 1445: 0.0561\n",
            "Seen so far: 46272 samples\n",
            "0.8687327\n",
            "Training loss (for one batch) at step 1450: 0.0471\n",
            "Seen so far: 46432 samples\n",
            "0.8689912\n",
            "Training loss (for one batch) at step 1455: 0.1300\n",
            "Seen so far: 46592 samples\n",
            "0.86926943\n",
            "Training loss (for one batch) at step 1460: 0.1105\n",
            "Seen so far: 46752 samples\n",
            "0.8695243\n",
            "Training loss (for one batch) at step 1465: 0.1210\n",
            "Seen so far: 46912 samples\n",
            "0.8697988\n",
            "Training loss (for one batch) at step 1470: 0.0792\n",
            "Seen so far: 47072 samples\n",
            "0.87007135\n",
            "Training loss (for one batch) at step 1475: 0.2434\n",
            "Seen so far: 47232 samples\n",
            "0.8702363\n",
            "Training loss (for one batch) at step 1480: 0.2656\n",
            "Seen so far: 47392 samples\n",
            "0.8704845\n",
            "Training loss (for one batch) at step 1485: 0.0769\n",
            "Seen so far: 47552 samples\n",
            "0.87075204\n",
            "Training loss (for one batch) at step 1490: 0.2254\n",
            "Seen so far: 47712 samples\n",
            "0.87091297\n",
            "Training loss (for one batch) at step 1495: 0.1545\n",
            "Seen so far: 47872 samples\n",
            "0.87123996\n",
            "Training loss (for one batch) at step 1500: 0.2107\n",
            "Seen so far: 48032 samples\n",
            "0.87154394\n",
            "Training loss (for one batch) at step 1505: 0.1796\n",
            "Seen so far: 48192 samples\n",
            "0.8717422\n",
            "Training loss (for one batch) at step 1510: 0.1521\n",
            "Seen so far: 48352 samples\n",
            "0.8719391\n",
            "Training loss (for one batch) at step 1515: 0.1411\n",
            "Seen so far: 48512 samples\n",
            "0.87227905\n",
            "Training loss (for one batch) at step 1520: 0.2327\n",
            "Seen so far: 48672 samples\n",
            "0.8724729\n",
            "Training loss (for one batch) at step 1525: 0.3158\n",
            "Seen so far: 48832 samples\n",
            "0.87268597\n",
            "Training loss (for one batch) at step 1530: 0.1196\n",
            "Seen so far: 48992 samples\n",
            "0.8728976\n",
            "Training loss (for one batch) at step 1535: 0.0511\n",
            "Seen so far: 49152 samples\n",
            "0.8731893\n",
            "Training loss (for one batch) at step 1540: 0.0817\n",
            "Seen so far: 49312 samples\n",
            "0.8734182\n",
            "Training loss (for one batch) at step 1545: 0.0939\n",
            "Seen so far: 49472 samples\n",
            "0.87376696\n",
            "Training loss (for one batch) at step 1550: 0.1118\n",
            "Seen so far: 49632 samples\n",
            "0.87399256\n",
            "Training loss (for one batch) at step 1555: 0.1970\n",
            "Seen so far: 49792 samples\n",
            "0.87419665\n",
            "Training loss (for one batch) at step 1560: 0.0734\n",
            "Seen so far: 49952 samples\n",
            "0.87433934\n",
            "Training loss (for one batch) at step 1565: 0.2964\n",
            "Seen so far: 50112 samples\n",
            "0.8745211\n",
            "Training loss (for one batch) at step 1570: 0.1962\n",
            "Seen so far: 50272 samples\n",
            "0.8748011\n",
            "Training loss (for one batch) at step 1575: 0.0018\n",
            "Seen so far: 50432 samples\n",
            "0.87507933\n",
            "Training loss (for one batch) at step 1580: 0.0482\n",
            "Seen so far: 50592 samples\n",
            "0.8753558\n",
            "Training loss (for one batch) at step 1585: 0.3155\n",
            "Seen so far: 50752 samples\n",
            "0.8756108\n",
            "Training loss (for one batch) at step 1590: 0.1462\n",
            "Seen so far: 50912 samples\n",
            "0.8758446\n",
            "Training loss (for one batch) at step 1595: 0.1010\n",
            "Seen so far: 51072 samples\n",
            "0.8760965\n",
            "Training loss (for one batch) at step 1600: 0.1437\n",
            "Seen so far: 51232 samples\n",
            "0.8762492\n",
            "Training loss (for one batch) at step 1605: 0.0624\n",
            "Seen so far: 51392 samples\n",
            "0.87655663\n",
            "Training loss (for one batch) at step 1610: 0.0313\n",
            "Seen so far: 51552 samples\n",
            "0.876707\n",
            "Training loss (for one batch) at step 1615: 1.0200\n",
            "Seen so far: 51712 samples\n",
            "0.8767791\n",
            "Training loss (for one batch) at step 1620: 0.1938\n",
            "Seen so far: 51872 samples\n",
            "0.8770049\n",
            "Training loss (for one batch) at step 1625: 0.2114\n",
            "Seen so far: 52032 samples\n",
            "0.87717175\n",
            "Training loss (for one batch) at step 1630: 0.1938\n",
            "Seen so far: 52192 samples\n",
            "0.8774333\n",
            "Training loss (for one batch) at step 1635: 0.3152\n",
            "Seen so far: 52352 samples\n",
            "0.8775787\n",
            "Training loss (for one batch) at step 1640: 0.0619\n",
            "Seen so far: 52512 samples\n",
            "0.8778755\n",
            "Training loss (for one batch) at step 1645: 0.1606\n",
            "Seen so far: 52672 samples\n",
            "0.8781136\n",
            "Training loss (for one batch) at step 1650: 0.1730\n",
            "Seen so far: 52832 samples\n",
            "0.87836915\n",
            "Training loss (for one batch) at step 1655: 0.2365\n",
            "Seen so far: 52992 samples\n",
            "0.8786232\n",
            "Training loss (for one batch) at step 1660: 0.0244\n",
            "Seen so far: 53152 samples\n",
            "0.8788757\n",
            "Training loss (for one batch) at step 1665: 0.0274\n",
            "Seen so far: 53312 samples\n",
            "0.87916416\n",
            "Training loss (for one batch) at step 1670: 0.1945\n",
            "Seen so far: 53472 samples\n",
            "0.87933874\n",
            "Training loss (for one batch) at step 1675: 0.3654\n",
            "Seen so far: 53632 samples\n",
            "0.8796427\n",
            "Training loss (for one batch) at step 1680: 0.0374\n",
            "Seen so far: 53792 samples\n",
            "0.879945\n",
            "Training loss (for one batch) at step 1685: 0.1905\n",
            "Seen so far: 53952 samples\n",
            "0.8801157\n",
            "Training loss (for one batch) at step 1690: 0.1485\n",
            "Seen so far: 54112 samples\n",
            "0.88034075\n",
            "Training loss (for one batch) at step 1695: 0.1254\n",
            "Seen so far: 54272 samples\n",
            "0.88054615\n",
            "Training loss (for one batch) at step 1700: 0.0434\n",
            "Seen so far: 54432 samples\n",
            "0.88073194\n",
            "Training loss (for one batch) at step 1705: 0.4323\n",
            "Seen so far: 54592 samples\n",
            "0.88095325\n",
            "Training loss (for one batch) at step 1710: 0.1317\n",
            "Seen so far: 54752 samples\n",
            "0.8812281\n",
            "Training loss (for one batch) at step 1715: 0.1416\n",
            "Seen so far: 54912 samples\n",
            "0.88135564\n",
            "Training loss (for one batch) at step 1720: 0.0080\n",
            "Seen so far: 55072 samples\n",
            "0.8815914\n",
            "Training loss (for one batch) at step 1725: 0.1066\n",
            "Seen so far: 55232 samples\n",
            "0.88177145\n",
            "Training loss (for one batch) at step 1730: 0.0169\n",
            "Seen so far: 55392 samples\n",
            "0.88204074\n",
            "Training loss (for one batch) at step 1735: 0.1562\n",
            "Seen so far: 55552 samples\n",
            "0.8823085\n",
            "Training loss (for one batch) at step 1740: 0.2818\n",
            "Seen so far: 55712 samples\n",
            "0.8825208\n",
            "Training loss (for one batch) at step 1745: 0.0693\n",
            "Seen so far: 55872 samples\n",
            "0.8826783\n",
            "Training loss (for one batch) at step 1750: 0.0207\n",
            "Seen so far: 56032 samples\n",
            "0.8829062\n",
            "Training loss (for one batch) at step 1755: 0.2250\n",
            "Seen so far: 56192 samples\n",
            "0.88307947\n",
            "Training loss (for one batch) at step 1760: 0.3164\n",
            "Seen so far: 56352 samples\n",
            "0.8832694\n",
            "Training loss (for one batch) at step 1765: 0.0253\n",
            "Seen so far: 56512 samples\n",
            "0.8835292\n",
            "Training loss (for one batch) at step 1770: 0.3138\n",
            "Seen so far: 56672 samples\n",
            "0.8836639\n",
            "Training loss (for one batch) at step 1775: 0.0319\n",
            "Seen so far: 56832 samples\n",
            "0.88388586\n",
            "Training loss (for one batch) at step 1780: 0.0145\n",
            "Seen so far: 56992 samples\n",
            "0.8841241\n",
            "Training loss (for one batch) at step 1785: 0.2587\n",
            "Seen so far: 57152 samples\n",
            "0.884326\n",
            "Training loss (for one batch) at step 1790: 0.0199\n",
            "Seen so far: 57312 samples\n",
            "0.88454425\n",
            "Training loss (for one batch) at step 1795: 0.5991\n",
            "Seen so far: 57472 samples\n",
            "0.88469166\n",
            "Training loss (for one batch) at step 1800: 0.0223\n",
            "Seen so far: 57632 samples\n",
            "0.884873\n",
            "Training loss (for one batch) at step 1805: 0.0269\n",
            "Seen so far: 57792 samples\n",
            "0.8850533\n",
            "Training loss (for one batch) at step 1810: 0.0816\n",
            "Seen so far: 57952 samples\n",
            "0.88528436\n",
            "Training loss (for one batch) at step 1815: 0.4415\n",
            "Seen so far: 58112 samples\n",
            "0.88544536\n",
            "Training loss (for one batch) at step 1820: 0.1078\n",
            "Seen so far: 58272 samples\n",
            "0.88560545\n",
            "Training loss (for one batch) at step 1825: 0.0623\n",
            "Seen so far: 58432 samples\n",
            "0.8857989\n",
            "Training loss (for one batch) at step 1830: 0.1194\n",
            "Seen so far: 58592 samples\n",
            "0.8860083\n",
            "Training loss (for one batch) at step 1835: 0.0750\n",
            "Seen so far: 58752 samples\n",
            "0.88616556\n",
            "Training loss (for one batch) at step 1840: 0.0766\n",
            "Seen so far: 58912 samples\n",
            "0.88633895\n",
            "Training loss (for one batch) at step 1845: 0.2036\n",
            "Seen so far: 59072 samples\n",
            "0.88647753\n",
            "Training loss (for one batch) at step 1850: 0.0835\n",
            "Seen so far: 59232 samples\n",
            "0.88673353\n",
            "Training loss (for one batch) at step 1855: 0.3849\n",
            "Seen so far: 59392 samples\n",
            "0.8869545\n",
            "Training loss (for one batch) at step 1860: 0.1326\n",
            "Seen so far: 59552 samples\n",
            "0.887191\n",
            "Training loss (for one batch) at step 1865: 0.0396\n",
            "Seen so far: 59712 samples\n",
            "0.8873593\n",
            "Training loss (for one batch) at step 1870: 0.0734\n",
            "Seen so far: 59872 samples\n",
            "0.8874432\n",
            "Training loss (for one batch) at step 1875: 0.0644\n",
            "Seen so far: 60032 samples\n",
            "0.88760996\n",
            "Training loss (for one batch) at step 1880: 0.4552\n",
            "Seen so far: 60192 samples\n",
            "0.88775915\n",
            "Training loss (for one batch) at step 1885: 0.0616\n",
            "Seen so far: 60352 samples\n",
            "0.88800704\n",
            "Training loss (for one batch) at step 1890: 0.0462\n",
            "Seen so far: 60512 samples\n",
            "0.8882205\n",
            "Training loss (for one batch) at step 1895: 0.0952\n",
            "Seen so far: 60672 samples\n",
            "0.8883999\n",
            "Training loss (for one batch) at step 1900: 0.0496\n",
            "Seen so far: 60832 samples\n",
            "0.8886277\n",
            "Training loss (for one batch) at step 1905: 0.3970\n",
            "Seen so far: 60992 samples\n",
            "0.88872313\n",
            "Training loss (for one batch) at step 1910: 0.1028\n",
            "Seen so far: 61152 samples\n",
            "0.8888834\n",
            "Training loss (for one batch) at step 1915: 0.2416\n",
            "Seen so far: 61312 samples\n",
            "0.8890103\n",
            "Training loss (for one batch) at step 1920: 0.3194\n",
            "Seen so far: 61472 samples\n",
            "0.88916904\n",
            "Training loss (for one batch) at step 1925: 0.0918\n",
            "Seen so far: 61632 samples\n",
            "0.8893108\n",
            "Training loss (for one batch) at step 1930: 0.1045\n",
            "Seen so far: 61792 samples\n",
            "0.8894517\n",
            "Training loss (for one batch) at step 1935: 0.0513\n",
            "Seen so far: 61952 samples\n",
            "0.8895274\n",
            "Training loss (for one batch) at step 1940: 0.5112\n",
            "Seen so far: 62112 samples\n",
            "0.88965094\n",
            "Training loss (for one batch) at step 1945: 0.2154\n",
            "Seen so far: 62272 samples\n",
            "0.88980603\n",
            "Training loss (for one batch) at step 1950: 0.0582\n",
            "Seen so far: 62432 samples\n",
            "0.88994426\n",
            "Training loss (for one batch) at step 1955: 0.0923\n",
            "Seen so far: 62592 samples\n",
            "0.8900818\n",
            "Training loss (for one batch) at step 1960: 0.0432\n",
            "Seen so far: 62752 samples\n",
            "0.8902664\n",
            "Training loss (for one batch) at step 1965: 0.0947\n",
            "Seen so far: 62912 samples\n",
            "0.89043427\n",
            "Training loss (for one batch) at step 1970: 0.2089\n",
            "Seen so far: 63072 samples\n",
            "0.8905378\n",
            "Training loss (for one batch) at step 1975: 0.1821\n",
            "Seen so far: 63232 samples\n",
            "0.8907199\n",
            "Training loss (for one batch) at step 1980: 0.0775\n",
            "Seen so far: 63392 samples\n",
            "0.8908695\n",
            "Training loss (for one batch) at step 1985: 0.0901\n",
            "Seen so far: 63552 samples\n",
            "0.8910656\n",
            "Training loss (for one batch) at step 1990: 0.1779\n",
            "Seen so far: 63712 samples\n",
            "0.8912136\n",
            "Training loss (for one batch) at step 1995: 0.2350\n",
            "Seen so far: 63872 samples\n",
            "0.89131385\n",
            "Training loss (for one batch) at step 2000: 0.2890\n",
            "Seen so far: 64032 samples\n",
            "0.8914293\n",
            "Training loss (for one batch) at step 2005: 0.0502\n",
            "Seen so far: 64192 samples\n",
            "0.8915753\n",
            "Training loss (for one batch) at step 2010: 0.1232\n",
            "Seen so far: 64352 samples\n",
            "0.8916895\n",
            "Training loss (for one batch) at step 2015: 0.0119\n",
            "Seen so far: 64512 samples\n",
            "0.89189607\n",
            "Training loss (for one batch) at step 2020: 0.1005\n",
            "Seen so far: 64672 samples\n",
            "0.8919625\n",
            "Training loss (for one batch) at step 2025: 0.0754\n",
            "Seen so far: 64832 samples\n",
            "0.8921366\n",
            "Training loss (for one batch) at step 2030: 0.1365\n",
            "Seen so far: 64992 samples\n",
            "0.8922944\n",
            "Training loss (for one batch) at step 2035: 0.0499\n",
            "Seen so far: 65152 samples\n",
            "0.89252824\n",
            "Training loss (for one batch) at step 2040: 0.0026\n",
            "Seen so far: 65312 samples\n",
            "0.89268434\n",
            "Training loss (for one batch) at step 2045: 0.2450\n",
            "Seen so far: 65472 samples\n",
            "0.892855\n",
            "Training loss (for one batch) at step 2050: 0.1054\n",
            "Seen so far: 65632 samples\n",
            "0.89304\n",
            "Training loss (for one batch) at step 2055: 0.1875\n",
            "Seen so far: 65792 samples\n",
            "0.8931481\n",
            "Training loss (for one batch) at step 2060: 0.0059\n",
            "Seen so far: 65952 samples\n",
            "0.8933163\n",
            "Training loss (for one batch) at step 2065: 0.2418\n",
            "Seen so far: 66112 samples\n",
            "0.89345354\n",
            "Training loss (for one batch) at step 2070: 0.0759\n",
            "Seen so far: 66272 samples\n",
            "0.89359003\n",
            "Training loss (for one batch) at step 2075: 0.4172\n",
            "Seen so far: 66432 samples\n",
            "0.89366573\n",
            "Training loss (for one batch) at step 2080: 0.0244\n",
            "Seen so far: 66592 samples\n",
            "0.89381605\n",
            "Training loss (for one batch) at step 2085: 0.0612\n",
            "Seen so far: 66752 samples\n",
            "0.89395076\n",
            "Training loss (for one batch) at step 2090: 0.0158\n",
            "Seen so far: 66912 samples\n",
            "0.8941147\n",
            "Training loss (for one batch) at step 2095: 0.0445\n",
            "Seen so far: 67072 samples\n",
            "0.89424795\n",
            "Training loss (for one batch) at step 2100: 0.1769\n",
            "Seen so far: 67232 samples\n",
            "0.8943063\n",
            "Training loss (for one batch) at step 2105: 0.0393\n",
            "Seen so far: 67392 samples\n",
            "0.89442366\n",
            "Training loss (for one batch) at step 2110: 0.4894\n",
            "Seen so far: 67552 samples\n",
            "0.8945405\n",
            "Training loss (for one batch) at step 2115: 0.3032\n",
            "Seen so far: 67712 samples\n",
            "0.8946568\n",
            "Training loss (for one batch) at step 2120: 0.1540\n",
            "Seen so far: 67872 samples\n",
            "0.894802\n",
            "Training loss (for one batch) at step 2125: 0.3535\n",
            "Seen so far: 68032 samples\n",
            "0.894873\n",
            "Training loss (for one batch) at step 2130: 0.1700\n",
            "Seen so far: 68192 samples\n",
            "0.8949584\n",
            "Training loss (for one batch) at step 2135: 0.1125\n",
            "Seen so far: 68352 samples\n",
            "0.89501405\n",
            "Training loss (for one batch) at step 2140: 0.4709\n",
            "Seen so far: 68512 samples\n",
            "0.8950695\n",
            "Training loss (for one batch) at step 2145: 0.0628\n",
            "Seen so far: 68672 samples\n",
            "0.8951829\n",
            "Training loss (for one batch) at step 2150: 0.2062\n",
            "Seen so far: 68832 samples\n",
            "0.8953248\n",
            "Training loss (for one batch) at step 2155: 0.0142\n",
            "Seen so far: 68992 samples\n",
            "0.89542264\n",
            "Training loss (for one batch) at step 2160: 0.0311\n",
            "Seen so far: 69152 samples\n",
            "0.89553446\n",
            "Training loss (for one batch) at step 2165: 0.0517\n",
            "Seen so far: 69312 samples\n",
            "0.89574677\n",
            "Training loss (for one batch) at step 2170: 0.1231\n",
            "Seen so far: 69472 samples\n",
            "0.89577097\n",
            "Training loss (for one batch) at step 2175: 0.0594\n",
            "Seen so far: 69632 samples\n",
            "0.895953\n",
            "Training loss (for one batch) at step 2180: 0.1085\n",
            "Seen so far: 69792 samples\n",
            "0.89604825\n",
            "Training loss (for one batch) at step 2185: 0.4539\n",
            "Seen so far: 69952 samples\n",
            "0.8961717\n",
            "Training loss (for one batch) at step 2190: 0.2506\n",
            "Seen so far: 70112 samples\n",
            "0.8963373\n",
            "Training loss (for one batch) at step 2195: 0.0302\n",
            "Seen so far: 70272 samples\n",
            "0.89648795\n",
            "Training loss (for one batch) at step 2200: 0.0280\n",
            "Seen so far: 70432 samples\n",
            "0.8966521\n",
            "Training loss (for one batch) at step 2205: 0.0929\n",
            "Seen so far: 70592 samples\n",
            "0.89674467\n",
            "Training loss (for one batch) at step 2210: 0.2230\n",
            "Seen so far: 70752 samples\n",
            "0.8968651\n",
            "Training loss (for one batch) at step 2215: 0.5018\n",
            "Seen so far: 70912 samples\n",
            "0.8970132\n",
            "Training loss (for one batch) at step 2220: 0.1024\n",
            "Seen so far: 71072 samples\n",
            "0.8971043\n",
            "Training loss (for one batch) at step 2225: 0.2054\n",
            "Seen so far: 71232 samples\n",
            "0.89718103\n",
            "Training loss (for one batch) at step 2230: 0.1593\n",
            "Seen so far: 71392 samples\n",
            "0.8972714\n",
            "Training loss (for one batch) at step 2235: 0.0056\n",
            "Seen so far: 71552 samples\n",
            "0.8974452\n",
            "Training loss (for one batch) at step 2240: 0.4484\n",
            "Seen so far: 71712 samples\n",
            "0.89752066\n",
            "Training loss (for one batch) at step 2245: 0.0716\n",
            "Seen so far: 71872 samples\n",
            "0.8976653\n",
            "Training loss (for one batch) at step 2250: 0.0641\n",
            "Seen so far: 72032 samples\n",
            "0.8978093\n",
            "Training loss (for one batch) at step 2255: 0.1218\n",
            "Seen so far: 72192 samples\n",
            "0.89791113\n",
            "Training loss (for one batch) at step 2260: 0.0113\n",
            "Seen so far: 72352 samples\n",
            "0.8980678\n",
            "Training loss (for one batch) at step 2265: 0.2397\n",
            "Seen so far: 72512 samples\n",
            "0.8982099\n",
            "Training loss (for one batch) at step 2270: 0.2362\n",
            "Seen so far: 72672 samples\n",
            "0.89831024\n",
            "Training loss (for one batch) at step 2275: 0.3246\n",
            "Seen so far: 72832 samples\n",
            "0.8983963\n",
            "Training loss (for one batch) at step 2280: 0.0230\n",
            "Seen so far: 72992 samples\n",
            "0.89859164\n",
            "Training loss (for one batch) at step 2285: 0.0153\n",
            "Seen so far: 73152 samples\n",
            "0.89867675\n",
            "Training loss (for one batch) at step 2290: 0.0036\n",
            "Seen so far: 73312 samples\n",
            "0.8988706\n",
            "Training loss (for one batch) at step 2295: 0.0202\n",
            "Seen so far: 73472 samples\n",
            "0.8989547\n",
            "Training loss (for one batch) at step 2300: 0.0336\n",
            "Seen so far: 73632 samples\n",
            "0.8990928\n",
            "Training loss (for one batch) at step 2305: 0.1718\n",
            "Seen so far: 73792 samples\n",
            "0.8991896\n",
            "Training loss (for one batch) at step 2310: 0.1143\n",
            "Seen so far: 73952 samples\n",
            "0.8993807\n",
            "Training loss (for one batch) at step 2315: 0.1129\n",
            "Seen so far: 74112 samples\n",
            "0.89950347\n",
            "Training loss (for one batch) at step 2320: 0.5497\n",
            "Seen so far: 74272 samples\n",
            "0.8995853\n",
            "Training loss (for one batch) at step 2325: 0.3757\n",
            "Seen so far: 74432 samples\n",
            "0.89968026\n",
            "Training loss (for one batch) at step 2330: 0.3672\n",
            "Seen so far: 74592 samples\n",
            "0.8996809\n",
            "Training loss (for one batch) at step 2335: 0.2174\n",
            "Seen so far: 74752 samples\n",
            "0.8997084\n",
            "Training loss (for one batch) at step 2340: 0.1787\n",
            "Seen so far: 74912 samples\n",
            "0.8998158\n",
            "Training loss (for one batch) at step 2345: 0.1361\n",
            "Seen so far: 75072 samples\n",
            "0.8998828\n",
            "Training loss (for one batch) at step 2350: 0.1250\n",
            "Seen so far: 75232 samples\n",
            "0.8999096\n",
            "Training loss (for one batch) at step 2355: 0.3050\n",
            "Seen so far: 75392 samples\n",
            "0.90000266\n",
            "Training loss (for one batch) at step 2360: 0.2159\n",
            "Seen so far: 75552 samples\n",
            "0.9001085\n",
            "Training loss (for one batch) at step 2365: 0.4431\n",
            "Seen so far: 75712 samples\n",
            "0.90021396\n",
            "Training loss (for one batch) at step 2370: 0.1142\n",
            "Seen so far: 75872 samples\n",
            "0.9003717\n",
            "Training loss (for one batch) at step 2375: 0.3787\n",
            "Seen so far: 76032 samples\n",
            "0.90041035\n",
            "Training loss (for one batch) at step 2380: 0.1722\n",
            "Seen so far: 76192 samples\n",
            "0.9005145\n",
            "Training loss (for one batch) at step 2385: 0.2032\n",
            "Seen so far: 76352 samples\n",
            "0.90059197\n",
            "Training loss (for one batch) at step 2390: 0.1438\n",
            "Seen so far: 76512 samples\n",
            "0.90073454\n",
            "Training loss (for one batch) at step 2395: 0.4349\n",
            "Seen so far: 76672 samples\n",
            "0.90078515\n",
            "Training loss (for one batch) at step 2400: 0.2414\n",
            "Seen so far: 76832 samples\n",
            "0.9008096\n",
            "Training loss (for one batch) at step 2405: 0.0344\n",
            "Seen so far: 76992 samples\n",
            "0.9009637\n",
            "Training loss (for one batch) at step 2410: 0.0537\n",
            "Seen so far: 77152 samples\n",
            "0.9010654\n",
            "Training loss (for one batch) at step 2415: 0.0356\n",
            "Seen so far: 77312 samples\n",
            "0.90123135\n",
            "Training loss (for one batch) at step 2420: 0.0681\n",
            "Seen so far: 77472 samples\n",
            "0.9012675\n",
            "Training loss (for one batch) at step 2425: 0.3231\n",
            "Seen so far: 77632 samples\n",
            "0.90132934\n",
            "Training loss (for one batch) at step 2430: 0.0951\n",
            "Seen so far: 77792 samples\n",
            "0.9013909\n",
            "Training loss (for one batch) at step 2435: 0.1988\n",
            "Seen so far: 77952 samples\n",
            "0.901465\n",
            "Training loss (for one batch) at step 2440: 0.3254\n",
            "Seen so far: 78112 samples\n",
            "0.9015644\n",
            "Training loss (for one batch) at step 2445: 0.1245\n",
            "Seen so far: 78272 samples\n",
            "0.90165067\n",
            "Training loss (for one batch) at step 2450: 0.1172\n",
            "Seen so far: 78432 samples\n",
            "0.9018003\n",
            "Training loss (for one batch) at step 2455: 0.0608\n",
            "Seen so far: 78592 samples\n",
            "0.9019493\n",
            "Training loss (for one batch) at step 2460: 0.1898\n",
            "Seen so far: 78752 samples\n",
            "0.9020342\n",
            "Training loss (for one batch) at step 2465: 0.0690\n",
            "Seen so far: 78912 samples\n",
            "0.90210617\n",
            "Training loss (for one batch) at step 2470: 0.0222\n",
            "Seen so far: 79072 samples\n",
            "0.90222836\n",
            "Training loss (for one batch) at step 2475: 0.1551\n",
            "Seen so far: 79232 samples\n",
            "0.9023122\n",
            "Training loss (for one batch) at step 2480: 0.0400\n",
            "Seen so far: 79392 samples\n",
            "0.9023957\n",
            "Training loss (for one batch) at step 2485: 0.0395\n",
            "Seen so far: 79552 samples\n",
            "0.90249145\n",
            "Training loss (for one batch) at step 2490: 0.0609\n",
            "Seen so far: 79712 samples\n",
            "0.90259933\n",
            "Training loss (for one batch) at step 2495: 0.2343\n",
            "Seen so far: 79872 samples\n",
            "0.90265673\n",
            "Training loss (for one batch) at step 2500: 0.1832\n",
            "Seen so far: 80032 samples\n",
            "0.9027514\n",
            "Training loss (for one batch) at step 2505: 0.1017\n",
            "Seen so far: 80192 samples\n",
            "0.90285814\n",
            "Training loss (for one batch) at step 2510: 0.1132\n",
            "Seen so far: 80352 samples\n",
            "0.9029893\n",
            "Training loss (for one batch) at step 2515: 0.0762\n",
            "Seen so far: 80512 samples\n",
            "0.90307033\n",
            "Training loss (for one batch) at step 2520: 0.1641\n",
            "Seen so far: 80672 samples\n",
            "0.90317583\n",
            "Training loss (for one batch) at step 2525: 0.0503\n",
            "Seen so far: 80832 samples\n",
            "0.90321904\n",
            "Training loss (for one batch) at step 2530: 0.1473\n",
            "Seen so far: 80992 samples\n",
            "0.90328676\n",
            "Training loss (for one batch) at step 2535: 0.0352\n",
            "Seen so far: 81152 samples\n",
            "0.90342814\n",
            "Training loss (for one batch) at step 2540: 0.3972\n",
            "Seen so far: 81312 samples\n",
            "0.90351975\n",
            "Training loss (for one batch) at step 2545: 0.1205\n",
            "Seen so far: 81472 samples\n",
            "0.90362334\n",
            "Training loss (for one batch) at step 2550: 0.0428\n",
            "Seen so far: 81632 samples\n",
            "0.90376323\n",
            "Training loss (for one batch) at step 2555: 0.2828\n",
            "Seen so far: 81792 samples\n",
            "0.90384144\n",
            "Training loss (for one batch) at step 2560: 0.1635\n",
            "Seen so far: 81952 samples\n",
            "0.9039438\n",
            "Training loss (for one batch) at step 2565: 0.0340\n",
            "Seen so far: 82112 samples\n",
            "0.90410656\n",
            "Training loss (for one batch) at step 2570: 0.0503\n",
            "Seen so far: 82272 samples\n",
            "0.90426874\n",
            "Training loss (for one batch) at step 2575: 0.1138\n",
            "Seen so far: 82432 samples\n",
            "0.9043333\n",
            "Training loss (for one batch) at step 2580: 0.1620\n",
            "Seen so far: 82592 samples\n",
            "0.90440965\n",
            "Training loss (for one batch) at step 2585: 0.0128\n",
            "Seen so far: 82752 samples\n",
            "0.90450984\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_403/183399485.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 optimizer=optimizer)\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m trainer.train(train_dataset=dataset,\n\u001b[0m\u001b[1;32m     21\u001b[0m             train_metric=train_acc_metric)\n",
            "\u001b[0;32m/tmp/ipykernel_403/1636777303.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataset, train_metric)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                     \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1037\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_403/1974525113.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pred\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1037\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtensors\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mthan\u001b[0m \u001b[0mone\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \"\"\"\n\u001b[0;32m--> 414\u001b[0;31m     return self._run_internal_graph(\n\u001b[0m\u001b[1;32m    415\u001b[0m         inputs, training=training, mask=mask)\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1037\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/layers/pooling.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'channels_last'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/backend.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(x, axis, keepdims)\u001b[0m\n\u001b[1;32m   2421\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2423\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_mean\u001b[0;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[1;32m   2579\u001b[0m   return _may_reduce_to_scalar(\n\u001b[1;32m   2580\u001b[0m       \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2581\u001b[0;31m       gen_math_ops.mean(\n\u001b[0m\u001b[1;32m   2582\u001b[0m           \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ReductionDims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2583\u001b[0m           name=name))\n",
            "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(input, axis, keep_dims, name)\u001b[0m\n\u001b[1;32m   5943\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5944\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5945\u001b[0;31m       return mean_eager_fallback(\n\u001b[0m\u001b[1;32m   5946\u001b[0m           input, axis, keep_dims=keep_dims, name=name, ctx=_ctx)\n\u001b[1;32m   5947\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmean_eager_fallback\u001b[0;34m(input, axis, keep_dims, name, ctx)\u001b[0m\n\u001b[1;32m   5978\u001b[0m                              ctx=ctx, name=name)\n\u001b[1;32m   5979\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5980\u001b[0;31m     _execute.record_gradient(\n\u001b[0m\u001b[1;32m   5981\u001b[0m         \"Mean\", _inputs_flat, _attrs, _result)\n\u001b[1;32m   5982\u001b[0m   \u001b[0m_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mrecord_gradient\u001b[0;34m(op_name, inputs, attrs, outputs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m   \"\"\"\n\u001b[0;32m--> 182\u001b[0;31m   pywrap_tfe.TFE_Py_RecordGradient(op_name, inputs, attrs, outputs,\n\u001b[0m\u001b[1;32m    183\u001b[0m                                    ops.get_name_scope())\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 모델 학습 코드\n",
        "\n",
        "train_path = \"/aiffel/aiffel/model-fit/data/30vnfoods/Train\"\n",
        "\n",
        "epoch = 5\n",
        "batch = 32\n",
        "\n",
        "model = Model(num_classes=10)\n",
        "dataset = load_data(data_path=train_path, batch_size=batch)\n",
        "loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "trainer = Trainer(model=model,\n",
        "                epochs=epoch,\n",
        "                batch=batch,\n",
        "                ds_length=train_length,\n",
        "                loss_fn=loss_function,\n",
        "                optimizer=optimizer)\n",
        "\n",
        "trainer.train(train_dataset=dataset,\n",
        "            train_metric=train_acc_metric)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9b29d57",
      "metadata": {
        "id": "f9b29d57"
      },
      "source": [
        "### Test model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b04bb14",
      "metadata": {
        "id": "1b04bb14",
        "outputId": "82803052-8c59-4b6a-b6dd-a982069f9faa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Bun rieu', 'Banh mi', 'Banh xeo', 'Chao long', 'Pho', 'Banh khot', 'Bun bo Hue', 'Banh cuon', 'Com tam', 'Bun dau mam tom']\n",
            "30/32\n",
            "26/32\n",
            "30/32\n",
            "25/32\n",
            "30/32\n",
            "31/32\n",
            "29/32\n",
            "30/32\n",
            "26/32\n",
            "29/32\n"
          ]
        }
      ],
      "source": [
        "# 학습이 20분 이상 걸려서 임의로 중단하고 테스트\n",
        "# 모델 테스트 코드\n",
        "\n",
        "test_ds = load_data(data_path=test_path)\n",
        "\n",
        "for step_train, (x_batch_train, y_batch_train) in enumerate(test_ds.take(10)):\n",
        "    prediction = model(x_batch_train)\n",
        "    print(\"{}/{}\".format(np.array(tf.equal(tf.argmax(y_batch_train, axis=1), tf.argmax(prediction, axis=1))).sum(), tf.argmax(y_batch_train, axis=1).shape[0]))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}